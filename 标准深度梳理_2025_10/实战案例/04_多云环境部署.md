# 实战案例：多云环境 OpenTelemetry 部署完整指南

> **场景**: AWS + GCP + Azure 多云环境统一可观测性  
> **技术栈**: Go 1.25.1, Kubernetes, Terraform, OpenTelemetry Collector  
> **特性**: 跨云追踪、统一监控、成本优化、灾备切换  
> **OpenTelemetry**: v1.32.0+  
> **难度**: ⭐⭐⭐⭐⭐ (生产级 + 多云架构)

---

## 📋 目录

- [实战案例：多云环境 OpenTelemetry 部署完整指南](#实战案例多云环境-opentelemetry-部署完整指南)
  - [📋 目录](#-目录)
  - [系统概述](#系统概述)
    - [业务场景](#业务场景)
    - [系统拓扑](#系统拓扑)
    - [关键挑战](#关键挑战)
  - [架构设计](#架构设计)
    - [三层架构](#三层架构)
    - [数据流](#数据流)
  - [各云平台集成](#各云平台集成)
    - [1. AWS (EKS) 集成](#1-aws-eks-集成)
      - [Kubernetes 部署](#kubernetes-部署)
      - [Collector 配置 (AWS)](#collector-配置-aws)
    - [2. GCP (GKE) 集成](#2-gcp-gke-集成)
      - [Collector 配置 (GCP)](#collector-配置-gcp)
    - [3. Azure (AKS) 集成](#3-azure-aks-集成)
      - [Collector 配置 (Azure)](#collector-配置-azure)
  - [统一采集配置](#统一采集配置)
    - [中心 Collector 配置](#中心-collector-配置)
  - [跨云追踪实现](#跨云追踪实现)
    - [Go 应用配置](#go-应用配置)
    - [跨云服务调用示例](#跨云服务调用示例)
    - [跨云 HTTP 客户端](#跨云-http-客户端)
  - [成本优化](#成本优化)
    - [1. 采样策略](#1-采样策略)
    - [2. 数据归档](#2-数据归档)
    - [3. 成本监控](#3-成本监控)
  - [灾备切换](#灾备切换)
    - [1. 双中心配置](#1-双中心配置)
    - [2. 健康检查](#2-健康检查)
  - [Terraform 自动化部署](#terraform-自动化部署)
    - [AWS EKS](#aws-eks)
  - [最佳实践总结](#最佳实践总结)
    - [1. 区域就近采集](#1-区域就近采集)
    - [2. 统一资源属性](#2-统一资源属性)
    - [3. 成本优化](#3-成本优化)
    - [4. 灾备](#4-灾备)
  - [🎯 学到的关键知识点](#-学到的关键知识点)

---

## 系统概述

### 业务场景

多云部署的典型场景：

- 🌍 **地理分布**: 不同区域使用不同云服务商
- 💰 **成本优化**: 利用不同云商的价格优势
- 🔒 **供应商锁定规避**: 避免单一云商依赖
- 🚀 **灾备**: 跨云冗余部署

### 系统拓扑

```text
┌─────────────── AWS (US-East) ──────────────┐
│  - API Gateway (EKS)                       │
│  - Order Service                           │
│  - OTLP Collector (DaemonSet)              │
│  - CloudWatch Logs                         │
└─────────────────┬───────────────────────────┘
                  │
┌─────────────── GCP (Europe-West) ──────────┐
│  - Product Service (GKE)                   │
│  - Inventory Service                       │
│  - OTLP Collector (DaemonSet)              │
│  - Cloud Logging                           │
└─────────────────┬───────────────────────────┘
                  │
┌─────────────── Azure (Asia-East) ──────────┐
│  - Payment Service (AKS)                   │
│  - User Service                            │
│  - OTLP Collector (DaemonSet)              │
│  - Azure Monitor                           │
└─────────────────┬───────────────────────────┘
                  │
         ┌────────┴────────┐
         │                 │
         ▼                 ▼
┌─────────────────┐  ┌─────────────────┐
│  Central OTLP   │  │   Backup OTLP   │
│   Collector     │  │    Collector    │
│  (AWS EKS)      │  │   (GCP GKE)     │
└────────┬────────┘  └────────┬────────┘
         │                    │
         └──────────┬─────────┘
                    │
     ┌──────────────┼──────────────┐
     ▼              ▼              ▼
┌─────────┐   ┌──────────┐   ┌─────────┐
│ Jaeger  │   │Prometheus│   │  S3     │
│(Tracing)│   │(Metrics) │   │(Archive)│
└─────────┘   └──────────┘   └─────────┘
```

### 关键挑战

| 挑战 | 解决方案 |
|------|---------|
| 跨云网络延迟 | 区域就近采集 + 异步批量传输 |
| 数据主权 | 区域内处理 + 合规采集 |
| 统一可观测性 | 中心化 Collector + 统一后端 |
| 云平台差异 | Resource Detection + 标准化属性 |
| 成本控制 | 采样策略 + 数据归档 |

---

## 架构设计

### 三层架构

```text
Layer 1: 应用层 (每个 Pod)
  ├─ Go Application (OTLP SDK)
  └─ Export → localhost:4317 (Sidecar/DaemonSet)

Layer 2: 区域采集层 (每个云/区域)
  ├─ OTLP Collector (DaemonSet/Sidecar)
  ├─ 本地预处理 (过滤、采样、富化)
  ├─ 云平台资源检测 (AWS/GCP/Azure)
  └─ Export → Central Collector

Layer 3: 中心层 (统一后端)
  ├─ Central OTLP Collector (负载均衡)
  ├─ 全局聚合与路由
  ├─ 数据持久化 (Jaeger, Prometheus, S3)
  └─ 查询与可视化 (Grafana, Jaeger UI)
```

### 数据流

```text
1. 应用生成遥测数据
   App → OTLP SDK → gRPC (localhost:4317)

2. 区域 Collector 采集
   DaemonSet Collector → 富化云平台元数据
     ├─ AWS: ec2, ecs, eks metadata
     ├─ GCP: gce, gke metadata
     └─ Azure: vm, aks metadata

3. 传输到中心 Collector
   Regional Collector → Central Collector (OTLP/gRPC)
     ├─ 压缩 (gzip)
     ├─ 批处理 (10s batch, 512 spans)
     └─ 重试 (exponential backoff)

4. 中心处理与存储
   Central Collector → 
     ├─ Jaeger (traces)
     ├─ Prometheus (metrics)
     ├─ S3 (long-term archive)
     └─ Cloud-specific backends (optional)
```

---

## 各云平台集成

### 1. AWS (EKS) 集成

#### Kubernetes 部署

```yaml:deployments/aws/otel-collector-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: otel-collector
  namespace: observability
spec:
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      serviceAccountName: otel-collector
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector-contrib:0.96.0
        ports:
        - containerPort: 4317  # OTLP gRPC
        - containerPort: 4318  # OTLP HTTP
        - containerPort: 8888  # Prometheus metrics
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        env:
        # AWS 元数据注入
        - name: AWS_REGION
          value: "us-east-1"
        - name: EKS_CLUSTER_NAME
          value: "prod-cluster"
        # Kubernetes Downward API
        - name: K8S_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: K8S_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: K8S_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: otel-config
          mountPath: /etc/otel
      volumes:
      - name: otel-config
        configMap:
          name: otel-collector-config
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: observability
  annotations:
    # AWS IAM Role for Service Account (IRSA)
    eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/OTELCollectorRole
---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: observability
spec:
  selector:
    app: otel-collector
  ports:
  - name: otlp-grpc
    port: 4317
    protocol: TCP
  - name: otlp-http
    port: 4318
    protocol: TCP
  type: ClusterIP
```

#### Collector 配置 (AWS)

```yaml:deployments/aws/otel-collector-config.yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  # 批处理优化
  batch:
    timeout: 10s
    send_batch_size: 512
    send_batch_max_size: 1024

  # AWS 资源检测
  resourcedetection/aws:
    detectors: [env, ec2, ecs, eks]
    timeout: 5s
    override: false

  # K8s 属性处理器
  k8sattributes:
    auth_type: "serviceAccount"
    passthrough: false
    extract:
      metadata:
        - k8s.namespace.name
        - k8s.deployment.name
        - k8s.pod.name
        - k8s.pod.uid
        - k8s.node.name
      labels:
        - tag_name: app
          key: app
          from: pod
      annotations:
        - tag_name: version
          key: version
          from: pod

  # 采样（降低成本）
  probabilistic_sampler:
    sampling_percentage: 10  # 10% 采样

  # 属性过滤（移除敏感信息）
  attributes:
    actions:
      - key: password
        action: delete
      - key: token
        action: delete
      - key: cloud.provider
        action: insert
        value: "aws"
      - key: cloud.region
        action: insert
        from_attribute: AWS_REGION

exporters:
  # 发送到中心 Collector
  otlp/central:
    endpoint: central-collector.example.com:4317
    compression: gzip
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s

  # CloudWatch Logs (可选，本地备份)
  awscloudwatchlogs:
    log_group_name: /aws/otel/traces
    log_stream_name: ${K8S_POD_NAME}
    region: us-east-1

  # Prometheus Metrics (本地暴露)
  prometheus:
    endpoint: 0.0.0.0:8888

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: 
        - resourcedetection/aws
        - k8sattributes
        - probabilistic_sampler
        - attributes
        - batch
      exporters: [otlp/central, awscloudwatchlogs]
      
    metrics:
      receivers: [otlp]
      processors:
        - resourcedetection/aws
        - k8sattributes
        - batch
      exporters: [otlp/central, prometheus]
```

### 2. GCP (GKE) 集成

#### Collector 配置 (GCP)

```yaml:deployments/gcp/otel-collector-config.yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317

processors:
  batch:
    timeout: 10s
    send_batch_size: 512

  # GCP 资源检测
  resourcedetection/gcp:
    detectors: [env, gcp]
    timeout: 5s

  k8sattributes:
    auth_type: "serviceAccount"
    extract:
      metadata:
        - k8s.namespace.name
        - k8s.pod.name
        - k8s.node.name

  attributes:
    actions:
      - key: cloud.provider
        action: insert
        value: "gcp"
      - key: cloud.region
        action: insert
        from_attribute: GCP_REGION

exporters:
  # 中心 Collector
  otlp/central:
    endpoint: central-collector.example.com:4317
    compression: gzip

  # Google Cloud Logging (可选)
  googlecloud:
    project: my-gcp-project
    log:
      default_log_name: opentelemetry.io/collector-exported-log

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors:
        - resourcedetection/gcp
        - k8sattributes
        - attributes
        - batch
      exporters: [otlp/central, googlecloud]
```

### 3. Azure (AKS) 集成

#### Collector 配置 (Azure)

```yaml:deployments/azure/otel-collector-config.yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317

processors:
  batch:
    timeout: 10s
    send_batch_size: 512

  # Azure 资源检测
  resourcedetection/azure:
    detectors: [env, azure]
    timeout: 5s

  k8sattributes:
    auth_type: "serviceAccount"

  attributes:
    actions:
      - key: cloud.provider
        action: insert
        value: "azure"
      - key: cloud.region
        action: insert
        from_attribute: AZURE_REGION

exporters:
  # 中心 Collector
  otlp/central:
    endpoint: central-collector.example.com:4317
    compression: gzip

  # Azure Monitor (可选)
  azuremonitor:
    instrumentation_key: ${AZURE_INSTRUMENTATION_KEY}

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors:
        - resourcedetection/azure
        - k8sattributes
        - attributes
        - batch
      exporters: [otlp/central, azuremonitor]
```

---

## 统一采集配置

### 中心 Collector 配置

```yaml:deployments/central/otel-collector-config.yaml
receivers:
  # 接收来自各区域的数据
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
        max_recv_msg_size_mib: 16

processors:
  batch:
    timeout: 15s
    send_batch_size: 1024
    send_batch_max_size: 2048

  # 全局属性添加
  attributes:
    actions:
      - key: deployment.environment
        action: insert
        value: "production"
      - key: telemetry.sdk.name
        action: insert
        value: "opentelemetry"

  # Span 过滤（只保留重要 Span）
  filter:
    error_mode: ignore
    traces:
      span:
        - 'attributes["http.status_code"] >= 500'
        - 'status.code == STATUS_CODE_ERROR'

  # Tail Sampling（尾部采样，保留所有错误trace）
  tail_sampling:
    decision_wait: 10s
    num_traces: 100
    expected_new_traces_per_sec: 10
    policies:
      - name: errors
        type: status_code
        status_code:
          status_codes: [ERROR]
      - name: slow
        type: latency
        latency:
          threshold_ms: 1000
      - name: random
        type: probabilistic
        probabilistic:
          sampling_percentage: 5

exporters:
  # Jaeger (Traces)
  jaeger:
    endpoint: jaeger-collector:14250
    tls:
      insecure: true

  # Prometheus Remote Write (Metrics)
  prometheusremotewrite:
    endpoint: http://prometheus:9090/api/v1/write

  # S3 归档（长期存储）
  s3:
    region: us-east-1
    s3_bucket: otel-archive
    s3_prefix: traces
    s3_partition: minute
    compression: gzip

  # Kafka (实时流处理，可选)
  kafka:
    brokers: [kafka:9092]
    topic: otel-traces
    encoding: otlp_proto

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch, attributes, tail_sampling]
      exporters: [jaeger, s3, kafka]
      
    metrics:
      receivers: [otlp]
      processors: [batch, attributes]
      exporters: [prometheusremotewrite]
```

---

## 跨云追踪实现

### Go 应用配置

```go:pkg/telemetry/init.go
package telemetry

import (
    "context"
    "os"

    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
    "go.opentelemetry.io/otel/propagation"
    "go.opentelemetry.io/otel/sdk/resource"
    sdktrace "go.opentelemetry.io/otel/sdk/trace"
    semconv "go.opentelemetry.io/otel/semconv/v1.26.0"
    "google.golang.org/grpc"
    "google.golang.org/grpc/credentials/insecure"
)

func InitTracer(ctx context.Context, serviceName string) (*sdktrace.TracerProvider, error) {
    // 读取环境变量（Kubernetes Downward API注入）
    cloudProvider := os.Getenv("CLOUD_PROVIDER")    // aws/gcp/azure
    cloudRegion := os.Getenv("CLOUD_REGION")
    k8sNamespace := os.Getenv("K8S_NAMESPACE")
    k8sPodName := os.Getenv("K8S_POD_NAME")
    k8sNodeName := os.Getenv("K8S_NODE_NAME")

    // 创建 Resource (包含云平台信息)
    res, err := resource.New(ctx,
        resource.WithAttributes(
            semconv.ServiceNameKey.String(serviceName),
            semconv.ServiceVersionKey.String("1.0.0"),
            semconv.CloudProviderKey.String(cloudProvider),
            semconv.CloudRegionKey.String(cloudRegion),
            semconv.K8SNamespaceNameKey.String(k8sNamespace),
            semconv.K8SPodNameKey.String(k8sPodName),
            semconv.K8SNodeNameKey.String(k8sNodeName),
        ),
    )
    if err != nil {
        return nil, err
    }

    // OTLP Exporter（连接到本地 DaemonSet Collector）
    conn, err := grpc.NewClient(
        "localhost:4317",  // DaemonSet collector
        grpc.WithTransportCredentials(insecure.NewCredentials()),
    )
    if err != nil {
        return nil, err
    }

    exporter, err := otlptracegrpc.New(ctx, otlptracegrpc.WithGRPCConn(conn))
    if err != nil {
        return nil, err
    }

    // TracerProvider
    tp := sdktrace.NewTracerProvider(
        sdktrace.WithBatcher(exporter),
        sdktrace.WithResource(res),
        // 让区域 Collector 处理采样
        sdktrace.WithSampler(sdktrace.AlwaysSample()),
    )

    otel.SetTracerProvider(tp)

    // 跨云传播器（支持 W3C Trace Context + Baggage）
    otel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(
        propagation.TraceContext{},
        propagation.Baggage{},
    ))

    return tp, nil
}
```

### 跨云服务调用示例

```go:internal/order/service.go
package order

import (
    "context"
    "fmt"

    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/baggage"
    "go.opentelemetry.io/otel/trace"
)

var tracer = otel.Tracer("order-service")

type Service struct {
    productClient  *ProductClient  // GCP
    inventoryClient *InventoryClient // GCP
    paymentClient  *PaymentClient  // Azure
}

// CreateOrder 创建订单（跨云调用）
func (s *Service) CreateOrder(ctx context.Context, req *CreateOrderRequest) (*Order, error) {
    ctx, span := tracer.Start(ctx, "order.create",
        trace.WithAttributes(
            attribute.String("order.id", req.OrderID),
            attribute.String("user.id", req.UserID),
        ),
    )
    defer span.End()

    // 添加 Baggage（跨云传播用户信息）
    member, _ := baggage.NewMember("user.id", req.UserID)
    bag, _ := baggage.New(member)
    ctx = baggage.ContextWithBaggage(ctx, bag)

    // 1. 调用产品服务（GCP）
    span.AddEvent("calling product service")
    product, err := s.productClient.GetProduct(ctx, req.ProductID)
    if err != nil {
        span.RecordError(err)
        return nil, fmt.Errorf("get product failed: %w", err)
    }
    span.SetAttributes(attribute.Float64("product.price", product.Price))

    // 2. 调用库存服务（GCP）
    span.AddEvent("calling inventory service")
    if err := s.inventoryClient.Deduct(ctx, req.ProductID, req.Quantity); err != nil {
        span.RecordError(err)
        return nil, fmt.Errorf("deduct inventory failed: %w", err)
    }

    // 3. 调用支付服务（Azure）
    span.AddEvent("calling payment service")
    payment, err := s.paymentClient.Process(ctx, &PaymentRequest{
        UserID: req.UserID,
        Amount: product.Price * float64(req.Quantity),
    })
    if err != nil {
        // 回滚库存
        s.inventoryClient.Refund(ctx, req.ProductID, req.Quantity)
        span.RecordError(err)
        return nil, fmt.Errorf("payment failed: %w", err)
    }

    span.SetAttributes(attribute.String("payment.id", payment.PaymentID))
    span.AddEvent("order created successfully")

    return &Order{
        OrderID:   req.OrderID,
        Status:    "confirmed",
        PaymentID: payment.PaymentID,
    }, nil
}
```

### 跨云 HTTP 客户端

```go:pkg/client/http.go
package client

import (
    "net/http"

    "go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp"
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/propagation"
)

// NewHTTPClient 创建跨云 HTTP 客户端（自动传播上下文）
func NewHTTPClient() *http.Client {
    return &http.Client{
        Transport: otelhttp.NewTransport(
            http.DefaultTransport,
            otelhttp.WithPropagators(otel.GetTextMapPropagator()),
        ),
    }
}
```

---

## 成本优化

### 1. 采样策略

```yaml
processors:
  # Head Sampling（头部采样，降低传输成本）
  probabilistic_sampler:
    sampling_percentage: 10  # 10% 采样

  # Tail Sampling（尾部采样，保留重要 trace）
  tail_sampling:
    policies:
      - name: errors
        type: status_code
        status_code:
          status_codes: [ERROR]
      - name: slow
        type: latency
        latency:
          threshold_ms: 1000
      - name: random
        type: probabilistic
        probabilistic:
          sampling_percentage: 1  # 剩余 1% 随机
```

### 2. 数据归档

```yaml
exporters:
  # S3 归档（低成本长期存储）
  s3:
    region: us-east-1
    s3_bucket: otel-archive
    s3_prefix: traces/${cloud.provider}/${cloud.region}
    s3_partition: hour
    compression: gzip
    
  # 生命周期策略（在 S3 侧配置）
  # - 30天后转为 Glacier
  # - 90天后删除
```

### 3. 成本监控

```promql
# 每小时数据量
sum(rate(otelcol_exporter_sent_spans[1h])) * 3600

# 每月预估成本（假设 $0.50/GB）
(sum(rate(otelcol_exporter_sent_spans[30d])) * 30 * 86400 * 0.001) * 0.50

# 按云平台分组
sum(rate(otelcol_exporter_sent_spans[1h])) by (cloud_provider)
```

---

## 灾备切换

### 1. 双中心配置

```yaml:deployments/central/otel-collector-config.yaml
exporters:
  # 主中心（AWS）
  otlp/primary:
    endpoint: primary-collector.aws.example.com:4317
    retry_on_failure:
      enabled: true
      max_elapsed_time: 60s

  # 备份中心（GCP）
  otlp/backup:
    endpoint: backup-collector.gcp.example.com:4317

  # 故障转移导出器
  loadbalancing:
    protocol:
      otlp:
        timeout: 1s
    resolver:
      static:
        hostnames:
          - primary-collector.aws.example.com:4317
          - backup-collector.gcp.example.com:4317

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [loadbalancing]  # 自动负载均衡 + 故障转移
```

### 2. 健康检查

```go:internal/health/check.go
package health

import (
    "context"
    "time"

    "google.golang.org/grpc"
    "google.golang.org/grpc/health/grpc_health_v1"
)

// CheckCollectorHealth 检查 Collector 健康状态
func CheckCollectorHealth(endpoint string) error {
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()

    conn, err := grpc.DialContext(ctx, endpoint, grpc.WithInsecure())
    if err != nil {
        return err
    }
    defer conn.Close()

    client := grpc_health_v1.NewHealthClient(conn)
    resp, err := client.Check(ctx, &grpc_health_v1.HealthCheckRequest{})
    if err != nil {
        return err
    }

    if resp.Status != grpc_health_v1.HealthCheckResponse_SERVING {
        return fmt.Errorf("collector not serving")
    }

    return nil
}
```

---

## Terraform 自动化部署

### AWS EKS

```hcl:terraform/aws/main.tf
provider "aws" {
  region = "us-east-1"
}

# EKS Cluster
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 19.0"

  cluster_name    = "prod-cluster"
  cluster_version = "1.28"

  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnets

  eks_managed_node_groups = {
    main = {
      desired_size = 3
      min_size     = 2
      max_size     = 5
      instance_types = ["t3.large"]
    }
  }
}

# OTLP Collector Namespace
resource "kubernetes_namespace" "observability" {
  metadata {
    name = "observability"
  }
}

# OTLP Collector DaemonSet
resource "kubernetes_daemonset" "otel_collector" {
  metadata {
    name      = "otel-collector"
    namespace = kubernetes_namespace.observability.metadata[0].name
  }

  spec {
    selector {
      match_labels = {
        app = "otel-collector"
      }
    }

    template {
      metadata {
        labels = {
          app = "otel-collector"
        }
      }

      spec {
        service_account_name = kubernetes_service_account.otel_collector.metadata[0].name

        container {
          name  = "otel-collector"
          image = "otel/opentelemetry-collector-contrib:0.96.0"

          port {
            container_port = 4317
          }

          env {
            name  = "AWS_REGION"
            value = "us-east-1"
          }

          env {
            name = "K8S_NODE_NAME"
            value_from {
              field_ref {
                field_path = "spec.nodeName"
              }
            }
          }

          resources {
            requests = {
              memory = "512Mi"
              cpu    = "500m"
            }
            limits = {
              memory = "1Gi"
              cpu    = "1000m"
            }
          }
        }
      }
    }
  }
}
```

---

## 最佳实践总结

### 1. 区域就近采集

✅ **DO**:

- 每个云/区域部署 DaemonSet Collector
- 本地预处理（采样、过滤、富化）
- 异步批量传输到中心

❌ **DON'T**:

- 不要让应用直接连接跨云 Collector（延迟高）
- 不要在区域 Collector 做重处理（影响性能）

### 2. 统一资源属性

✅ **DO**:

- 使用 Resource Detection Processor
- 标准化云平台属性（`cloud.provider`, `cloud.region`）
- 添加 Kubernetes 元数据

### 3. 成本优化

✅ **DO**:

- 采样策略（Head + Tail Sampling）
- 数据归档（S3 + Glacier）
- 监控数据量与成本

### 4. 灾备

✅ **DO**:

- 双中心部署
- 负载均衡 + 故障转移
- 定期灾备演练

---

## 🎯 学到的关键知识点

1. ✅ **多云架构**: 三层架构（应用层、区域层、中心层）
2. ✅ **跨云追踪**: W3C Trace Context + Baggage 跨云传播
3. ✅ **资源检测**: AWS/GCP/Azure 自动检测与富化
4. ✅ **成本优化**: 采样策略 + 数据归档
5. ✅ **灾备**: 双中心 + 负载均衡 + 故障转移
6. ✅ **自动化**: Terraform 一键部署

---

**🎊 恭喜！** 您已经掌握了多云环境 OpenTelemetry 的完整部署方案！

**下一步**:

- 📚 [配置模板库](../配置模板/)
