# 77. OTLP故障排查与诊断完全指南（2025版）

> **适用版本**: Go 1.25.1 + OpenTelemetry v1.32.0  
> **完成日期**: 2025-10-11  
> **状态**: ✅ 生产验证

---

## 📋 目录

- [1. 常见问题FAQ](#1-常见问题faq)
- [2. 追踪问题排查流程](#2-追踪问题排查流程)
- [3. 性能问题诊断](#3-性能问题诊断)
- [4. 生产环境诊断工具](#4-生产环境诊断工具)
- [5. 监控告警配置](#5-监控告警配置)

---

## 1. 常见问题FAQ

### 1.1 初始化问题

#### Q1: 程序启动后没有看到任何Trace

**症状**:
```bash
# Jaeger UI: 没有任何Trace数据
# 日志: 无错误信息
```

**排查步骤**:

```go
// 1. 检查TracerProvider是否正确初始化
tp, err := initTracing(ctx, "localhost:4317")
if err != nil {
    log.Fatal(err)  // 确保不会静默失败
}
defer tp.Shutdown(ctx)  // ⚠️ 必须调用Shutdown

// 2. 验证Tracer是否正确获取
tracer := otel.Tracer("my-service")  // ⚠️ 必须在SetTracerProvider之后

// 3. 确认Span被正确创建和结束
ctx, span := tracer.Start(ctx, "operation")
defer span.End()  // ⚠️ 必须调用End()

// 4. 检查采样器配置
// ❌ 错误: 采样率为0
tp := sdktrace.NewTracerProvider(
    sdktrace.WithSampler(sdktrace.NeverSample()),  // 永不采样
)

// ✅ 正确: 使用AlwaysSample或合理的采样率
tp := sdktrace.NewTracerProvider(
    sdktrace.WithSampler(sdktrace.AlwaysSample()),  // 100%采样 (测试环境)
    // 或
    sdktrace.WithSampler(sdktrace.TraceIDRatioBased(0.1)),  // 10%采样 (生产环境)
)
```

**常见原因**:

| 原因 | 解决方案 |
|-----|---------|
| ❌ 忘记调用`Shutdown()` | 程序退出前调用`tp.Shutdown(ctx)` |
| ❌ 采样率为0 | 使用`AlwaysSample()`或合理采样率 |
| ❌ Exporter未初始化 | 检查`otlptracegrpc.New()`是否成功 |
| ❌ Collector未启动 | `docker-compose up otel-collector` |

**快速验证**:

```bash
# 1. 检查Collector是否可达
nc -zv localhost 4317

# 2. 查看Collector日志
docker logs otel-collector

# 3. 使用环境变量启用调试日志
export OTEL_LOG_LEVEL=debug
go run main.go
```

---

#### Q2: Trace上报后在Collector中丢失

**症状**:
```bash
# 客户端日志: 成功发送
2025-10-11T10:00:00Z INFO Successfully exported 100 spans

# Jaeger UI: 仍然没有数据
```

**排查步骤**:

```yaml
# 1. 检查Collector配置
# otel-collector-config.yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317  # ✅ 监听所有接口

processors:
  batch:
    timeout: 1s
    send_batch_size: 100

exporters:
  jaeger:
    endpoint: jaeger:14250  # ⚠️ 检查Jaeger地址是否正确
    tls:
      insecure: true

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [jaeger]  # ⚠️ 确保exporters拼写正确
```

**调试命令**:

```bash
# 2. 查看Collector指标
curl http://localhost:8888/metrics | grep -E "(otelcol_receiver|otelcol_exporter)"

# 关键指标:
# otelcol_receiver_accepted_spans{} - 接收的Span数
# otelcol_exporter_sent_spans{} - 发送的Span数
# otelcol_exporter_send_failed_spans{} - 发送失败的Span数

# 3. 检查Collector健康状态
curl http://localhost:13133/
```

**常见原因**:

| 原因 | 解决方案 |
|-----|---------|
| ❌ Jaeger地址错误 | 使用`docker-compose`网络内部地址 |
| ❌ Pipeline配置错误 | 检查`receivers/processors/exporters`拼写 |
| ❌ Batch处理器超时 | 调整`timeout`参数 (如: 1s) |
| ❌ TLS配置问题 | 使用`insecure: true` (测试环境) |

---

#### Q3: Context传播失败 (分布式追踪断链)

**症状**:
```text
Service A (TraceID: abc123)
  ↓
Service B (TraceID: def456)  ❌ 新的TraceID, 追踪断链
```

**排查步骤**:

```go
// 1. 检查Propagator是否正确配置
// ❌ 错误: 未设置Propagator
// (默认使用NoOp Propagator, 不传播Context)

// ✅ 正确: 设置W3C TraceContext
otel.SetTextMapPropagator(
    propagation.NewCompositeTextMapPropagator(
        propagation.TraceContext{},  // W3C标准
        propagation.Baggage{},       // Baggage传播
    ),
)

// 2. HTTP客户端: 必须注入Context
func makeHTTPRequest(ctx context.Context, url string) (*http.Response, error) {
    req, err := http.NewRequestWithContext(ctx, "GET", url, nil)
    if err != nil {
        return nil, err
    }

    // ✅ 注入TraceContext到HTTP Header
    otel.GetTextMapPropagator().Inject(ctx, propagation.HeaderCarrier(req.Header))

    client := &http.Client{
        Transport: otelhttp.NewTransport(http.DefaultTransport),  // ✅ 使用instrumented transport
    }
    return client.Do(req)
}

// 3. HTTP服务端: 必须提取Context
func httpHandler(w http.ResponseWriter, r *http.Request) {
    // ✅ 提取TraceContext
    ctx := otel.GetTextMapPropagator().Extract(r.Context(),
        propagation.HeaderCarrier(r.Header))

    // 使用提取的Context
    tracer := otel.Tracer("my-service")
    ctx, span := tracer.Start(ctx, "handle-request")
    defer span.End()

    // ... 业务逻辑
}

// 4. gRPC: 使用官方拦截器
import "go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc"

// 客户端
conn, err := grpc.Dial(address,
    grpc.WithStatsHandler(otelgrpc.NewClientHandler()),  // ✅ 自动传播
)

// 服务端
server := grpc.NewServer(
    grpc.StatsHandler(otelgrpc.NewServerHandler()),  // ✅ 自动提取
)
```

**验证传播**:

```go
// 验证HTTP Header是否包含traceparent
func verifyPropagation(r *http.Request) {
    traceparent := r.Header.Get("traceparent")
    if traceparent == "" {
        log.Error("Missing traceparent header!")
        return
    }
    
    log.Printf("traceparent: %s", traceparent)
    // 格式: 00-{trace-id}-{span-id}-{trace-flags}
    // 示例: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01
}
```

**常见原因**:

| 原因 | 解决方案 |
|-----|---------|
| ❌ 未设置Propagator | 调用`otel.SetTextMapPropagator()` |
| ❌ HTTP客户端未注入 | 使用`otelhttp.NewTransport()` |
| ❌ HTTP服务端未提取 | 使用`otelhttp.NewHandler()` |
| ❌ gRPC未使用拦截器 | 使用`otelgrpc.NewClientHandler()` |

---

### 1.2 性能问题

#### Q4: 启用OTLP后性能大幅下降

**症状**:
```text
无OTLP: QPS=10000, P99=50ms
启用OTLP: QPS=3000, P99=180ms  ❌ 性能下降70%
```

**排查步骤**:

```go
// 1. 检查是否使用了同步Exporter
// ❌ 错误: 使用SimpleSpanProcessor (同步导出)
tp := sdktrace.NewTracerProvider(
    sdktrace.WithSpanProcessor(
        sdktrace.NewSimpleSpanProcessor(exporter),  // 每个Span立即导出
    ),
)

// ✅ 正确: 使用BatchSpanProcessor (批量异步导出)
tp := sdktrace.NewTracerProvider(
    sdktrace.WithBatcher(exporter,
        sdktrace.WithMaxQueueSize(10000),      // 大队列
        sdktrace.WithBatchTimeout(5*time.Second),  // 5秒批量
        sdktrace.WithMaxExportBatchSize(512),  // 每批512个
    ),
)

// 2. 检查采样率
// ❌ 错误: 100%采样 (高QPS场景)
tp := sdktrace.NewTracerProvider(
    sdktrace.WithSampler(sdktrace.AlwaysSample()),  // 所有请求都追踪
)

// ✅ 正确: 合理采样率
tp := sdktrace.NewTracerProvider(
    sdktrace.WithSampler(sdktrace.TraceIDRatioBased(0.01)),  // 1%采样
)

// 3. 检查Span属性数量
// ❌ 错误: 添加大量属性
span.SetAttributes(
    attribute.String("user.id", userID),
    attribute.String("user.name", userName),
    attribute.String("user.email", userEmail),
    attribute.String("user.phone", userPhone),
    // ... 100+ 个属性
)

// ✅ 正确: 只添加关键属性
span.SetAttributes(
    attribute.String("user.id", userID),
    attribute.String("operation", "checkout"),
)

// 4. 检查是否启用了Resource检测器
// ❌ 错误: 使用所有检测器
resource.New(ctx,
    resource.WithFromEnv(),
    resource.WithHost(),
    resource.WithOS(),
    resource.WithProcess(),
    resource.WithContainer(),
    resource.WithOSType(),
    // ... 启动耗时5-10秒
)

// ✅ 正确: 只使用必要的检测器
resource.New(ctx,
    resource.WithAttributes(
        semconv.ServiceName("my-service"),
        semconv.ServiceVersion("1.0.0"),
    ),
)
```

**性能优化清单**:

| 优化项 | 影响 | 实现 |
|-------|------|------|
| ✅ BatchSpanProcessor | -90% 延迟 | `WithBatcher()` |
| ✅ 合理采样率 | -95% CPU | `TraceIDRatioBased(0.01)` |
| ✅ 减少Attributes | -50% 内存 | 只保留关键属性 |
| ✅ 异步导出 | -99% 阻塞 | `NonBlocking` |
| ✅ 连接池 | -30% 网络 | gRPC连接复用 |

---

#### Q5: 内存持续增长 (OOM)

**症状**:
```bash
# 程序运行2小时后
FATAL: runtime: out of memory
```

**排查步骤**:

```bash
# 1. 启用pprof
import _ "net/http/pprof"
go func() {
    http.ListenAndServe(":6060", nil)
}()

# 2. 获取heap profile
curl http://localhost:6060/debug/pprof/heap > heap.prof

# 3. 分析内存分配
go tool pprof -http=:8080 heap.prof

# 关键指标:
# - BatchSpanProcessor的队列是否无限增长
# - Span对象是否泄漏 (未调用End())
# - Context泄漏
```

**常见原因与解决方案**:

```go
// 原因1: Span未调用End()
// ❌ 错误
ctx, span := tracer.Start(ctx, "operation")
if err != nil {
    return err  // 忘记调用span.End()
}
span.End()

// ✅ 正确: 使用defer
ctx, span := tracer.Start(ctx, "operation")
defer span.End()  // 无论如何都会调用

// 原因2: BatchProcessor队列满了
// ❌ 错误: Collector宕机, 队列持续积压
tp := sdktrace.NewTracerProvider(
    sdktrace.WithBatcher(exporter,
        sdktrace.WithMaxQueueSize(1000000),  // 100万, 占用大量内存
    ),
)

// ✅ 正确: 合理队列大小 + 溢出策略
tp := sdktrace.NewTracerProvider(
    sdktrace.WithBatcher(exporter,
        sdktrace.WithMaxQueueSize(10000),  // 1万
        sdktrace.WithBlocking(),  // 队列满时阻塞 (或直接丢弃)
    ),
)

// 原因3: Context泄漏
// ❌ 错误: Goroutine持有Context
go func() {
    time.Sleep(1 * time.Hour)
    processData(ctx)  // ctx永不释放
}()

// ✅ 正确: 使用独立Context
go func() {
    newCtx := context.Background()
    newCtx = trace.ContextWithSpanContext(newCtx, trace.SpanContextFromContext(ctx))
    processData(newCtx)
}()
```

---

### 1.3 配置问题

#### Q6: 环境变量不生效

**症状**:
```bash
export OTEL_EXPORTER_OTLP_ENDPOINT=https://collector.example.com:4317
# 但程序仍连接localhost:4317
```

**原因**: 环境变量优先级低于代码配置

```go
// ❌ 代码显式指定, 环境变量被忽略
exporter, _ := otlptracegrpc.New(ctx,
    otlptracegrpc.WithEndpoint("localhost:4317"),  // 硬编码
    otlptracegrpc.WithInsecure(),
)

// ✅ 正确: 不指定Endpoint, 自动读取环境变量
exporter, _ := otlptracegrpc.New(ctx)  // 使用OTEL_EXPORTER_OTLP_ENDPOINT
```

**环境变量列表**:

| 环境变量 | 默认值 | 说明 |
|---------|-------|------|
| `OTEL_EXPORTER_OTLP_ENDPOINT` | `localhost:4317` | Collector地址 |
| `OTEL_SERVICE_NAME` | `unknown_service:go` | 服务名 |
| `OTEL_TRACES_SAMPLER` | `parentbased_always_on` | 采样器 |
| `OTEL_TRACES_SAMPLER_ARG` | - | 采样率 (如: `0.1`) |
| `OTEL_LOG_LEVEL` | `info` | 日志级别 |

---

## 2. 追踪问题排查流程

### 2.1 Span丢失诊断

#### 流程图

```text
Span丢失
   ↓
1. 客户端是否创建Span?
   ├─ 否 → 检查Tracer初始化
   └─ 是 → 继续
      ↓
2. 客户端是否调用End()?
   ├─ 否 → 添加defer span.End()
   └─ 是 → 继续
      ↓
3. 采样器是否丢弃?
   ├─ 是 → 调整采样率
   └─ 否 → 继续
      ↓
4. Exporter是否成功导出?
   ├─ 否 → 检查Collector连接
   └─ 是 → 继续
      ↓
5. Collector是否收到?
   ├─ 否 → 检查网络/防火墙
   └─ 是 → 继续
      ↓
6. Collector是否成功导出到Backend?
   ├─ 否 → 检查Jaeger配置
   └─ 是 → 检查Backend存储
```

#### 诊断工具

```bash
# 1. 检查客户端日志 (启用debug)
export OTEL_LOG_LEVEL=debug
go run main.go 2>&1 | grep -i "span"

# 2. 抓包分析
tcpdump -i lo -A 'port 4317'

# 3. 检查Collector接收
curl http://localhost:8888/metrics | grep otelcol_receiver_accepted_spans

# 4. 检查Collector导出
curl http://localhost:8888/metrics | grep otelcol_exporter_sent_spans

# 5. 检查Jaeger
curl http://localhost:16686/api/traces?service=my-service&limit=10
```

### 2.2 TraceID查询技巧

```bash
# 1. 从日志中提取TraceID
# 假设日志格式: [2025-10-11T10:00:00Z] [INFO] [trace_id=abc123] Processing request

grep "Processing request" app.log | sed 's/.*trace_id=\([a-z0-9]*\).*/\1/'

# 2. 在Jaeger中查询
curl "http://localhost:16686/api/traces/abc123"

# 3. 使用TraceID关联多个服务的日志
# Service A
grep "trace_id=abc123" service-a.log

# Service B
grep "trace_id=abc123" service-b.log

# Service C
grep "trace_id=abc123" service-c.log
```

### 2.3 Span关系验证

```go
// 验证父子Span关系
func verifySpanRelationship(spans []trace.ReadOnlySpan) {
    for _, span := range spans {
        parentSpanID := span.Parent().SpanID()
        if !parentSpanID.IsValid() {
            log.Printf("Root Span: %s", span.Name())
        } else {
            log.Printf("Child Span: %s (Parent: %s)", span.Name(), parentSpanID)
        }
    }
}

// 示例输出:
// Root Span: HTTP GET /api/users/123
// Child Span: db.query (Parent: 00f067aa0ba902b7)
// Child Span: cache.get (Parent: 00f067aa0ba902b7)
```

---

## 3. 性能问题诊断

### 3.1 高延迟定位

#### 3.1.1 使用Jaeger UI分析

```text
# 1. 打开Jaeger UI
http://localhost:16686

# 2. 搜索慢请求
Service: my-service
Operation: HTTP GET /api/users/{id}
Min Duration: 1s  # 找出延迟>1s的请求

# 3. 查看Trace详情
- 总耗时: 1.2s
  - HTTP GET /api/users/123 (1.2s)
    - db.query (800ms)  ← 瓶颈
    - cache.get (50ms)
    - redis.set (20ms)

# 4. 定位问题
- db.query耗时800ms → 检查SQL索引
```

#### 3.1.2 自动化分析脚本

```go
package main

import (
    "context"
    "fmt"
    "sort"
    "time"

    sdktrace "go.opentelemetry.io/otel/sdk/trace"
)

// AnalyzeSlowSpans 分析慢Span
func AnalyzeSlowSpans(spans []sdktrace.ReadOnlySpan, threshold time.Duration) {
    type SpanDuration struct {
        Name     string
        Duration time.Duration
    }

    var slowSpans []SpanDuration
    for _, span := range spans {
        duration := span.EndTime().Sub(span.StartTime())
        if duration > threshold {
            slowSpans = append(slowSpans, SpanDuration{
                Name:     span.Name(),
                Duration: duration,
            })
        }
    }

    // 按耗时排序
    sort.Slice(slowSpans, func(i, j int) bool {
        return slowSpans[i].Duration > slowSpans[j].Duration
    })

    // 输出Top 10
    fmt.Println("=== Top 10 Slow Spans ===")
    for i, span := range slowSpans {
        if i >= 10 {
            break
        }
        fmt.Printf("%d. %s: %v\n", i+1, span.Name, span.Duration)
    }
}

// 使用示例
func main() {
    // 从Jaeger API获取spans
    spans := fetchSpansFromJaeger("my-service", time.Now().Add(-1*time.Hour), time.Now())
    
    // 分析延迟>500ms的Span
    AnalyzeSlowSpans(spans, 500*time.Millisecond)
}
```

### 3.2 高内存占用定位

```bash
# 1. 获取heap profile
curl http://localhost:6060/debug/pprof/heap > heap.prof

# 2. 分析Top分配
go tool pprof -top heap.prof

# 示例输出:
#   flat  flat%   sum%        cum   cum%
#  512MB 45.71% 45.71%     512MB 45.71%  BatchSpanProcessor.enqueue
#  256MB 22.86% 68.57%     256MB 22.86%  Span.SetAttributes
#  128MB 11.43% 80.00%     128MB 11.43%  Context.WithValue

# 3. 查看调用栈
go tool pprof -http=:8080 heap.prof
# 浏览器打开 http://localhost:8080
# View → Flame Graph → 找到占用内存最多的路径
```

**优化建议**:

```go
// 1. 使用对象池
var spanPool = sync.Pool{
    New: func() interface{} {
        return &Span{}
    },
}

func getSpan() *Span {
    return spanPool.Get().(*Span)
}

func putSpan(s *Span) {
    s.Reset()
    spanPool.Put(s)
}

// 2. 限制Attributes数量
const maxAttributes = 20

func setAttributes(span trace.Span, attrs []attribute.KeyValue) {
    if len(attrs) > maxAttributes {
        attrs = attrs[:maxAttributes]
    }
    span.SetAttributes(attrs...)
}

// 3. 使用流式处理 (避免大批量accumulate)
processor := NewStreamingProcessor(exporter)
```

### 3.3 高CPU占用定位

```bash
# 1. 获取CPU profile
curl http://localhost:6060/debug/pprof/profile?seconds=30 > cpu.prof

# 2. 分析热点函数
go tool pprof -top cpu.prof

# 示例输出:
#   flat  flat%   sum%        cum   cum%
#  12.5s 45.45% 45.45%     15.2s 55.27%  BatchSpanProcessor.exportSpans
#   5.2s 18.91% 64.36%      5.2s 18.91%  proto.Marshal
#   3.1s 11.27% 75.63%      3.1s 11.27%  gzip.compress

# 3. 查看火焰图
go tool pprof -http=:8080 cpu.prof
```

**优化建议**:

```go
// 1. 减少序列化次数
// ❌ 错误: 每个Span单独序列化
for _, span := range spans {
    data := proto.Marshal(span)
    send(data)
}

// ✅ 正确: 批量序列化
batch := &tracepb.TracesData{Spans: spans}
data := proto.Marshal(batch)
send(data)

// 2. 禁用压缩 (如果带宽充足)
exporter, _ := otlptracegrpc.New(ctx,
    otlptracegrpc.WithCompressor(""),  // 禁用gzip
)

// 3. 使用更快的序列化库
import "github.com/gogo/protobuf/proto"  // 比标准protobuf快30%
```

---

## 4. 生产环境诊断工具

### 4.1 实时监控Dashboard

```yaml
# Grafana Dashboard配置
# prometheus.yml
scrape_configs:
  - job_name: 'otel-sdk'
    static_configs:
      - targets: ['app:8888']  # 应用暴露的metrics端点

# 关键指标
- otelcol_receiver_accepted_spans  # 接收的Span数
- otelcol_exporter_sent_spans      # 发送的Span数
- otelcol_exporter_send_failed_spans  # 失败的Span数
- otelcol_processor_batch_batch_send_size  # 批量大小
- process_cpu_seconds_total  # CPU使用
- process_resident_memory_bytes  # 内存使用
```

### 4.2 自动化健康检查

```go
package main

import (
    "context"
    "fmt"
    "net/http"
    "time"

    "go.opentelemetry.io/otel"
)

// HealthChecker OTLP健康检查
type HealthChecker struct {
    tp           *sdktrace.TracerProvider
    lastSpanTime time.Time
}

// Check 健康检查
func (hc *HealthChecker) Check(w http.ResponseWriter, r *http.Request) {
    ctx := r.Context()

    // 1. 检查TracerProvider
    if hc.tp == nil {
        http.Error(w, "TracerProvider not initialized", 503)
        return
    }

    // 2. 测试创建Span
    tracer := otel.Tracer("health-check")
    ctx, span := tracer.Start(ctx, "health-check")
    span.End()

    // 3. 检查最近是否有Span导出
    if time.Since(hc.lastSpanTime) > 5*time.Minute {
        http.Error(w, "No spans exported in last 5 minutes", 503)
        return
    }

    // 4. 检查Collector连接
    if err := hc.checkCollectorConnection(ctx); err != nil {
        http.Error(w, fmt.Sprintf("Collector unreachable: %v", err), 503)
        return
    }

    w.WriteHeader(200)
    w.Write([]byte("OK"))
}

func (hc *HealthChecker) checkCollectorConnection(ctx context.Context) error {
    // 尝试连接Collector
    // (简化实现)
    return nil
}

// 使用示例
func main() {
    hc := &HealthChecker{tp: initTracing()}
    http.HandleFunc("/health/otlp", hc.Check)
    http.ListenAndServe(":8080", nil)
}
```

### 4.3 Trace质量监控

```go
// TraceQualityMonitor 追踪质量监控
type TraceQualityMonitor struct {
    meter metric.Meter
}

func (m *TraceQualityMonitor) MonitorTrace(spans []trace.ReadOnlySpan) {
    // 1. 检查Span完整性
    for _, span := range spans {
        if !span.EndTime().After(span.StartTime()) {
            m.recordAnomaly("invalid_span_time")
        }

        if span.Name() == "" {
            m.recordAnomaly("empty_span_name")
        }

        // 检查必需属性
        hasServiceName := false
        for _, attr := range span.Resource().Attributes() {
            if attr.Key == "service.name" {
                hasServiceName = true
                break
            }
        }
        if !hasServiceName {
            m.recordAnomaly("missing_service_name")
        }
    }

    // 2. 检查Trace深度
    depth := calculateTraceDepth(spans)
    if depth > 20 {
        m.recordAnomaly("trace_too_deep")
    }

    // 3. 检查Span数量
    if len(spans) > 1000 {
        m.recordAnomaly("too_many_spans")
    }
}

func (m *TraceQualityMonitor) recordAnomaly(anomalyType string) {
    counter, _ := m.meter.Int64Counter("trace.anomalies")
    counter.Add(context.Background(), 1,
        metric.WithAttributes(attribute.String("type", anomalyType)))
}
```

---

## 5. 监控告警配置

### 5.1 Prometheus告警规则

```yaml
# prometheus-alerts.yml
groups:
  - name: otlp-alerts
    rules:
      # 1. Span导出失败率过高
      - alert: HighSpanExportFailureRate
        expr: |
          rate(otelcol_exporter_send_failed_spans[5m])
          / rate(otelcol_exporter_sent_spans[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High span export failure rate (>5%)"
          description: "{{ $value | humanizePercentage }} spans failed to export"

      # 2. Collector队列积压
      - alert: CollectorQueueBacklog
        expr: otelcol_processor_batch_batch_send_size_bucket > 5000
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Collector queue backlog"

      # 3. 应用OTLP延迟过高
      - alert: HighOTLPOverhead
        expr: |
          histogram_quantile(0.99,
            rate(http_request_duration_with_otlp_seconds_bucket[5m]))
          / histogram_quantile(0.99,
            rate(http_request_duration_without_otlp_seconds_bucket[5m]))
          > 1.2
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: "OTLP overhead > 20%"

      # 4. Trace数据丢失
      - alert: TraceDataLoss
        expr: |
          rate(otelcol_receiver_accepted_spans[5m])
          - rate(otelcol_exporter_sent_spans[5m]) > 1000
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Trace data loss detected"
```

### 5.2 告警通知配置

```yaml
# alertmanager.yml
route:
  receiver: 'default'
  group_by: ['alertname', 'severity']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h

  routes:
    - match:
        severity: critical
      receiver: 'pagerduty'
    
    - match:
        severity: warning
      receiver: 'slack'

receivers:
  - name: 'default'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/xxx'
        channel: '#otlp-alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'xxx'
```

---

## 总结

### 快速排查清单

| 问题类型 | 检查项 | 工具 |
|---------|-------|------|
| **Trace丢失** | 采样率、Exporter、Collector配置 | Jaeger UI, Collector logs |
| **Context断链** | Propagator、HTTP/gRPC拦截器 | curl, tcpdump |
| **性能下降** | BatchProcessor、采样率、Attributes | pprof, benchmarks |
| **内存泄漏** | Span.End()、队列大小、Context泄漏 | pprof heap |
| **CPU过高** | 序列化、压缩、批量大小 | pprof cpu |

### 推荐实践

✅ **日志关联**: 在日志中记录TraceID  
✅ **健康检查**: 定期验证OTLP连通性  
✅ **监控告警**: 配置关键指标告警  
✅ **定期审计**: 每月review Trace质量  
✅ **文档更新**: 记录故障处理过程

---

**版本**: v1.0.0  
**完成日期**: 2025-10-11  
**下次更新**: 根据用户反馈持续优化

