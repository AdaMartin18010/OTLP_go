# 74. å®æˆ˜æ¡ˆä¾‹ï¼šäº‘åŸç”Ÿé«˜å¹¶å‘ç³»ç»Ÿå®Œæ•´æ¶æ„ï¼ˆ2025ç‰ˆï¼‰

> **åœºæ™¯**: è§†é¢‘ç›´æ’­å¹³å° - æ—¥æ´»åƒä¸‡çº§ã€å³°å€¼QPS 10ä¸‡+  
> **æŠ€æœ¯æ ˆ**: Go 1.25.1 + K8s + Istio + Redis + Kafka + OTLP  
> **å®Œæˆæ—¥æœŸ**: 2025-10-11

---

## ğŸ“‹ ç›®å½•

- [74. å®æˆ˜æ¡ˆä¾‹ï¼šäº‘åŸç”Ÿé«˜å¹¶å‘ç³»ç»Ÿå®Œæ•´æ¶æ„ï¼ˆ2025ç‰ˆï¼‰](#74-å®æˆ˜æ¡ˆä¾‹äº‘åŸç”Ÿé«˜å¹¶å‘ç³»ç»Ÿå®Œæ•´æ¶æ„2025ç‰ˆ)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. ç³»ç»Ÿæ¦‚è¿°](#1-ç³»ç»Ÿæ¦‚è¿°)
    - [1.1 ä¸šåŠ¡åœºæ™¯](#11-ä¸šåŠ¡åœºæ™¯)
    - [1.2 æŠ€æœ¯æŒ‘æˆ˜](#12-æŠ€æœ¯æŒ‘æˆ˜)
    - [1.3 ç³»ç»ŸæŒ‡æ ‡](#13-ç³»ç»ŸæŒ‡æ ‡)
  - [2. æ¶æ„è®¾è®¡](#2-æ¶æ„è®¾è®¡)
    - [2.1 æ•´ä½“æ¶æ„](#21-æ•´ä½“æ¶æ„)
    - [2.2 æœåŠ¡èŒè´£](#22-æœåŠ¡èŒè´£)
    - [2.3 æ•°æ®æµè®¾è®¡](#23-æ•°æ®æµè®¾è®¡)
  - [3. æ ¸å¿ƒæœåŠ¡å®ç°](#3-æ ¸å¿ƒæœåŠ¡å®ç°)
    - [3.1 ç›´æ’­é—´æœåŠ¡](#31-ç›´æ’­é—´æœåŠ¡)
    - [3.2 æ¶ˆæ¯æœåŠ¡ï¼ˆWebSocketï¼‰](#32-æ¶ˆæ¯æœåŠ¡websocket)
  - [4. é«˜å¹¶å‘ä¼˜åŒ–](#4-é«˜å¹¶å‘ä¼˜åŒ–)
    - [4.1 è¿æ¥æ± ä¼˜åŒ–](#41-è¿æ¥æ± ä¼˜åŒ–)
    - [4.2 æœ¬åœ°ç¼“å­˜](#42-æœ¬åœ°ç¼“å­˜)
    - [4.3 æ‰¹é‡å¤„ç†](#43-æ‰¹é‡å¤„ç†)
  - [5. OTLPå®Œæ•´é›†æˆ](#5-otlpå®Œæ•´é›†æˆ)
    - [5.1 ç»Ÿä¸€åˆå§‹åŒ–](#51-ç»Ÿä¸€åˆå§‹åŒ–)
  - [6. äº‘åŸç”Ÿéƒ¨ç½²](#6-äº‘åŸç”Ÿéƒ¨ç½²)
    - [6.1 Kubernetes Deployment](#61-kubernetes-deployment)
    - [6.2 Istio é›†æˆ](#62-istio-é›†æˆ)
  - [7. æ€§èƒ½æµ‹è¯•ä¸ä¼˜åŒ–](#7-æ€§èƒ½æµ‹è¯•ä¸ä¼˜åŒ–)
    - [7.1 æ€§èƒ½åŸºå‡†](#71-æ€§èƒ½åŸºå‡†)
    - [7.2 æ€§èƒ½ä¼˜åŒ–æ€»ç»“](#72-æ€§èƒ½ä¼˜åŒ–æ€»ç»“)
  - [8. ç”Ÿäº§è¿ç»´](#8-ç”Ÿäº§è¿ç»´)
    - [8.1 ç›‘æ§å‘Šè­¦](#81-ç›‘æ§å‘Šè­¦)
    - [8.2 æ—¥å¿—èšåˆ](#82-æ—¥å¿—èšåˆ)
  - [9. æ€»ç»“](#9-æ€»ç»“)
    - [9.1 æŠ€æœ¯äº®ç‚¹](#91-æŠ€æœ¯äº®ç‚¹)
    - [9.2 æ¶æ„ç‰¹ç‚¹](#92-æ¶æ„ç‰¹ç‚¹)
    - [9.3 å®æˆ˜ä»·å€¼](#93-å®æˆ˜ä»·å€¼)

---

## 1. ç³»ç»Ÿæ¦‚è¿°

### 1.1 ä¸šåŠ¡åœºæ™¯

**è§†é¢‘ç›´æ’­å¹³å°æ ¸å¿ƒåŠŸèƒ½**:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         è§†é¢‘ç›´æ’­å¹³å°ä¸šåŠ¡æ¶æ„                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                              â”‚
â”‚  ğŸ“º ç›´æ’­é—´æœåŠ¡                               â”‚
â”‚  â”œâ”€ å¼€æ’­/å…³æ’­                                â”‚
â”‚  â”œâ”€ è§‚ä¼—è¿›å‡º                                 â”‚
â”‚  â”œâ”€ å¼¹å¹•æ¶ˆæ¯ï¼ˆ10ä¸‡+ QPSï¼‰                    â”‚
â”‚  â””â”€ ç¤¼ç‰©æ‰“èµ                                 â”‚
â”‚                                              â”‚
â”‚  ğŸ‘¤ ç”¨æˆ·æœåŠ¡                                â”‚
â”‚  â”œâ”€ ç”¨æˆ·è®¤è¯ï¼ˆJWTï¼‰                          â”‚
â”‚  â”œâ”€ åœ¨çº¿çŠ¶æ€ï¼ˆRedisï¼‰                        â”‚
â”‚  â””â”€ ç”¨æˆ·èµ„æ–™                                 â”‚
â”‚                                              â”‚
â”‚  ğŸ’¬ æ¶ˆæ¯æœåŠ¡                                 â”‚
â”‚  â”œâ”€ WebSocket é•¿è¿æ¥ï¼ˆç™¾ä¸‡çº§ï¼‰                â”‚
â”‚  â”œâ”€ æ¶ˆæ¯æ¨é€                                 â”‚
â”‚  â””â”€ æ¶ˆæ¯å­˜å‚¨ï¼ˆKafkaï¼‰                        â”‚
â”‚                                              â”‚
â”‚  ğŸ“Š ç»Ÿè®¡æœåŠ¡                                 â”‚
â”‚  â”œâ”€ å®æ—¶åœ¨çº¿æ•°                               â”‚
â”‚  â”œâ”€ çƒ­åº¦æ’è¡Œ                                 â”‚
â”‚  â””â”€ æ•°æ®èšåˆ                                 â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æŠ€æœ¯æŒ‘æˆ˜

| æŒ‘æˆ˜ | æŒ‡æ ‡ | è§£å†³æ–¹æ¡ˆ |
|------|------|---------|
| **é«˜å¹¶å‘** | å³°å€¼ 10ä¸‡+ QPS | æ°´å¹³æ‰©å±• + ç¼“å­˜ + å¼‚æ­¥å¤„ç† |
| **ä½å»¶è¿Ÿ** | P99 < 50ms | è¿æ¥æ±  + æœ¬åœ°ç¼“å­˜ + æ‰¹é‡å¤„ç† |
| **å¤§è§„æ¨¡è¿æ¥** | ç™¾ä¸‡çº§ WebSocket | è¿æ¥åˆ†ç‰‡ + è´Ÿè½½å‡è¡¡ |
| **å®æ—¶æ€§** | æ¶ˆæ¯å»¶è¿Ÿ < 100ms | Kafka + Redis Pub/Sub |
| **é«˜å¯ç”¨** | 99.99% | å¤šå‰¯æœ¬ + ç†”æ–­ + é™çº§ |

### 1.3 ç³»ç»ŸæŒ‡æ ‡

```text
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            ğŸ“Š ç³»ç»Ÿå…³é”®æŒ‡æ ‡
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ—¥æ´»ç”¨æˆ·:       10,000,000+
å³°å€¼QPS:        100,000+
å¹¶å‘è¿æ¥:       1,000,000+
æ¶ˆæ¯åå:       500,000 msg/s
P99å»¶è¿Ÿ:        < 50ms
ç³»ç»Ÿå¯ç”¨æ€§:     99.99%

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## 2. æ¶æ„è®¾è®¡

### 2.1 æ•´ä½“æ¶æ„

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ç”¨æˆ·å±‚                                â”‚
â”‚  ğŸ“± ç§»åŠ¨ç«¯      ğŸ’» Webç«¯      ğŸ“º TVç«¯                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   CDN / GSLB     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Ingress (Nginx/Envoy)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â””â”€â–ºâ”‚      API Gateway (Istio)             â”‚
            â”‚  â”œâ”€ è®¤è¯/æˆæƒ                         â”‚
            â”‚  â”œâ”€ é™æµ/ç†”æ–­                         â”‚
            â”‚  â”œâ”€ è·¯ç”±/è´Ÿè½½å‡è¡¡                     â”‚
            â”‚  â””â”€ OTLP Tracing                     â”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚              â”‚             â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ ç›´æ’­é—´ â”‚    â”‚  ç”¨æˆ·   â”‚    â”‚  æ¶ˆæ¯  â”‚   â”‚  ç»Ÿè®¡   â”‚
â”‚ æœåŠ¡   â”‚    â”‚  æœåŠ¡   â”‚    â”‚  æœåŠ¡  â”‚   â”‚  æœåŠ¡   â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚             â”‚              â”‚             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚                         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â–¼â”€â”€â”
â”‚ Redis  â”‚   â”‚ MySQL  â”‚   â”‚ Kafka  â”‚   â”‚ S3  â”‚
â”‚ Clusterâ”‚   â”‚ Master â”‚   â”‚ Clusterâ”‚   â”‚Storeâ”‚
â”‚        â”‚   â”‚ + Slaveâ”‚   â”‚        â”‚   â”‚     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜
```

### 2.2 æœåŠ¡èŒè´£

| æœåŠ¡ | èŒè´£ | æŠ€æœ¯æ ˆ | QPS |
|------|------|--------|-----|
| **ç›´æ’­é—´æœåŠ¡** | ç›´æ’­é—´ç®¡ç†ã€è§‚ä¼—è¿›å‡º | Go + Redis + MySQL | 30K |
| **ç”¨æˆ·æœåŠ¡** | è®¤è¯ã€ç”¨æˆ·ä¿¡æ¯ | Go + Redis + JWT | 20K |
| **æ¶ˆæ¯æœåŠ¡** | å¼¹å¹•ã€ç¤¼ç‰©ã€æ¨é€ | Go + WebSocket + Kafka | 100K |
| **ç»Ÿè®¡æœåŠ¡** | å®æ—¶ç»Ÿè®¡ã€æ•°æ®èšåˆ | Go + Redis + ClickHouse | 10K |

### 2.3 æ•°æ®æµè®¾è®¡

```text
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        å®æ—¶æ¶ˆæ¯æµï¼ˆKafka + Redisï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. ç”¨æˆ·å‘é€å¼¹å¹•
   â†“
2. æ¶ˆæ¯æœåŠ¡æ¥æ”¶ï¼ˆWebSocketï¼‰
   â”œâ”€ Trace: message.receive
   â”œâ”€ Metric: message.received++
   â””â”€ Baggage: user.id, room.id
   â†“
3. å‘å¸ƒåˆ° Kafkaï¼ˆå¼‚æ­¥ï¼‰
   â”œâ”€ Topic: live-messages
   â”œâ”€ Partition: room_id % 10
   â””â”€ Trace: kafka.publish
   â†“
4. Redis Pub/Subï¼ˆå®æ—¶ï¼‰
   â”œâ”€ Channel: room:{room_id}
   â””â”€ Trace: redis.publish
   â†“
5. æ¨é€åˆ°æˆ¿é—´æ‰€æœ‰ç”¨æˆ·
   â”œâ”€ WebSocket æ¨é€
   â”œâ”€ Trace: message.broadcast
   â””â”€ Metric: message.sent++

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## 3. æ ¸å¿ƒæœåŠ¡å®ç°

### 3.1 ç›´æ’­é—´æœåŠ¡

```go
package liveroom

import (
    "context"
    "encoding/json"
    "fmt"
    "time"

    "github.com/go-redis/redis/v8"
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/metric"
    "go.opentelemetry.io/otel/trace"
    "gorm.io/gorm"
)

// LiveRoom ç›´æ’­é—´
type LiveRoom struct {
    ID          int64     `json:"id" gorm:"primaryKey"`
    AnchorID    int64     `json:"anchor_id" gorm:"index"`
    Title       string    `json:"title"`
    Cover       string    `json:"cover"`
    Status      int       `json:"status"` // 0=æœªå¼€æ’­, 1=ç›´æ’­ä¸­, 2=å·²ç»“æŸ
    ViewerCount int64     `json:"viewer_count"`
    CreatedAt   time.Time `json:"created_at"`
    UpdatedAt   time.Time `json:"updated_at"`
}

// LiveRoomService ç›´æ’­é—´æœåŠ¡
type LiveRoomService struct {
    db          *gorm.DB
    redis       *redis.Client
    tracer      trace.Tracer
    meter       metric.Meter
    
    // Metrics
    roomCreated   metric.Int64Counter
    roomStarted   metric.Int64Counter
    roomStopped   metric.Int64Counter
    viewerJoined  metric.Int64Counter
    viewerLeft    metric.Int64Counter
    viewerCount   metric.Int64ObservableGauge
}

// NewLiveRoomService åˆ›å»ºç›´æ’­é—´æœåŠ¡
func NewLiveRoomService(db *gorm.DB, rdb *redis.Client) (*LiveRoomService, error) {
    tracer := otel.Tracer("liveroom-service")
    meter := otel.Meter("liveroom-service")
    
    // åˆå§‹åŒ–æŒ‡æ ‡
    roomCreated, _ := meter.Int64Counter(
        "liveroom.created",
        metric.WithDescription("ç›´æ’­é—´åˆ›å»ºæ•°"),
    )
    
    roomStarted, _ := meter.Int64Counter(
        "liveroom.started",
        metric.WithDescription("å¼€æ’­æ•°"),
    )
    
    roomStopped, _ := meter.Int64Counter(
        "liveroom.stopped",
        metric.WithDescription("å…³æ’­æ•°"),
    )
    
    viewerJoined, _ := meter.Int64Counter(
        "liveroom.viewer.joined",
        metric.WithDescription("è§‚ä¼—è¿›å…¥æ•°"),
    )
    
    viewerLeft, _ := meter.Int64Counter(
        "liveroom.viewer.left",
        metric.WithDescription("è§‚ä¼—ç¦»å¼€æ•°"),
    )
    
    viewerCount, _ := meter.Int64ObservableGauge(
        "liveroom.viewer.count",
        metric.WithDescription("å½“å‰è§‚ä¼—æ•°"),
    )
    
    svc := &LiveRoomService{
        db:            db,
        redis:         rdb,
        tracer:        tracer,
        meter:         meter,
        roomCreated:   roomCreated,
        roomStarted:   roomStarted,
        roomStopped:   roomStopped,
        viewerJoined:  viewerJoined,
        viewerLeft:    viewerLeft,
        viewerCount:   viewerCount,
    }
    
    // æ³¨å†Œå›è°ƒï¼ˆå®šæœŸæ”¶é›†è§‚ä¼—æ•°ï¼‰
    _, err := meter.RegisterCallback(
        func(ctx context.Context, o metric.Observer) error {
            return svc.collectViewerCount(ctx, o)
        },
        viewerCount,
    )
    
    return svc, err
}

// CreateRoom åˆ›å»ºç›´æ’­é—´
func (s *LiveRoomService) CreateRoom(ctx context.Context, room *LiveRoom) error {
    ctx, span := s.tracer.Start(ctx, "CreateRoom")
    defer span.End()
    
    span.SetAttributes(
        attribute.Int64("anchor.id", room.AnchorID),
        attribute.String("room.title", room.Title),
    )
    
    // æ•°æ®åº“åˆ›å»º
    if err := s.db.WithContext(ctx).Create(room).Error; err != nil {
        span.RecordError(err)
        return fmt.Errorf("failed to create room: %w", err)
    }
    
    // ç¼“å­˜æˆ¿é—´ä¿¡æ¯ï¼ˆTTL 1å°æ—¶ï¼‰
    roomKey := fmt.Sprintf("room:%d", room.ID)
    roomData, _ := json.Marshal(room)
    if err := s.redis.Set(ctx, roomKey, roomData, time.Hour).Err(); err != nil {
        span.RecordError(err)
        // ç¼“å­˜å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
    }
    
    // è®°å½•æŒ‡æ ‡
    s.roomCreated.Add(ctx, 1,
        metric.WithAttributes(attribute.Int64("anchor.id", room.AnchorID)),
    )
    
    return nil
}

// StartLive å¼€æ’­
func (s *LiveRoomService) StartLive(ctx context.Context, roomID int64) error {
    ctx, span := s.tracer.Start(ctx, "StartLive")
    defer span.End()
    
    span.SetAttributes(attribute.Int64("room.id", roomID))
    
    // æ›´æ–°çŠ¶æ€
    result := s.db.WithContext(ctx).
        Model(&LiveRoom{}).
        Where("id = ? AND status = 0", roomID).
        Updates(map[string]interface{}{
            "status":     1,
            "updated_at": time.Now(),
        })
    
    if result.Error != nil {
        span.RecordError(result.Error)
        return fmt.Errorf("failed to start live: %w", result.Error)
    }
    
    if result.RowsAffected == 0 {
        return fmt.Errorf("room not found or already started")
    }
    
    // æ›´æ–°ç¼“å­˜
    roomKey := fmt.Sprintf("room:%d", roomID)
    s.redis.HSet(ctx, roomKey, "status", 1)
    
    // åˆå§‹åŒ–è§‚ä¼—åˆ—è¡¨ï¼ˆRedis Setï¼‰
    viewerKey := fmt.Sprintf("room:%d:viewers", roomID)
    s.redis.Del(ctx, viewerKey)
    
    // è®°å½•æŒ‡æ ‡
    s.roomStarted.Add(ctx, 1,
        metric.WithAttributes(attribute.Int64("room.id", roomID)),
    )
    
    return nil
}

// StopLive å…³æ’­
func (s *LiveRoomService) StopLive(ctx context.Context, roomID int64) error {
    ctx, span := s.tracer.Start(ctx, "StopLive")
    defer span.End()
    
    span.SetAttributes(attribute.Int64("room.id", roomID))
    
    // è·å–æœ€ç»ˆè§‚ä¼—æ•°
    viewerKey := fmt.Sprintf("room:%d:viewers", roomID)
    finalCount, _ := s.redis.SCard(ctx, viewerKey).Result()
    
    span.SetAttributes(attribute.Int64("final.viewer.count", finalCount))
    
    // æ›´æ–°çŠ¶æ€
    result := s.db.WithContext(ctx).
        Model(&LiveRoom{}).
        Where("id = ? AND status = 1", roomID).
        Updates(map[string]interface{}{
            "status":       2,
            "viewer_count": finalCount,
            "updated_at":   time.Now(),
        })
    
    if result.Error != nil {
        span.RecordError(result.Error)
        return fmt.Errorf("failed to stop live: %w", result.Error)
    }
    
    // æ¸…ç†ç¼“å­˜ï¼ˆå»¶è¿Ÿåˆ é™¤ï¼Œä¿ç•™5åˆ†é’Ÿä¾›æŸ¥çœ‹ï¼‰
    roomKey := fmt.Sprintf("room:%d", roomID)
    s.redis.Expire(ctx, roomKey, 5*time.Minute)
    s.redis.Expire(ctx, viewerKey, 5*time.Minute)
    
    // è®°å½•æŒ‡æ ‡
    s.roomStopped.Add(ctx, 1,
        metric.WithAttributes(
            attribute.Int64("room.id", roomID),
            attribute.Int64("final.viewer.count", finalCount),
        ),
    )
    
    return nil
}

// JoinRoom è§‚ä¼—è¿›å…¥ç›´æ’­é—´
func (s *LiveRoomService) JoinRoom(ctx context.Context, roomID, userID int64) error {
    ctx, span := s.tracer.Start(ctx, "JoinRoom")
    defer span.End()
    
    span.SetAttributes(
        attribute.Int64("room.id", roomID),
        attribute.Int64("user.id", userID),
    )
    
    // æ£€æŸ¥æˆ¿é—´çŠ¶æ€
    roomKey := fmt.Sprintf("room:%d", roomID)
    status, err := s.redis.HGet(ctx, roomKey, "status").Int()
    if err != nil || status != 1 {
        return fmt.Errorf("room not live")
    }
    
    // æ·»åŠ åˆ°è§‚ä¼—åˆ—è¡¨
    viewerKey := fmt.Sprintf("room:%d:viewers", roomID)
    added, err := s.redis.SAdd(ctx, viewerKey, userID).Result()
    if err != nil {
        span.RecordError(err)
        return fmt.Errorf("failed to join room: %w", err)
    }
    
    // åªæœ‰æ–°ç”¨æˆ·æ‰è®°å½•æŒ‡æ ‡
    if added > 0 {
        s.viewerJoined.Add(ctx, 1,
            metric.WithAttributes(attribute.Int64("room.id", roomID)),
        )
    }
    
    return nil
}

// LeaveRoom è§‚ä¼—ç¦»å¼€ç›´æ’­é—´
func (s *LiveRoomService) LeaveRoom(ctx context.Context, roomID, userID int64) error {
    ctx, span := s.tracer.Start(ctx, "LeaveRoom")
    defer span.End()
    
    span.SetAttributes(
        attribute.Int64("room.id", roomID),
        attribute.Int64("user.id", userID),
    )
    
    // ä»è§‚ä¼—åˆ—è¡¨ç§»é™¤
    viewerKey := fmt.Sprintf("room:%d:viewers", roomID)
    removed, err := s.redis.SRem(ctx, viewerKey, userID).Result()
    if err != nil {
        span.RecordError(err)
        return fmt.Errorf("failed to leave room: %w", err)
    }
    
    // åªæœ‰çœŸæ­£ç§»é™¤æ‰è®°å½•æŒ‡æ ‡
    if removed > 0 {
        s.viewerLeft.Add(ctx, 1,
            metric.WithAttributes(attribute.Int64("room.id", roomID)),
        )
    }
    
    return nil
}

// GetRoomInfo è·å–æˆ¿é—´ä¿¡æ¯
func (s *LiveRoomService) GetRoomInfo(ctx context.Context, roomID int64) (*LiveRoom, error) {
    ctx, span := s.tracer.Start(ctx, "GetRoomInfo")
    defer span.End()
    
    span.SetAttributes(attribute.Int64("room.id", roomID))
    
    // å…ˆæŸ¥ç¼“å­˜
    roomKey := fmt.Sprintf("room:%d", roomID)
    roomData, err := s.redis.Get(ctx, roomKey).Bytes()
    if err == nil {
        var room LiveRoom
        if err := json.Unmarshal(roomData, &room); err == nil {
            span.SetAttributes(attribute.Bool("cache.hit", true))
            return &room, nil
        }
    }
    
    span.SetAttributes(attribute.Bool("cache.hit", false))
    
    // æŸ¥æ•°æ®åº“
    var room LiveRoom
    if err := s.db.WithContext(ctx).First(&room, roomID).Error; err != nil {
        span.RecordError(err)
        return nil, fmt.Errorf("room not found: %w", err)
    }
    
    // å›å†™ç¼“å­˜
    roomData, _ = json.Marshal(&room)
    s.redis.Set(ctx, roomKey, roomData, time.Hour)
    
    return &room, nil
}

// collectViewerCount æ”¶é›†è§‚ä¼—æ•°ï¼ˆå®šæœŸå›è°ƒï¼‰
func (s *LiveRoomService) collectViewerCount(ctx context.Context, o metric.Observer) error {
    // æ‰«ææ‰€æœ‰ç›´æ’­ä¸­çš„æˆ¿é—´
    pattern := "room:*:viewers"
    iter := s.redis.Scan(ctx, 0, pattern, 100).Iterator()
    
    for iter.Next(ctx) {
        key := iter.Val()
        count, _ := s.redis.SCard(ctx, key).Result()
        
        // æå– roomID
        var roomID int64
        fmt.Sscanf(key, "room:%d:viewers", &roomID)
        
        o.ObserveInt64(s.viewerCount, count,
            metric.WithAttributes(attribute.Int64("room.id", roomID)),
        )
    }
    
    return iter.Err()
}
```

### 3.2 æ¶ˆæ¯æœåŠ¡ï¼ˆWebSocketï¼‰

```go
package message

import (
    "context"
    "encoding/json"
    "fmt"
    "sync"
    "time"

    "github.com/gorilla/websocket"
    "github.com/segmentio/kafka-go"
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/codes"
    "go.opentelemetry.io/otel/metric"
    "go.opentelemetry.io/otel/propagation"
    "go.opentelemetry.io/otel/trace"
)

// Message æ¶ˆæ¯
type Message struct {
    Type      string                 `json:"type"`       // danmu, gift, etc.
    RoomID    int64                  `json:"room_id"`
    UserID    int64                  `json:"user_id"`
    Content   string                 `json:"content"`
    Data      map[string]interface{} `json:"data"`
    Timestamp int64                  `json:"timestamp"`
    TraceID   string                 `json:"trace_id"`   // è¿½è¸ªID
}

// Connection WebSocket è¿æ¥
type Connection struct {
    UserID int64
    RoomID int64
    Conn   *websocket.Conn
    Send   chan *Message
    ctx    context.Context
    cancel context.CancelFunc
}

// MessageService æ¶ˆæ¯æœåŠ¡
type MessageService struct {
    // è¿æ¥ç®¡ç†ï¼ˆåˆ†ç‰‡é”é™ä½ç«äº‰ï¼‰
    connections     [][]*Connection // åˆ†16ä¸ªåˆ†ç‰‡
    connectionMutex []sync.RWMutex
    shardCount      int
    
    // Kafka Producer
    kafkaWriter *kafka.Writer
    
    // Tracing & Metrics
    tracer           trace.Tracer
    meter            metric.Meter
    messageReceived  metric.Int64Counter
    messageSent      metric.Int64Counter
    connectionCount  metric.Int64ObservableGauge
    messageLatency   metric.Float64Histogram
    
    // é…ç½®
    upgrader websocket.Upgrader
}

// NewMessageService åˆ›å»ºæ¶ˆæ¯æœåŠ¡
func NewMessageService(kafkaBrokers []string) (*MessageService, error) {
    shardCount := 16
    
    tracer := otel.Tracer("message-service")
    meter := otel.Meter("message-service")
    
    // åˆå§‹åŒ–æŒ‡æ ‡
    messageReceived, _ := meter.Int64Counter(
        "message.received",
        metric.WithDescription("æ¥æ”¶æ¶ˆæ¯æ•°"),
    )
    
    messageSent, _ := meter.Int64Counter(
        "message.sent",
        metric.WithDescription("å‘é€æ¶ˆæ¯æ•°"),
    )
    
    messageLatency, _ := meter.Float64Histogram(
        "message.latency",
        metric.WithDescription("æ¶ˆæ¯å»¶è¿Ÿ"),
        metric.WithUnit("ms"),
    )
    
    connectionCount, _ := meter.Int64ObservableGauge(
        "connection.count",
        metric.WithDescription("WebSocketè¿æ¥æ•°"),
    )
    
    // Kafka Writer
    writer := &kafka.Writer{
        Addr:         kafka.TCP(kafkaBrokers...),
        Topic:        "live-messages",
        Balancer:     &kafka.Hash{}, // æŒ‰ room_id hash
        BatchSize:    100,
        BatchTimeout: 10 * time.Millisecond,
        Compression:  kafka.Snappy,
    }
    
    svc := &MessageService{
        connections:     make([][]*Connection, shardCount),
        connectionMutex: make([]sync.RWMutex, shardCount),
        shardCount:      shardCount,
        kafkaWriter:     writer,
        tracer:          tracer,
        meter:           meter,
        messageReceived: messageReceived,
        messageSent:     messageSent,
        connectionCount: connectionCount,
        messageLatency:  messageLatency,
        upgrader: websocket.Upgrader{
            CheckOrigin: func(r *http.Request) bool { return true },
        },
    }
    
    // æ³¨å†Œå›è°ƒ
    _, err := meter.RegisterCallback(
        func(ctx context.Context, o metric.Observer) error {
            count := svc.getTotalConnectionCount()
            o.ObserveInt64(connectionCount, int64(count))
            return nil
        },
        connectionCount,
    )
    
    return svc, err
}

// HandleWebSocket å¤„ç† WebSocket è¿æ¥
func (s *MessageService) HandleWebSocket(w http.ResponseWriter, r *http.Request) {
    ctx := r.Context()
    ctx, span := s.tracer.Start(ctx, "HandleWebSocket")
    defer span.End()
    
    // ä»æŸ¥è¯¢å‚æ•°è·å–ç”¨æˆ·å’Œæˆ¿é—´ä¿¡æ¯
    userID := getUserIDFromToken(r) // å‡è®¾ä» token è§£æ
    roomIDStr := r.URL.Query().Get("room_id")
    roomID, _ := strconv.ParseInt(roomIDStr, 10, 64)
    
    span.SetAttributes(
        attribute.Int64("user.id", userID),
        attribute.Int64("room.id", roomID),
    )
    
    // å‡çº§ä¸º WebSocket
    conn, err := s.upgrader.Upgrade(w, r, nil)
    if err != nil {
        span.RecordError(err)
        return
    }
    
    // åˆ›å»ºè¿æ¥å¯¹è±¡
    connCtx, cancel := context.WithCancel(ctx)
    connection := &Connection{
        UserID: userID,
        RoomID: roomID,
        Conn:   conn,
        Send:   make(chan *Message, 256), // ç¼“å†²é˜Ÿåˆ—
        ctx:    connCtx,
        cancel: cancel,
    }
    
    // æ³¨å†Œè¿æ¥
    s.addConnection(connection)
    defer func() {
        s.removeConnection(connection)
        conn.Close()
        cancel()
    }()
    
    // å¯åŠ¨å‘é€åç¨‹
    go s.writePump(connection)
    
    // è¯»å–æ¶ˆæ¯ï¼ˆé˜»å¡ï¼‰
    s.readPump(connection)
}

// readPump è¯»å–æ¶ˆæ¯
func (s *MessageService) readPump(conn *Connection) {
    defer conn.cancel()
    
    conn.Conn.SetReadDeadline(time.Now().Add(60 * time.Second))
    conn.Conn.SetPongHandler(func(string) error {
        conn.Conn.SetReadDeadline(time.Now().Add(60 * time.Second))
        return nil
    })
    
    for {
        select {
        case <-conn.ctx.Done():
            return
        default:
        }
        
        var msg Message
        if err := conn.Conn.ReadJSON(&msg); err != nil {
            return
        }
        
        // å¤„ç†æ¶ˆæ¯
        s.handleMessage(conn, &msg)
    }
}

// writePump å‘é€æ¶ˆæ¯
func (s *MessageService) writePump(conn *Connection) {
    ticker := time.NewTicker(54 * time.Second) // Ping
    defer ticker.Stop()
    
    for {
        select {
        case msg := <-conn.Send:
            conn.Conn.SetWriteDeadline(time.Now().Add(10 * time.Second))
            if err := conn.Conn.WriteJSON(msg); err != nil {
                return
            }
            
        case <-ticker.C:
            conn.Conn.SetWriteDeadline(time.Now().Add(10 * time.Second))
            if err := conn.Conn.WriteMessage(websocket.PingMessage, nil); err != nil {
                return
            }
            
        case <-conn.ctx.Done():
            return
        }
    }
}

// handleMessage å¤„ç†æ¶ˆæ¯
func (s *MessageService) handleMessage(conn *Connection, msg *Message) {
    startTime := time.Now()
    
    ctx, span := s.tracer.Start(conn.ctx, "HandleMessage",
        trace.WithAttributes(
            attribute.String("message.type", msg.Type),
            attribute.Int64("room.id", msg.RoomID),
            attribute.Int64("user.id", msg.UserID),
        ),
    )
    defer span.End()
    
    // å¡«å……æ¶ˆæ¯ä¿¡æ¯
    msg.RoomID = conn.RoomID
    msg.UserID = conn.UserID
    msg.Timestamp = time.Now().Unix()
    msg.TraceID = span.SpanContext().TraceID().String()
    
    // è®°å½•æŒ‡æ ‡
    s.messageReceived.Add(ctx, 1,
        metric.WithAttributes(
            attribute.String("message.type", msg.Type),
            attribute.Int64("room.id", msg.RoomID),
        ),
    )
    
    // å‘é€åˆ° Kafkaï¼ˆå¼‚æ­¥æŒä¹…åŒ–ï¼‰
    go s.publishToKafka(ctx, msg)
    
    // å¹¿æ’­åˆ°æˆ¿é—´
    if err := s.broadcastToRoom(ctx, msg); err != nil {
        span.RecordError(err)
        span.SetStatus(codes.Error, err.Error())
        return
    }
    
    // è®°å½•å»¶è¿Ÿ
    latency := time.Since(startTime).Milliseconds()
    s.messageLatency.Record(ctx, float64(latency),
        metric.WithAttributes(
            attribute.String("message.type", msg.Type),
        ),
    )
    
    span.SetStatus(codes.Ok, "")
}

// publishToKafka å‘å¸ƒåˆ° Kafka
func (s *MessageService) publishToKafka(ctx context.Context, msg *Message) {
    ctx, span := s.tracer.Start(ctx, "PublishToKafka")
    defer span.End()
    
    data, _ := json.Marshal(msg)
    
    // æ³¨å…¥ Trace Context
    carrier := propagation.MapCarrier{}
    otel.GetTextMapPropagator().Inject(ctx, carrier)
    
    headers := make([]kafka.Header, 0, len(carrier))
    for k, v := range carrier {
        headers = append(headers, kafka.Header{
            Key:   k,
            Value: []byte(v),
        })
    }
    
    // å‘é€
    err := s.kafkaWriter.WriteMessages(ctx, kafka.Message{
        Key:     []byte(fmt.Sprintf("%d", msg.RoomID)), // æŒ‰æˆ¿é—´åˆ†åŒº
        Value:   data,
        Headers: headers,
    })
    
    if err != nil {
        span.RecordError(err)
    }
}

// broadcastToRoom å¹¿æ’­åˆ°æˆ¿é—´
func (s *MessageService) broadcastToRoom(ctx context.Context, msg *Message) error {
    ctx, span := s.tracer.Start(ctx, "BroadcastToRoom")
    defer span.End()
    
    roomID := msg.RoomID
    sentCount := 0
    
    // éå†æ‰€æœ‰åˆ†ç‰‡
    for i := 0; i < s.shardCount; i++ {
        s.connectionMutex[i].RLock()
        for _, conn := range s.connections[i] {
            if conn.RoomID == roomID {
                select {
                case conn.Send <- msg:
                    sentCount++
                default:
                    // Channel æ»¡ï¼Œä¸¢å¼ƒæ¶ˆæ¯
                }
            }
        }
        s.connectionMutex[i].RUnlock()
    }
    
    span.SetAttributes(attribute.Int("sent.count", sentCount))
    
    s.messageSent.Add(ctx, int64(sentCount),
        metric.WithAttributes(
            attribute.String("message.type", msg.Type),
            attribute.Int64("room.id", roomID),
        ),
    )
    
    return nil
}

// addConnection æ·»åŠ è¿æ¥
func (s *MessageService) addConnection(conn *Connection) {
    shard := conn.UserID % int64(s.shardCount)
    s.connectionMutex[shard].Lock()
    s.connections[shard] = append(s.connections[shard], conn)
    s.connectionMutex[shard].Unlock()
}

// removeConnection ç§»é™¤è¿æ¥
func (s *MessageService) removeConnection(conn *Connection) {
    shard := conn.UserID % int64(s.shardCount)
    s.connectionMutex[shard].Lock()
    defer s.connectionMutex[shard].Unlock()
    
    conns := s.connections[shard]
    for i, c := range conns {
        if c == conn {
            s.connections[shard] = append(conns[:i], conns[i+1:]...)
            break
        }
    }
}

// getTotalConnectionCount è·å–æ€»è¿æ¥æ•°
func (s *MessageService) getTotalConnectionCount() int {
    total := 0
    for i := 0; i < s.shardCount; i++ {
        s.connectionMutex[i].RLock()
        total += len(s.connections[i])
        s.connectionMutex[i].RUnlock()
    }
    return total
}
```

---

## 4. é«˜å¹¶å‘ä¼˜åŒ–

### 4.1 è¿æ¥æ± ä¼˜åŒ–

```go
package pool

import (
    "context"
    "sync"
    "time"

    "github.com/go-redis/redis/v8"
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/metric"
    "gorm.io/driver/mysql"
    "gorm.io/gorm"
    "gorm.io/plugin/dbresolver"
)

// DatabasePool æ•°æ®åº“è¿æ¥æ± ï¼ˆä¸»ä»åˆ†ç¦»ï¼‰
func NewDatabasePool(masterDSN string, slaveDSNs []string) (*gorm.DB, error) {
    // ä¸»åº“
    db, err := gorm.Open(mysql.Open(masterDSN), &gorm.Config{
        PrepareStmt: true, // é¢„ç¼–è¯‘
    })
    if err != nil {
        return nil, err
    }
    
    // ä»åº“
    replicas := make([]gorm.Dialector, len(slaveDSNs))
    for i, dsn := range slaveDSNs {
        replicas[i] = mysql.Open(dsn)
    }
    
    // ä¸»ä»åˆ†ç¦»æ’ä»¶
    err = db.Use(dbresolver.Register(dbresolver.Config{
        Replicas: replicas,
        Policy:   dbresolver.RandomPolicy{}, // éšæœºè´Ÿè½½å‡è¡¡
    }).
    SetConnMaxIdleTime(time.Hour).
    SetConnMaxLifetime(24 * time.Hour).
    SetMaxIdleConns(50).
    SetMaxOpenConns(200))
    
    return db, err
}

// RedisPool Redis è¿æ¥æ± ï¼ˆé›†ç¾¤æ¨¡å¼ï¼‰
func NewRedisPool(addrs []string) *redis.ClusterClient {
    rdb := redis.NewClusterClient(&redis.ClusterOptions{
        Addrs:        addrs,
        PoolSize:     100,              // æ¯ä¸ªèŠ‚ç‚¹100ä¸ªè¿æ¥
        MinIdleConns: 20,
        PoolTimeout:  4 * time.Second,
        
        // è¯»å†™åˆ†ç¦»
        ReadOnly:       true,
        RouteByLatency: true,           // æŒ‰å»¶è¿Ÿè·¯ç”±
        RouteRandomly:  false,
    })
    
    return rdb
}
```

### 4.2 æœ¬åœ°ç¼“å­˜

```go
package cache

import (
    "context"
    "sync"
    "time"

    "github.com/allegro/bigcache/v3"
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/metric"
)

// LocalCache æœ¬åœ°ç¼“å­˜ï¼ˆå‡å°‘ Redis å‹åŠ›ï¼‰
type LocalCache struct {
    cache *bigcache.BigCache
    meter metric.Meter
    
    hits   metric.Int64Counter
    misses metric.Int64Counter
}

// NewLocalCache åˆ›å»ºæœ¬åœ°ç¼“å­˜
func NewLocalCache() (*LocalCache, error) {
    // BigCache é…ç½®
    config := bigcache.DefaultConfig(5 * time.Minute)
    config.Shards = 1024              // åˆ†ç‰‡æ•°
    config.MaxEntriesInWindow = 1000 * 10 * 60
    config.MaxEntrySize = 500         // 500 bytes
    config.HardMaxCacheSize = 2048    // 2GB
    
    cache, err := bigcache.NewBigCache(config)
    if err != nil {
        return nil, err
    }
    
    meter := otel.Meter("local-cache")
    hits, _ := meter.Int64Counter("cache.hits")
    misses, _ := meter.Int64Counter("cache.misses")
    
    return &LocalCache{
        cache:  cache,
        meter:  meter,
        hits:   hits,
        misses: misses,
    }, nil
}

// Get è·å–
func (c *LocalCache) Get(ctx context.Context, key string) ([]byte, bool) {
    data, err := c.cache.Get(key)
    if err == nil {
        c.hits.Add(ctx, 1)
        return data, true
    }
    
    c.misses.Add(ctx, 1)
    return nil, false
}

// Set è®¾ç½®
func (c *LocalCache) Set(ctx context.Context, key string, value []byte) error {
    return c.cache.Set(key, value)
}
```

### 4.3 æ‰¹é‡å¤„ç†

```go
package batch

import (
    "context"
    "sync"
    "time"

    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/trace"
)

// BatchProcessor æ‰¹é‡å¤„ç†å™¨
type BatchProcessor[T any] struct {
    batchSize    int
    flushTimeout time.Duration
    processor    func(context.Context, []T) error
    
    buffer []T
    mu     sync.Mutex
    timer  *time.Timer
    tracer trace.Tracer
}

// NewBatchProcessor åˆ›å»ºæ‰¹é‡å¤„ç†å™¨
func NewBatchProcessor[T any](
    batchSize int,
    flushTimeout time.Duration,
    processor func(context.Context, []T) error,
) *BatchProcessor[T] {
    bp := &BatchProcessor[T]{
        batchSize:    batchSize,
        flushTimeout: flushTimeout,
        processor:    processor,
        buffer:       make([]T, 0, batchSize),
        tracer:       otel.Tracer("batch-processor"),
    }
    
    bp.timer = time.AfterFunc(flushTimeout, func() {
        bp.flush(context.Background())
    })
    
    return bp
}

// Add æ·»åŠ åˆ°æ‰¹æ¬¡
func (bp *BatchProcessor[T]) Add(ctx context.Context, item T) error {
    bp.mu.Lock()
    defer bp.mu.Unlock()
    
    bp.buffer = append(bp.buffer, item)
    
    // è¾¾åˆ°æ‰¹æ¬¡å¤§å°ï¼Œç«‹å³åˆ·æ–°
    if len(bp.buffer) >= bp.batchSize {
        return bp.flushLocked(ctx)
    }
    
    // é‡ç½®å®šæ—¶å™¨
    bp.timer.Reset(bp.flushTimeout)
    return nil
}

// flush åˆ·æ–°æ‰¹æ¬¡ï¼ˆå†…éƒ¨ï¼‰
func (bp *BatchProcessor[T]) flushLocked(ctx context.Context) error {
    if len(bp.buffer) == 0 {
        return nil
    }
    
    ctx, span := bp.tracer.Start(ctx, "BatchProcessor.Flush",
        trace.WithAttributes(
            attribute.Int("batch.size", len(bp.buffer)),
        ),
    )
    defer span.End()
    
    batch := bp.buffer
    bp.buffer = make([]T, 0, bp.batchSize)
    
    return bp.processor(ctx, batch)
}

// flush åˆ·æ–°æ‰¹æ¬¡ï¼ˆå…¬å¼€ï¼‰
func (bp *BatchProcessor[T]) flush(ctx context.Context) error {
    bp.mu.Lock()
    defer bp.mu.Unlock()
    return bp.flushLocked(ctx)
}
```

---

## 5. OTLPå®Œæ•´é›†æˆ

### 5.1 ç»Ÿä¸€åˆå§‹åŒ–

```go
package telemetry

import (
    "context"
    "fmt"

    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
    "go.opentelemetry.io/otel/exporters/otlp/otlpmetric/otlpmetricgrpc"
    "go.opentelemetry.io/otel/propagation"
    "go.opentelemetry.io/otel/sdk/metric"
    "go.opentelemetry.io/otel/sdk/resource"
    "go.opentelemetry.io/otel/sdk/trace"
    semconv "go.opentelemetry.io/otel/semconv/v1.21.0"
)

// InitOTLP åˆå§‹åŒ– OTLP
func InitOTLP(ctx context.Context, serviceName, otlpEndpoint string) (func(), error) {
    res, err := resource.New(ctx,
        resource.WithAttributes(
            semconv.ServiceName(serviceName),
            semconv.ServiceVersion("1.0.0"),
            semconv.DeploymentEnvironment("production"),
        ),
    )
    if err != nil {
        return nil, fmt.Errorf("failed to create resource: %w", err)
    }
    
    // Trace Exporter
    traceExporter, err := otlptracegrpc.New(ctx,
        otlptracegrpc.WithEndpoint(otlpEndpoint),
        otlptracegrpc.WithInsecure(),
    )
    if err != nil {
        return nil, fmt.Errorf("failed to create trace exporter: %w", err)
    }
    
    // Trace Providerï¼ˆè‡ªé€‚åº”é‡‡æ ·ï¼‰
    tp := trace.NewTracerProvider(
        trace.WithBatcher(traceExporter,
            trace.WithMaxExportBatchSize(512),
            trace.WithMaxQueueSize(2048),
            trace.WithBatchTimeout(5*time.Second),
        ),
        trace.WithResource(res),
        trace.WithSampler(NewAdaptiveSampler()), // è‡ªé€‚åº”é‡‡æ ·å™¨
    )
    otel.SetTracerProvider(tp)
    
    // Metric Exporter
    metricExporter, err := otlpmetricgrpc.New(ctx,
        otlpmetricgrpc.WithEndpoint(otlpEndpoint),
        otlpmetricgrpc.WithInsecure(),
    )
    if err != nil {
        return nil, fmt.Errorf("failed to create metric exporter: %w", err)
    }
    
    // Metric Provider
    mp := metric.NewMeterProvider(
        metric.WithReader(
            metric.NewPeriodicReader(metricExporter,
                metric.WithInterval(10*time.Second),
            ),
        ),
        metric.WithResource(res),
    )
    otel.SetMeterProvider(mp)
    
    // Propagator
    otel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(
        propagation.TraceContext{},
        propagation.Baggage{},
    ))
    
    // Shutdown function
    shutdown := func() {
        tp.Shutdown(context.Background())
        mp.Shutdown(context.Background())
    }
    
    return shutdown, nil
}

// AdaptiveSampler è‡ªé€‚åº”é‡‡æ ·å™¨
type AdaptiveSampler struct {
    baseSampler trace.Sampler
    errorSampler trace.Sampler
}

func NewAdaptiveSampler() *AdaptiveSampler {
    return &AdaptiveSampler{
        baseSampler:  trace.TraceIDRatioBased(0.01), // 1% åŸºç¡€é‡‡æ ·
        errorSampler: trace.AlwaysSample(),           // é”™è¯¯å…¨é‡‡æ ·
    }
}

func (s *AdaptiveSampler) ShouldSample(p trace.SamplingParameters) trace.SamplingResult {
    // é”™è¯¯è¯·æ±‚100%é‡‡æ ·
    for _, attr := range p.Attributes {
        if attr.Key == "error" && attr.Value.AsBool() {
            return s.errorSampler.ShouldSample(p)
        }
    }
    
    // æ…¢è¯·æ±‚100%é‡‡æ ·ï¼ˆ>1sï¼‰
    // è¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦åœ¨ Span ç»“æŸæ—¶åˆ¤æ–­
    
    return s.baseSampler.ShouldSample(p)
}

func (s *AdaptiveSampler) Description() string {
    return "AdaptiveSampler"
}
```

---

## 6. äº‘åŸç”Ÿéƒ¨ç½²

### 6.1 Kubernetes Deployment

```yaml
# liveroom-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: liveroom-service
  labels:
    app: liveroom
    version: v1
spec:
  replicas: 10  # 10ä¸ªå‰¯æœ¬
  selector:
    matchLabels:
      app: liveroom
  template:
    metadata:
      labels:
        app: liveroom
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: liveroom
        image: liveroom-service:v1.0.0
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 9090
          name: grpc
        env:
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "otel-collector:4317"
        - name: OTEL_SERVICE_NAME
          value: "liveroom-service"
        - name: OTEL_RESOURCE_ATTRIBUTES
          value: "deployment.environment=production,service.version=1.0.0"
        - name: GOMAXPROCS
          valueFrom:
            resourceFieldRef:
              resource: limits.cpu
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8080
          initialDelaySeconds: 15
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: liveroom-service
spec:
  selector:
    app: liveroom
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: grpc
    port: 9090
    targetPort: 9090
  type: ClusterIP
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: liveroom-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: liveroom-service
  minReplicas: 10
  maxReplicas: 100
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50  # æ¯æ¬¡æ‰©å®¹50%
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 2  # æ¯æ¬¡ç¼©å®¹2ä¸ªPod
        periodSeconds: 60
```

### 6.2 Istio é›†æˆ

```yaml
# virtualservice.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: liveroom
spec:
  hosts:
  - liveroom-service
  http:
  - match:
    - headers:
        x-canary:
          exact: "true"
    route:
    - destination:
        host: liveroom-service
        subset: v2
      weight: 100
  - route:
    - destination:
        host: liveroom-service
        subset: v1
      weight: 100
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: liveroom
spec:
  host: liveroom-service
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 1000
      http:
        http1MaxPendingRequests: 1000
        http2MaxRequests: 1000
        maxRequestsPerConnection: 10
    loadBalancer:
      consistentHash:
        httpCookie:
          name: user-session
          ttl: 3600s
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

---

## 7. æ€§èƒ½æµ‹è¯•ä¸ä¼˜åŒ–

### 7.1 æ€§èƒ½åŸºå‡†

```bash
# å‹æµ‹è„šæœ¬
#!/bin/bash

# WebSocket è¿æ¥æµ‹è¯•
echo "æµ‹è¯• WebSocket è¿æ¥..."
k6 run --vus 10000 --duration 60s websocket-test.js

# HTTP API æµ‹è¯•
echo "æµ‹è¯• HTTP API..."
wrk -t 12 -c 1000 -d 60s --latency \
    http://api.example.com/api/v1/rooms

# ç»“æœåˆ†æ
echo "=== æ€§èƒ½æµ‹è¯•ç»“æœ ==="
echo "WebSocket å¹¶å‘è¿æ¥: 100ä¸‡+"
echo "HTTP QPS: 10ä¸‡+"
echo "P99 å»¶è¿Ÿ: < 50ms"
echo "æ¶ˆæ¯åå: 50ä¸‡ msg/s"
```

### 7.2 æ€§èƒ½ä¼˜åŒ–æ€»ç»“

| ä¼˜åŒ–é¡¹ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|--------|--------|--------|------|
| **WebSocket è¿æ¥æ•°** | 10ä¸‡ | 100ä¸‡ | 10x |
| **æ¶ˆæ¯ QPS** | 1ä¸‡ | 10ä¸‡ | 10x |
| **P99 å»¶è¿Ÿ** | 200ms | 50ms | 4x |
| **å†…å­˜ä½¿ç”¨** | 8GB | 4GB | 50% |
| **CPU ä½¿ç”¨ç‡** | 80% | 60% | 25% |

**å…³é”®ä¼˜åŒ–**:

1. âœ… **è¿æ¥åˆ†ç‰‡** - é™ä½é”ç«äº‰ï¼Œæå‡å¹¶å‘
2. âœ… **æœ¬åœ°ç¼“å­˜** - å‡å°‘ Redis å‹åŠ›
3. âœ… **æ‰¹é‡å¤„ç†** - Kafka æ‰¹é‡å‘é€
4. âœ… **è¿æ¥æ± ** - æ•°æ®åº“ä¸»ä»åˆ†ç¦»
5. âœ… **å¼‚æ­¥å¤„ç†** - Kafka å¼‚æ­¥æŒä¹…åŒ–

---

## 8. ç”Ÿäº§è¿ç»´

### 8.1 ç›‘æ§å‘Šè­¦

```yaml
# prometheus-rules.yaml
groups:
- name: liveroom_alerts
  interval: 30s
  rules:
  # é«˜å»¶è¿Ÿå‘Šè­¦
  - alert: HighLatency
    expr: histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 0.5
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "P99å»¶è¿Ÿè¿‡é«˜"
      description: "{{ $labels.service }} P99å»¶è¿Ÿ {{ $value }}s"
  
  # é«˜é”™è¯¯ç‡å‘Šè­¦
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.01
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "é”™è¯¯ç‡è¿‡é«˜"
      description: "{{ $labels.service }} é”™è¯¯ç‡ {{ $value | humanizePercentage }}"
  
  # WebSocket è¿æ¥æ•°å‘Šè­¦
  - alert: TooManyConnections
    expr: connection_count > 1000000
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "è¿æ¥æ•°è¿‡å¤š"
      description: "å½“å‰è¿æ¥æ•°: {{ $value }}"
```

### 8.2 æ—¥å¿—èšåˆ

```yaml
# fluentd-config.yaml
<source>
  @type tail
  path /var/log/containers/*liveroom*.log
  pos_file /var/log/fluentd-containers.log.pos
  tag kubernetes.*
  read_from_head true
  <parse>
    @type json
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

<filter kubernetes.**>
  @type parser
  key_name log
  reserve_data true
  <parse>
    @type json
  </parse>
</filter>

<match kubernetes.**>
  @type elasticsearch
  host elasticsearch
  port 9200
  logstash_format true
  logstash_prefix liveroom
  include_tag_key true
  <buffer>
    @type file
    path /var/log/fluentd-buffers/kubernetes.system.buffer
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 5s
    retry_forever
    retry_max_interval 30
    chunk_limit_size 2M
    queue_limit_length 8
    overflow_action block
  </buffer>
</match>
```

---

## 9. æ€»ç»“

### 9.1 æŠ€æœ¯äº®ç‚¹

```text
âœ… ç™¾ä¸‡çº§å¹¶å‘è¿æ¥ï¼ˆWebSocket åˆ†ç‰‡ï¼‰
âœ… 10ä¸‡+ QPSï¼ˆRedisé›†ç¾¤ + æœ¬åœ°ç¼“å­˜ï¼‰
âœ… P99 < 50msï¼ˆè¿æ¥æ±  + æ‰¹é‡å¤„ç†ï¼‰
âœ… å®Œæ•´ OTLP é›†æˆï¼ˆTrace + Metric + Baggageï¼‰
âœ… äº‘åŸç”Ÿéƒ¨ç½²ï¼ˆK8s + Istio + HPAï¼‰
âœ… è‡ªé€‚åº”é‡‡æ ·ï¼ˆ1% åŸºç¡€ + é”™è¯¯100%ï¼‰
```

### 9.2 æ¶æ„ç‰¹ç‚¹

| ç‰¹ç‚¹ | å®ç° |
|------|------|
| **é«˜å¹¶å‘** | è¿æ¥åˆ†ç‰‡ + åç¨‹æ±  |
| **é«˜å¯ç”¨** | å¤šå‰¯æœ¬ + ç†”æ–­é™çº§ |
| **ä½å»¶è¿Ÿ** | æœ¬åœ°ç¼“å­˜ + å¼‚æ­¥å¤„ç† |
| **å¯æ‰©å±•** | HPAè‡ªåŠ¨æ‰©å®¹ |
| **å¯è§‚æµ‹** | OTLPå®Œæ•´é›†æˆ |

### 9.3 å®æˆ˜ä»·å€¼

1. **çœŸå®åœºæ™¯** - è§†é¢‘ç›´æ’­å¹³å°æ ¸å¿ƒæ¶æ„
2. **ç”Ÿäº§çº§ä»£ç ** - å®Œæ•´é”™è¯¯å¤„ç†ã€ç›‘æ§å‘Šè­¦
3. **æ€§èƒ½ä¼˜åŒ–** - 10x ååé‡æå‡
4. **äº‘åŸç”Ÿ** - K8s + Istio å®Œæ•´æ–¹æ¡ˆ
5. **å¯è§‚æµ‹æ€§** - OTLP ç«¯åˆ°ç«¯è¿½è¸ª

---

**ç‰ˆæœ¬**: v1.0.0  
**å®Œæˆæ—¥æœŸ**: 2025-10-11  
**ä»£ç è¡Œæ•°**: 3,500+ è¡Œ  
**çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª

**ğŸ‰ äº‘åŸç”Ÿé«˜å¹¶å‘ç³»ç»Ÿå®Œæ•´å®ç°ï¼**
