# ğŸš€ Goç”Ÿäº§ç¯å¢ƒéƒ¨ç½²è¿ç»´æŒ‡å— - ä¼ä¸šçº§å®æˆ˜

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
> **åˆ›å»ºæ—¥æœŸ**: 2025-10-17  
> **ç›®æ ‡è¡Œæ•°**: 2,000+ è¡Œ  
> **æŠ€æœ¯æ ˆ**: Go 1.25.1 + Kubernetes 1.31+ + Docker + Helm + OTLP + Prometheus + Grafana

---

## ğŸ“‹ æ–‡æ¡£æ¦‚è§ˆ

æœ¬æŒ‡å—ä¸“æ³¨äº**Goåº”ç”¨ç¨‹åºåœ¨ç”Ÿäº§ç¯å¢ƒçš„éƒ¨ç½²ã€è¿ç»´å’Œå¯è§‚æµ‹æ€§**ï¼Œæ¶µç›–å®¹å™¨åŒ–ã€Kubernetesç¼–æ’ã€é…ç½®ç®¡ç†ã€ç›‘æ§å‘Šè­¦ã€æ—¥å¿—èšåˆã€æ€§èƒ½ä¼˜åŒ–ã€å®‰å…¨åŠ å›ºã€ç¾éš¾æ¢å¤ç­‰ä¼ä¸šçº§æœ€ä½³å®è·µã€‚

### ğŸ¯ æ ¸å¿ƒç›®æ ‡

1. **å®¹å™¨åŒ–**: Dockerå¤šé˜¶æ®µæ„å»ºã€é•œåƒä¼˜åŒ–ã€å®‰å…¨æ‰«æ
2. **Kubernetesç¼–æ’**: Deploymentã€StatefulSetã€DaemonSetã€æ»šåŠ¨æ›´æ–°ã€HPA/VPA
3. **é…ç½®ç®¡ç†**: ConfigMapã€Secretã€ç¯å¢ƒå˜é‡æ³¨å…¥ã€åŠ¨æ€é…ç½®çƒ­åŠ è½½
4. **å¯è§‚æµ‹æ€§**: OTLPå…¨é“¾è·¯ã€PrometheusæŒ‡æ ‡ã€Grafanaå¯è§†åŒ–ã€å‘Šè­¦è§„åˆ™
5. **æ—¥å¿—èšåˆ**: ç»“æ„åŒ–æ—¥å¿—ã€ELK/Lokié›†æˆã€æ—¥å¿—ç­‰çº§åŠ¨æ€è°ƒæ•´
6. **å¥åº·æ£€æŸ¥**: Liveness/Readiness/Startup Probeã€ä¼˜é›…å¯åœ
7. **æ€§èƒ½ä¼˜åŒ–**: èµ„æºé…é¢ã€è¿æ¥æ± ã€å¹¶å‘æ§åˆ¶ã€å†…å­˜ä¼˜åŒ–
8. **å®‰å…¨åŠ å›º**: RBACã€Pod Security Standardsã€SecretåŠ å¯†ã€é•œåƒç­¾å
9. **ç¾éš¾æ¢å¤**: å¤‡ä»½ç­–ç•¥ã€å¤šåŒºåŸŸéƒ¨ç½²ã€æ•…éšœæ¼”ç»ƒ

---

## ç¬¬1ç« : Goåº”ç”¨å®¹å™¨åŒ–æœ€ä½³å®è·µ

### 1.1 Dockerå¤šé˜¶æ®µæ„å»º

#### åŸºç¡€å¤šé˜¶æ®µDockerfile

```dockerfile
# ============================================================
# Stage 1: æ„å»ºé˜¶æ®µ
# ============================================================
FROM golang:1.25.1-alpine AS builder

# å®‰è£…å¿…è¦çš„æ„å»ºå·¥å…·
RUN apk add --no-cache git ca-certificates tzdata

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /build

# å¤åˆ¶go.modå’Œgo.sumï¼Œåˆ©ç”¨Dockerç¼“å­˜å±‚
COPY go.mod go.sum ./
RUN go mod download

# å¤åˆ¶æºä»£ç 
COPY . .

# ç¼–è¯‘Goåº”ç”¨ï¼ˆå¯ç”¨é™æ€ç¼–è¯‘ã€å…³é—­CGOã€å»é™¤è°ƒè¯•ä¿¡æ¯ï¼‰
RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build \
    -ldflags="-w -s -X main.Version=${VERSION} -X main.BuildTime=$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
    -o /app/server \
    ./cmd/server

# ============================================================
# Stage 2: è¿è¡Œæ—¶é˜¶æ®µï¼ˆæœ€å°åŒ–é•œåƒï¼‰
# ============================================================
FROM scratch

# ä»builderå¤åˆ¶å¿…è¦æ–‡ä»¶
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/
COPY --from=builder /usr/share/zoneinfo /usr/share/zoneinfo
COPY --from=builder /app/server /server

# è®¾ç½®æ—¶åŒº
ENV TZ=Asia/Shanghai

# æš´éœ²ç«¯å£
EXPOSE 8080 9090

# ä½¿ç”¨érootç”¨æˆ·ï¼ˆé€šè¿‡COPYæ—¶è®¾ç½®æƒé™ï¼‰
USER 65534:65534

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=5s --start-period=10s --retries=3 \
    CMD ["/server", "health"]

# å¯åŠ¨å‘½ä»¤
ENTRYPOINT ["/server"]
CMD ["--config", "/etc/app/config.yaml"]
```

**å…³é”®ä¼˜åŒ–ç‚¹**:

- âœ… ä½¿ç”¨`scratch`åŸºç¡€é•œåƒï¼ˆæœ€ç»ˆé•œåƒä»…10-20MBï¼‰
- âœ… é™æ€ç¼–è¯‘ï¼ˆ`CGO_ENABLED=0`ï¼‰å»é™¤ä¾èµ–
- âœ… å»é™¤è°ƒè¯•ç¬¦å·ï¼ˆ`-w -s`ï¼‰å‡å°ä½“ç§¯
- âœ… åˆ†å±‚å¤åˆ¶`go.mod`/`go.sum`åˆ©ç”¨ç¼“å­˜
- âœ… érootç”¨æˆ·è¿è¡Œï¼ˆUID 65534 = nobodyï¼‰

#### å¸¦CGOä¾èµ–çš„Dockerfile

æŸäº›åœºæ™¯ï¼ˆå¦‚eBPFã€SQLiteï¼‰éœ€è¦CGOï¼š

```dockerfile
FROM golang:1.25.1-alpine AS builder

# å®‰è£…CGOä¾èµ–
RUN apk add --no-cache gcc musl-dev linux-headers

WORKDIR /build
COPY go.mod go.sum ./
RUN go mod download

COPY . .

# å¯ç”¨CGOä½†é™æ€é“¾æ¥
RUN CGO_ENABLED=1 GOOS=linux GOARCH=amd64 go build \
    -ldflags="-w -s -linkmode external -extldflags '-static'" \
    -o /app/server \
    ./cmd/server

# ============================================================
# è¿è¡Œæ—¶é˜¶æ®µï¼ˆAlpineè€Œéscratchï¼‰
# ============================================================
FROM alpine:3.19

RUN apk add --no-cache ca-certificates tzdata

COPY --from=builder /app/server /server

USER 65534:65534
ENTRYPOINT ["/server"]
```

**CGOåœºæ™¯æƒè¡¡**:

- âŒ é•œåƒå¢å¤§è‡³30-50MBï¼ˆAlpineåŸºç¡€é•œåƒï¼‰
- âœ… æ”¯æŒéœ€è¦CGOçš„åº“ï¼ˆå¦‚`cilium/ebpf`ï¼‰
- âœ… ä»ç„¶ä¿æŒç›¸å¯¹å°çš„é•œåƒä½“ç§¯

#### é•œåƒå®‰å…¨æ‰«æ

ä½¿ç”¨Trivyæ‰«ææ¼æ´ï¼š

```bash
# æ„å»ºé•œåƒ
docker build -t myapp:1.0.0 .

# Trivyæ‰«æ
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
    aquasec/trivy image \
    --severity HIGH,CRITICAL \
    --exit-code 1 \
    myapp:1.0.0

# æ‰«æè¾“å‡ºç¤ºä¾‹
myapp:1.0.0 (alpine 3.19.0)
============================
Total: 0 (HIGH: 0, CRITICAL: 0)
```

**CI/CDé›†æˆ**:

```yaml
# .gitlab-ci.yml
security:scan:
  stage: test
  image: aquasec/trivy:latest
  script:
    - trivy image --exit-code 1 --severity HIGH,CRITICAL $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
  only:
    - merge_requests
    - main
```

---

### 1.2 é•œåƒä¼˜åŒ–æŠ€å·§

#### å‡å°é•œåƒä½“ç§¯

| æŠ€æœ¯ | ä½“ç§¯å¯¹æ¯” | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|
| scratchåŸºç¡€é•œåƒ | 10-20 MB | çº¯Goåº”ç”¨ï¼ˆæ— CGOï¼‰ |
| AlpineåŸºç¡€é•œåƒ | 30-50 MB | éœ€è¦shellæˆ–CGOä¾èµ– |
| Distroless | 15-25 MB | Googleæ¨èçš„æœ€å°è¿è¡Œæ—¶ |
| UPXå‹ç¼© | å‡å°‘40-60% | å¯æ‰§è¡Œæ–‡ä»¶å‹ç¼©ï¼ˆéœ€æƒè¡¡å¯åŠ¨æ—¶é—´ï¼‰ |

**UPXå‹ç¼©ç¤ºä¾‹**:

```dockerfile
FROM golang:1.25.1-alpine AS builder
RUN apk add --no-cache upx

WORKDIR /build
COPY . .
RUN go build -o /app/server ./cmd/server

# UPXå‹ç¼©ï¼ˆå‹ç¼©çº§åˆ«9ï¼‰
RUN upx --best --lzma /app/server

FROM scratch
COPY --from=builder /app/server /server
ENTRYPOINT ["/server"]
```

**å‹ç¼©æ•ˆæœ**:

- åŸå§‹äºŒè¿›åˆ¶: 45 MB
- UPXå‹ç¼©å: 18 MBï¼ˆå‡å°‘60%ï¼‰
- âš ï¸ å¯åŠ¨æ—¶é—´å¢åŠ 5-10msï¼ˆéœ€è§£å‹ï¼‰

#### .dockerignoreä¼˜åŒ–

```bash
# .dockerignore
.git
.github
.vscode
*.md
*.swp
*~
.DS_Store
vendor/
test/
coverage.out
*.test
deployment/
docs/
```

---

### 1.3 é•œåƒåˆ†å‘ç­–ç•¥

#### Harborç§æœ‰é•œåƒä»“åº“

```yaml
# docker-compose.yml (Harboréƒ¨ç½²)
version: '3.8'
services:
  harbor:
    image: goharbor/harbor-core:v2.10.0
    ports:
      - "443:443"
    volumes:
      - ./harbor.yml:/etc/harbor/harbor.yml
      - /data/harbor:/data
    environment:
      - HARBOR_ADMIN_PASSWORD=YourSecurePassword
```

**é•œåƒæ¨é€æµç¨‹**:

```bash
# ç™»å½•Harbor
docker login harbor.example.com

# æ‰“æ ‡ç­¾
docker tag myapp:1.0.0 harbor.example.com/production/myapp:1.0.0

# æ¨é€
docker push harbor.example.com/production/myapp:1.0.0

# Kubernetesæ‹‰å–ï¼ˆé…ç½®ImagePullSecretsï¼‰
kubectl create secret docker-registry harbor-secret \
    --docker-server=harbor.example.com \
    --docker-username=admin \
    --docker-password=YourSecurePassword \
    --namespace=production
```

#### é•œåƒç¼“å­˜åŠ é€Ÿ

```yaml
# /etc/docker/daemon.json
{
  "registry-mirrors": [
    "https://registry.aliyuncs.com",
    "https://docker.mirrors.ustc.edu.cn"
  ],
  "insecure-registries": ["harbor.example.com"],
  "max-concurrent-downloads": 10,
  "max-concurrent-uploads": 5
}
```

---

## ç¬¬2ç« : Kuberneteséƒ¨ç½²ç¼–æ’

### 2.1 Deploymentæœ€ä½³å®è·µ

#### ç”Ÿäº§çº§Deployment YAML

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  namespace: production
  labels:
    app: myapp
    version: v1.0.0
    env: production
  annotations:
    deployment.kubernetes.io/revision: "1"
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # æ»šåŠ¨æ›´æ–°æ—¶æœ€å¤šå¤šåˆ›å»º1ä¸ªPod
      maxUnavailable: 0  # ç¡®ä¿é›¶åœæœº
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
        version: v1.0.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      # åäº²å’Œæ€§ï¼šç¡®ä¿Podåˆ†æ•£åœ¨ä¸åŒèŠ‚ç‚¹
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - myapp
                topologyKey: kubernetes.io/hostname
      
      # èŠ‚ç‚¹é€‰æ‹©å™¨ï¼šéƒ¨ç½²åˆ°ç‰¹å®šèŠ‚ç‚¹æ± 
      nodeSelector:
        node-role.kubernetes.io/worker: "true"
        workload-type: compute-optimized
      
      # å®¹å¿åº¦ï¼šå…è®¸åœ¨æœ‰ç‰¹å®šæ±¡ç‚¹çš„èŠ‚ç‚¹è¿è¡Œ
      tolerations:
        - key: "workload-type"
          operator: "Equal"
          value: "compute-optimized"
          effect: "NoSchedule"
      
      # Initå®¹å™¨ï¼šé¢„çƒ­æ•°æ®æˆ–ç­‰å¾…ä¾èµ–
      initContainers:
        - name: wait-for-redis
          image: busybox:1.36
          command:
            - sh
            - -c
            - |
              until nc -z redis-service 6379; do
                echo "ç­‰å¾…Rediså¯åŠ¨..."
                sleep 2
              done
      
      containers:
        - name: myapp
          image: harbor.example.com/production/myapp:1.0.0
          imagePullPolicy: IfNotPresent
          
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          
          # ç¯å¢ƒå˜é‡
          env:
            - name: GO_ENV
              value: "production"
            - name: LOG_LEVEL
              value: "info"
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  resource: limits.cpu
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myapp-secrets
                  key: redis-password
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          
          # èµ„æºé™åˆ¶
          resources:
            requests:
              cpu: "500m"      # 0.5æ ¸
              memory: "512Mi"
            limits:
              cpu: "2000m"     # 2æ ¸
              memory: "2Gi"
          
          # å¥åº·æ£€æŸ¥
          livenessProbe:
            httpGet:
              path: /health/live
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          
          readinessProbe:
            httpGet:
              path: /health/ready
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 3
          
          startupProbe:
            httpGet:
              path: /health/startup
              port: 8080
            initialDelaySeconds: 0
            periodSeconds: 5
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 30  # å…è®¸150ç§’å¯åŠ¨æ—¶é—´
          
          # æŒ‚è½½é…ç½®
          volumeMounts:
            - name: config
              mountPath: /etc/app
              readOnly: true
            - name: tmp
              mountPath: /tmp
            - name: cache
              mountPath: /var/cache/app
          
          # å®‰å…¨ä¸Šä¸‹æ–‡
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
      
      # å·å®šä¹‰
      volumes:
        - name: config
          configMap:
            name: myapp-config
        - name: tmp
          emptyDir: {}
        - name: cache
          emptyDir:
            sizeLimit: 1Gi
      
      # é•œåƒæ‹‰å–å‡­è¯
      imagePullSecrets:
        - name: harbor-secret
      
      # ä¼˜é›…ç»ˆæ­¢
      terminationGracePeriodSeconds: 60
```

**æ ¸å¿ƒé…ç½®è¯´æ˜**:

1. **åäº²å’Œæ€§**: ç¡®ä¿Podåˆ†æ•£åœ¨ä¸åŒç‰©ç†èŠ‚ç‚¹ï¼Œæé«˜å¯ç”¨æ€§
2. **èµ„æºé™åˆ¶**:
   - `requests`: è°ƒåº¦å™¨ä¿è¯çš„æœ€å°èµ„æº
   - `limits`: ç¡¬æ€§ä¸Šé™ï¼Œè¶…å‡ºä¼šè¢«é™æµæˆ–OOM
3. **å¥åº·æ£€æŸ¥ä¸‰é‡å¥**:
   - `startupProbe`: åˆå§‹åŒ–é˜¶æ®µï¼ˆå…è®¸è¾ƒé•¿å¯åŠ¨æ—¶é—´ï¼‰
   - `livenessProbe`: å­˜æ´»æ£€æŸ¥ï¼ˆå¤±è´¥åˆ™é‡å¯Podï¼‰
   - `readinessProbe`: å°±ç»ªæ£€æŸ¥ï¼ˆå¤±è´¥åˆ™ä»Serviceç§»é™¤ï¼‰
4. **å®‰å…¨ä¸Šä¸‹æ–‡**: érootç”¨æˆ·ã€åªè¯»æ–‡ä»¶ç³»ç»Ÿã€ç¦ç”¨ç‰¹æƒæå‡

#### Goå¥åº·æ£€æŸ¥å®ç°

```go
// cmd/server/main.go
package main

import (
 "context"
 "fmt"
 "net/http"
 "os"
 "os/signal"
 "sync/atomic"
 "syscall"
 "time"

 "github.com/gin-gonic/gin"
)

var (
 // åŸå­æ ‡å¿—ä½
 healthy   atomic.Bool
 ready     atomic.Bool
 startupOK atomic.Bool
)

func main() {
 // åˆå§‹åŒ–çŠ¶æ€
 healthy.Store(true)
 ready.Store(false)
 startupOK.Store(false)

 // å¯åŠ¨HTTPæœåŠ¡å™¨
 router := gin.Default()
 setupHealthEndpoints(router)

 server := &http.Server{
  Addr:    ":8080",
  Handler: router,
 }

 // åå°å¯åŠ¨
 go func() {
  if err := server.ListenAndServe(); err != nil && err != http.ErrServerClosed {
   fmt.Printf("HTTP server error: %v\n", err)
   healthy.Store(false)
  }
 }()

 // æ¨¡æ‹Ÿå¯åŠ¨ä»»åŠ¡ï¼ˆé¢„çƒ­ç¼“å­˜ã€è¿æ¥æ•°æ®åº“ç­‰ï¼‰
 go func() {
  time.Sleep(5 * time.Second) // æ¨¡æ‹Ÿå¯åŠ¨å»¶è¿Ÿ
  startupOK.Store(true)
  ready.Store(true)
  fmt.Println("Application startup complete")
 }()

 // ä¼˜é›…å…³é—­
 quit := make(chan os.Signal, 1)
 signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
 <-quit

 fmt.Println("Shutting down server...")
 ready.Store(false) // ç«‹å³æ ‡è®°ä¸ºä¸å¯ç”¨

 ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
 defer cancel()

 if err := server.Shutdown(ctx); err != nil {
  fmt.Printf("Server forced to shutdown: %v\n", err)
 }
 fmt.Println("Server exited")
}

func setupHealthEndpoints(router *gin.Engine) {
 health := router.Group("/health")
 {
  // Startup Probe: å¯åŠ¨æ£€æŸ¥
  health.GET("/startup", func(c *gin.Context) {
   if startupOK.Load() {
    c.JSON(http.StatusOK, gin.H{
     "status": "ok",
     "type":   "startup",
    })
   } else {
    c.JSON(http.StatusServiceUnavailable, gin.H{
     "status": "initializing",
     "type":   "startup",
    })
   }
  })

  // Liveness Probe: å­˜æ´»æ£€æŸ¥
  health.GET("/live", func(c *gin.Context) {
   if healthy.Load() {
    c.JSON(http.StatusOK, gin.H{
     "status": "alive",
     "type":   "liveness",
    })
   } else {
    c.JSON(http.StatusServiceUnavailable, gin.H{
     "status": "dead",
     "type":   "liveness",
    })
   }
  })

  // Readiness Probe: å°±ç»ªæ£€æŸ¥
  health.GET("/ready", func(c *gin.Context) {
   if ready.Load() && healthy.Load() {
    // æ£€æŸ¥ä¾èµ–ï¼ˆæ•°æ®åº“ã€Redisç­‰ï¼‰
    if checkDependencies() {
     c.JSON(http.StatusOK, gin.H{
      "status": "ready",
      "type":   "readiness",
     })
     return
    }
   }
   c.JSON(http.StatusServiceUnavailable, gin.H{
    "status": "not_ready",
    "type":   "readiness",
   })
  })
 }
}

func checkDependencies() bool {
 // æ£€æŸ¥Redisè¿æ¥
 // æ£€æŸ¥æ•°æ®åº“è¿æ¥
 // æ£€æŸ¥å¤–éƒ¨APIå¯ç”¨æ€§
 return true // ç®€åŒ–ç¤ºä¾‹
}
```

**å¥åº·æ£€æŸ¥æœ€ä½³å®è·µ**:

- âœ… **StartupProbe**: ä»…æ£€æŸ¥å¯åŠ¨ä»»åŠ¡æ˜¯å¦å®Œæˆ
- âœ… **LivenessProbe**: æ£€æŸ¥è¿›ç¨‹æ˜¯å¦æ­»é”æˆ–å´©æºƒï¼ˆä¸è¦æ£€æŸ¥å¤–éƒ¨ä¾èµ–ï¼‰
- âœ… **ReadinessProbe**: æ£€æŸ¥æœåŠ¡æ˜¯å¦èƒ½å¤„ç†æµé‡ï¼ˆåŒ…æ‹¬å¤–éƒ¨ä¾èµ–ï¼‰
- âš ï¸ é¿å…Livenessæ£€æŸ¥å¤–éƒ¨ä¾èµ–ï¼Œå¦åˆ™ä¼šå¯¼è‡´é›ªå´©æ•ˆåº”

---

### 2.2 Serviceä¸Ingressé…ç½®

#### Service YAML

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
  namespace: production
  labels:
    app: myapp
  annotations:
    # Prometheus Service Discovery
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"
spec:
  type: ClusterIP
  sessionAffinity: None
  selector:
    app: myapp
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: http
    - name: metrics
      protocol: TCP
      port: 9090
      targetPort: metrics
```

#### Ingressé…ç½®ï¼ˆNGINX Ingress Controllerï¼‰

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp-ingress
  namespace: production
  annotations:
    kubernetes.io/ingress.class: "nginx"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    
    # é€Ÿç‡é™åˆ¶
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/limit-connections: "20"
    
    # è¶…æ—¶é…ç½®
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "30"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "60"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "60"
    
    # CORS
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://example.com"
    
    # è¯·æ±‚ä½“å¤§å°é™åˆ¶
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
    
    # ç™½åå•
    nginx.ingress.kubernetes.io/whitelist-source-range: "10.0.0.0/8,172.16.0.0/12"
spec:
  tls:
    - hosts:
        - api.example.com
      secretName: myapp-tls-cert
  rules:
    - host: api.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: myapp-service
                port:
                  number: 80
```

---

### 2.3 è‡ªåŠ¨æ‰©ç¼©å®¹ï¼ˆHPA/VPAï¼‰

#### HPAï¼ˆæ°´å¹³Podè‡ªåŠ¨æ‰©ç¼©å®¹ï¼‰

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 3
  maxReplicas: 20
  metrics:
    # CPUåˆ©ç”¨ç‡
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    
    # å†…å­˜åˆ©ç”¨ç‡
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
    
    # è‡ªå®šä¹‰æŒ‡æ ‡ï¼šè¯·æ±‚QPS
    - type: Pods
      pods:
        metric:
          name: http_requests_per_second
        target:
          type: AverageValue
          averageValue: "1000"
  
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # 5åˆ†é’Ÿç¨³å®šæœŸ
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
        - type: Pods
          value: 2
          periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
        - type: Pods
          value: 4
          periodSeconds: 30
      selectPolicy: Max
```

#### VPAï¼ˆå‚ç›´Podè‡ªåŠ¨æ‰©ç¼©å®¹ï¼‰

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: myapp-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  updatePolicy:
    updateMode: "Auto"  # è‡ªåŠ¨è°ƒæ•´ï¼ˆä¼šé‡å¯Podï¼‰
  resourcePolicy:
    containerPolicies:
      - containerName: myapp
        minAllowed:
          cpu: 500m
          memory: 512Mi
        maxAllowed:
          cpu: 4000m
          memory: 8Gi
        controlledResources:
          - cpu
          - memory
```

**HPA vs VPA å¯¹æ¯”**:

| ç‰¹æ€§ | HPA | VPA |
|------|-----|-----|
| æ‰©ç¼©ç»´åº¦ | æ°´å¹³ï¼ˆå¢å‡Podæ•°ï¼‰ | å‚ç›´ï¼ˆè°ƒæ•´èµ„æºé…é¢ï¼‰ |
| é€‚ç”¨åœºæ™¯ | æ— çŠ¶æ€æœåŠ¡ | æœ‰çŠ¶æ€æœåŠ¡ã€æ•°æ®åº“ |
| å“åº”é€Ÿåº¦ | å¿«ï¼ˆç§’çº§ï¼‰ | æ…¢ï¼ˆéœ€é‡å¯Podï¼‰ |
| æˆæœ¬ä¼˜åŒ– | æŒ‰éœ€æ‰©å®¹ | ç²¾å‡†é…é¢é¿å…æµªè´¹ |
| å…¼å®¹æ€§ | âš ï¸ ä¸èƒ½ä¸VPAåŒæ—¶ä½¿ç”¨CPU/å†…å­˜æŒ‡æ ‡ | âš ï¸ éœ€è¦v1.25+ Kubernetes |

---

## ç¬¬3ç« : é…ç½®ç®¡ç†ä¸Secretå®‰å…¨

### 3.1 ConfigMapç®¡ç†

#### åº”ç”¨é…ç½®ç¤ºä¾‹

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myapp-config
  namespace: production
data:
  # ç®€å•é”®å€¼å¯¹
  LOG_LEVEL: "info"
  HTTP_TIMEOUT: "30s"
  
  # å®Œæ•´é…ç½®æ–‡ä»¶
  config.yaml: |
    server:
      host: 0.0.0.0
      port: 8080
      read_timeout: 30s
      write_timeout: 60s
    
    database:
      host: postgres-service
      port: 5432
      max_connections: 100
      max_idle_connections: 10
      connection_max_lifetime: 1h
    
    redis:
      host: redis-service
      port: 6379
      pool_size: 50
      max_retries: 3
    
    otlp:
      endpoint: otel-collector:4317
      timeout: 10s
      retry:
        enabled: true
        max_elapsed_time: 1m
    
    feature_flags:
      new_checkout: true
      experimental_ui: false
```

#### Goé…ç½®çƒ­åŠ è½½

```go
// pkg/config/watcher.go
package config

import (
 "fmt"
 "os"
 "sync"
 "time"

 "github.com/fsnotify/fsnotify"
 "gopkg.in/yaml.v3"
)

type Config struct {
 mu sync.RWMutex
 
 Server struct {
  Host         string        `yaml:"host"`
  Port         int           `yaml:"port"`
  ReadTimeout  time.Duration `yaml:"read_timeout"`
  WriteTimeout time.Duration `yaml:"write_timeout"`
 } `yaml:"server"`
 
 Database struct {
  Host                  string        `yaml:"host"`
  Port                  int           `yaml:"port"`
  MaxConnections        int           `yaml:"max_connections"`
  MaxIdleConnections    int           `yaml:"max_idle_connections"`
  ConnectionMaxLifetime time.Duration `yaml:"connection_max_lifetime"`
 } `yaml:"database"`
 
 FeatureFlags map[string]bool `yaml:"feature_flags"`
}

type Watcher struct {
 filePath string
 config   *Config
 watcher  *fsnotify.Watcher
 onChange func(*Config)
}

func NewWatcher(filePath string, onChange func(*Config)) (*Watcher, error) {
 w := &Watcher{
  filePath: filePath,
  config:   &Config{},
  onChange: onChange,
 }
 
 // åˆå§‹åŠ è½½
 if err := w.reload(); err != nil {
  return nil, fmt.Errorf("initial load failed: %w", err)
 }
 
 // åˆ›å»ºfsnotifyç›‘è§†å™¨
 watcher, err := fsnotify.NewWatcher()
 if err != nil {
  return nil, fmt.Errorf("create watcher failed: %w", err)
 }
 w.watcher = watcher
 
 if err := watcher.Add(filePath); err != nil {
  return nil, fmt.Errorf("add watch failed: %w", err)
 }
 
 // å¯åŠ¨ç›‘è§†goroutine
 go w.watch()
 
 return w, nil
}

func (w *Watcher) reload() error {
 data, err := os.ReadFile(w.filePath)
 if err != nil {
  return fmt.Errorf("read file failed: %w", err)
 }
 
 newConfig := &Config{}
 if err := yaml.Unmarshal(data, newConfig); err != nil {
  return fmt.Errorf("parse yaml failed: %w", err)
 }
 
 w.config.mu.Lock()
 *w.config = *newConfig
 w.config.mu.Unlock()
 
 fmt.Printf("Config reloaded at %s\n", time.Now().Format(time.RFC3339))
 
 if w.onChange != nil {
  w.onChange(w.config)
 }
 
 return nil
}

func (w *Watcher) watch() {
 for {
  select {
  case event, ok := <-w.watcher.Events:
   if !ok {
    return
   }
   
   // ConfigMapæ›´æ–°ä¼šè§¦å‘WRITEæˆ–CREATEäº‹ä»¶
   if event.Op&fsnotify.Write == fsnotify.Write || event.Op&fsnotify.Create == fsnotify.Create {
    fmt.Printf("Config file changed: %s\n", event.Name)
    time.Sleep(100 * time.Millisecond) // é˜²æŠ–
    if err := w.reload(); err != nil {
     fmt.Printf("Reload config failed: %v\n", err)
    }
   }
   
  case err, ok := <-w.watcher.Errors:
   if !ok {
    return
   }
   fmt.Printf("Watcher error: %v\n", err)
  }
 }
}

func (w *Watcher) Get() *Config {
 w.config.mu.RLock()
 defer w.config.mu.RUnlock()
 
 // è¿”å›æ‹·è´é¿å…å¹¶å‘è¯»å†™
 cfg := *w.config
 return &cfg
}

func (w *Watcher) Close() error {
 return w.watcher.Close()
}
```

**ä½¿ç”¨ç¤ºä¾‹**:

```go
func main() {
 watcher, err := config.NewWatcher("/etc/app/config.yaml", func(cfg *config.Config) {
  // é…ç½®æ›´æ–°å›è°ƒ
  fmt.Printf("New feature flags: %+v\n", cfg.FeatureFlags)
 })
 if err != nil {
  panic(err)
 }
 defer watcher.Close()
 
 // è·å–å½“å‰é…ç½®
 cfg := watcher.Get()
 fmt.Printf("Database host: %s\n", cfg.Database.Host)
}
```

---

### 3.2 Secretç®¡ç†

#### åˆ›å»ºSecret

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: myapp-secrets
  namespace: production
type: Opaque
stringData:
  # è‡ªåŠ¨Base64ç¼–ç 
  postgres-password: "SuperSecretPassword123!"
  redis-password: "AnotherSecretPassword!"
  jwt-secret: "your-256-bit-secret-key-here"
  
  # å®Œæ•´æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
  database-url: "postgresql://user:password@postgres-service:5432/mydb?sslmode=require"
```

#### ç¯å¢ƒå˜é‡æ³¨å…¥

```yaml
spec:
  containers:
    - name: myapp
      env:
        # å•ä¸ªSecreté”®
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: myapp-secrets
              key: postgres-password
        
        # æ‰¹é‡æ³¨å…¥ï¼ˆæ‰€æœ‰é”®ï¼‰
        envFrom:
          - secretRef:
              name: myapp-secrets
              prefix: APP_  # æ·»åŠ å‰ç¼€é¿å…å†²çª
```

#### æ–‡ä»¶æŒ‚è½½æ–¹å¼

```yaml
spec:
  containers:
    - name: myapp
      volumeMounts:
        - name: secrets
          mountPath: /var/secrets
          readOnly: true
  volumes:
    - name: secrets
      secret:
        secretName: myapp-secrets
        items:
          - key: database-url
            path: db_url.txt  # æ˜ å°„åˆ°/var/secrets/db_url.txt
```

#### Sealed Secretsï¼ˆåŠ å¯†å­˜å‚¨ï¼‰

å®‰è£…Sealed Secrets Controller:

```bash
kubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.24.0/controller.yaml

# å®‰è£…kubeseal CLI
wget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.24.0/kubeseal-linux-amd64
sudo install -m 755 kubeseal-linux-amd64 /usr/local/bin/kubeseal
```

åˆ›å»ºSealedSecret:

```bash
# åˆ›å»ºæ™®é€šSecret YAML
kubectl create secret generic myapp-secrets \
    --from-literal=postgres-password=SuperSecret123 \
    --dry-run=client -o yaml > secret.yaml

# åŠ å¯†ä¸ºSealedSecret
kubeseal < secret.yaml > sealed-secret.yaml

# æäº¤åˆ°Gitï¼ˆå®‰å…¨ï¼‰
git add sealed-secret.yaml
git commit -m "Add sealed secrets"
```

SealedSecretä¼šè‡ªåŠ¨è§£å¯†ä¸ºSecret:

```yaml
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: myapp-secrets
  namespace: production
spec:
  encryptedData:
    postgres-password: AgCZXj8a...  # åŠ å¯†å†…å®¹
```

---

### 3.3 å¤–éƒ¨Secretç®¡ç†ï¼ˆHashiCorp Vaultï¼‰

#### Vaulté›†æˆæ¶æ„

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vault     â”‚  â† é›†ä¸­å­˜å‚¨æ•æ„Ÿæ•°æ®
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ è®¤è¯ & è¯»å–
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vault Agent Injector â”‚  â† Sidecaræ³¨å…¥
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Go App Pod â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚ Vault  â”‚  â”‚  â† Sidecarå®¹å™¨
    â”‚  â”‚ Agent  â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚  App   â”‚  â”‚  â† åº”ç”¨å®¹å™¨
    â”‚  â”‚Containerâ”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Deploymentæ³¨è§£ï¼ˆVault Sidecaræ³¨å…¥ï¼‰

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  template:
    metadata:
      annotations:
        vault.hashicorp.com/agent-inject: "true"
        vault.hashicorp.com/role: "myapp"
        
        # æ³¨å…¥æ•°æ®åº“å¯†ç 
        vault.hashicorp.com/agent-inject-secret-db-password: "secret/data/myapp/postgres"
        vault.hashicorp.com/agent-inject-template-db-password: |
          {{- with secret "secret/data/myapp/postgres" -}}
          {{ .Data.data.password }}
          {{- end }}
        
        # æ³¨å…¥å®Œæ•´é…ç½®æ–‡ä»¶
        vault.hashicorp.com/agent-inject-secret-config.yaml: "secret/data/myapp/config"
        vault.hashicorp.com/agent-inject-template-config.yaml: |
          {{- with secret "secret/data/myapp/config" -}}
          database:
            host: {{ .Data.data.db_host }}
            password: {{ .Data.data.db_password }}
          redis:
            password: {{ .Data.data.redis_password }}
          {{- end }}
    spec:
      serviceAccountName: myapp
      containers:
        - name: myapp
          image: myapp:1.0.0
          env:
            - name: DB_PASSWORD_FILE
              value: /vault/secrets/db-password
          volumeMounts:
            - name: vault-secrets
              mountPath: /vault/secrets
              readOnly: true
```

**Vaultä¼˜åŠ¿**:

- âœ… é›†ä¸­ç®¡ç†æ‰€æœ‰æ•æ„Ÿæ•°æ®
- âœ… åŠ¨æ€Secretï¼ˆè‡ªåŠ¨è½®æ¢ï¼‰
- âœ… å®¡è®¡æ—¥å¿—ï¼ˆè°åœ¨ä½•æ—¶è®¿é—®äº†ä»€ä¹ˆï¼‰
- âœ… ç»†ç²’åº¦è®¿é—®æ§åˆ¶ï¼ˆRBACï¼‰

---

## ç¬¬4ç« : OTLPå¯è§‚æµ‹æ€§å…¨é“¾è·¯

### 4.1 OpenTelemetry Go SDKé›†æˆ

#### å®‰è£…ä¾èµ–

```bash
go get go.opentelemetry.io/otel@v1.32.0
go get go.opentelemetry.io/otel/sdk@v1.32.0
go get go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc@v1.32.0
go get go.opentelemetry.io/otel/exporters/otlp/otlpmetric/otlpmetricgrpc@v1.32.0
go get go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp@v0.58.0
go get go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc@v0.58.0
```

#### åˆå§‹åŒ–OTLP Pipeline

```go
// pkg/telemetry/otlp.go
package telemetry

import (
 "context"
 "fmt"
 "time"

 "go.opentelemetry.io/otel"
 "go.opentelemetry.io/otel/exporters/otlp/otlpmetric/otlpmetricgrpc"
 "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
 "go.opentelemetry.io/otel/propagation"
 "go.opentelemetry.io/otel/sdk/metric"
 "go.opentelemetry.io/otel/sdk/resource"
 "go.opentelemetry.io/otel/sdk/trace"
 semconv "go.opentelemetry.io/otel/semconv/v1.26.0"
)

type OTLPConfig struct {
 ServiceName    string
 ServiceVersion string
 Endpoint       string
 Insecure       bool
}

func InitOTLP(ctx context.Context, cfg OTLPConfig) (func(context.Context) error, error) {
 // 1. åˆ›å»ºResourceï¼ˆæœåŠ¡å…ƒæ•°æ®ï¼‰
 res, err := resource.New(ctx,
  resource.WithAttributes(
   semconv.ServiceName(cfg.ServiceName),
   semconv.ServiceVersion(cfg.ServiceVersion),
   semconv.DeploymentEnvironment("production"),
  ),
  resource.WithProcess(),
  resource.WithOS(),
  resource.WithContainer(),
  resource.WithHost(),
 )
 if err != nil {
  return nil, fmt.Errorf("create resource: %w", err)
 }

 // 2. åˆå§‹åŒ–Trace Exporter
 traceExporter, err := otlptracegrpc.New(ctx,
  otlptracegrpc.WithEndpoint(cfg.Endpoint),
  otlptracegrpc.WithInsecure(),
  otlptracegrpc.WithTimeout(10*time.Second),
 )
 if err != nil {
  return nil, fmt.Errorf("create trace exporter: %w", err)
 }

 // 3. åˆ›å»ºTracerProvider
 tp := trace.NewTracerProvider(
  trace.WithBatcher(traceExporter,
   trace.WithMaxExportBatchSize(512),
   trace.WithBatchTimeout(5*time.Second),
  ),
  trace.WithResource(res),
  trace.WithSampler(trace.ParentBased(trace.TraceIDRatioBased(0.1))), // 10%é‡‡æ ·ç‡
 )
 otel.SetTracerProvider(tp)

 // 4. åˆå§‹åŒ–Metric Exporter
 metricExporter, err := otlpmetricgrpc.New(ctx,
  otlpmetricgrpc.WithEndpoint(cfg.Endpoint),
  otlpmetricgrpc.WithInsecure(),
  otlpmetricgrpc.WithTimeout(10*time.Second),
 )
 if err != nil {
  return nil, fmt.Errorf("create metric exporter: %w", err)
 }

 // 5. åˆ›å»ºMeterProvider
 mp := metric.NewMeterProvider(
  metric.WithReader(metric.NewPeriodicReader(metricExporter,
   metric.WithInterval(15*time.Second),
  )),
  metric.WithResource(res),
 )
 otel.SetMeterProvider(mp)

 // 6. è®¾ç½®Context Propagatorï¼ˆæ”¯æŒW3C Trace Contextï¼‰
 otel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(
  propagation.TraceContext{},
  propagation.Baggage{},
 ))

 // è¿”å›shutdownå‡½æ•°
 return func(ctx context.Context) error {
  if err := tp.Shutdown(ctx); err != nil {
   return fmt.Errorf("shutdown tracer provider: %w", err)
  }
  if err := mp.Shutdown(ctx); err != nil {
   return fmt.Errorf("shutdown meter provider: %w", err)
  }
  return nil
 }, nil
}
```

#### HTTPæœåŠ¡å™¨é›†æˆ

```go
// cmd/server/main.go
package main

import (
 "context"
 "fmt"
 "log"
 "net/http"
 "os"
 "os/signal"
 "syscall"
 "time"

 "github.com/gin-gonic/gin"
 "go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin"
 "go.opentelemetry.io/otel"
 "go.opentelemetry.io/otel/attribute"
 "go.opentelemetry.io/otel/metric"
 "go.opentelemetry.io/otel/trace"

 "myapp/pkg/telemetry"
)

var (
 tracer  trace.Tracer
 meter   metric.Meter
 httpReq metric.Int64Counter
)

func main() {
 ctx := context.Background()

 // åˆå§‹åŒ–OTLP
 shutdown, err := telemetry.InitOTLP(ctx, telemetry.OTLPConfig{
  ServiceName:    "myapp",
  ServiceVersion: "1.0.0",
  Endpoint:       os.Getenv("OTEL_EXPORTER_OTLP_ENDPOINT"),
  Insecure:       true,
 })
 if err != nil {
  log.Fatalf("Failed to initialize OTLP: %v", err)
 }
 defer func() {
  if err := shutdown(context.Background()); err != nil {
   log.Printf("Failed to shutdown OTLP: %v", err)
  }
 }()

 // è·å–Tracerå’ŒMeter
 tracer = otel.Tracer("myapp")
 meter = otel.Meter("myapp")

 // åˆ›å»ºè‡ªå®šä¹‰æŒ‡æ ‡
 httpReq, err = meter.Int64Counter(
  "http_requests_total",
  metric.WithDescription("Total HTTP requests"),
  metric.WithUnit("{request}"),
 )
 if err != nil {
  log.Fatalf("Failed to create counter: %v", err)
 }

 // åˆ›å»ºGinè·¯ç”±
 router := gin.Default()
 
 // å…¨å±€OTelä¸­é—´ä»¶
 router.Use(otelgin.Middleware("myapp"))
 
 // ä¸šåŠ¡è·¯ç”±
 router.GET("/api/orders/:id", getOrder)
 router.POST("/api/orders", createOrder)

 // å¯åŠ¨æœåŠ¡å™¨
 server := &http.Server{
  Addr:    ":8080",
  Handler: router,
 }

 go func() {
  if err := server.ListenAndServe(); err != nil && err != http.ErrServerClosed {
   log.Fatalf("Server error: %v", err)
  }
 }()

 // ä¼˜é›…å…³é—­
 quit := make(chan os.Signal, 1)
 signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
 <-quit

 ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
 defer cancel()
 if err := server.Shutdown(ctx); err != nil {
  log.Fatalf("Server shutdown error: %v", err)
 }
}

func getOrder(c *gin.Context) {
 ctx := c.Request.Context()
 
 // åˆ›å»ºå­Span
 ctx, span := tracer.Start(ctx, "getOrder",
  trace.WithAttributes(
   attribute.String("order.id", c.Param("id")),
  ),
 )
 defer span.End()

 // è®°å½•æŒ‡æ ‡
 httpReq.Add(ctx, 1,
  metric.WithAttributes(
   attribute.String("method", "GET"),
   attribute.String("endpoint", "/api/orders/:id"),
  ),
 )

 // æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
 order, err := fetchOrderFromDB(ctx, c.Param("id"))
 if err != nil {
  span.RecordError(err)
  c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
  return
 }

 span.SetAttributes(attribute.Float64("order.amount", order.Amount))
 c.JSON(http.StatusOK, order)
}

func fetchOrderFromDB(ctx context.Context, orderID string) (*Order, error) {
 ctx, span := tracer.Start(ctx, "fetchOrderFromDB",
  trace.WithSpanKind(trace.SpanKindClient),
  trace.WithAttributes(
   attribute.String("db.system", "postgresql"),
   attribute.String("db.operation", "SELECT"),
  ),
 )
 defer span.End()

 // æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢å»¶è¿Ÿ
 time.Sleep(50 * time.Millisecond)

 return &Order{
  ID:     orderID,
  Amount: 199.99,
  Status: "paid",
 }, nil
}

type Order struct {
 ID     string  `json:"id"`
 Amount float64 `json:"amount"`
 Status string  `json:"status"`
}

func createOrder(c *gin.Context) {
 // å®ç°ç•¥
 c.JSON(http.StatusCreated, gin.H{"status": "created"})
}
```

---

### 4.2 OTLP Collectoréƒ¨ç½²

#### Collector ConfigMap

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: observability
data:
  collector.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
    
    processors:
      batch:
        timeout: 10s
        send_batch_size: 1024
      
      memory_limiter:
        check_interval: 1s
        limit_mib: 2000
      
      # æ·»åŠ K8så…ƒæ•°æ®
      k8sattributes:
        auth_type: "serviceAccount"
        passthrough: false
        extract:
          metadata:
            - k8s.namespace.name
            - k8s.deployment.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.node.name
    
    exporters:
      # Jaegerè¿½è¸ª
      jaeger:
        endpoint: jaeger-collector:14250
        tls:
          insecure: true
      
      # PrometheusæŒ‡æ ‡
      prometheusremotewrite:
        endpoint: http://prometheus:9090/api/v1/write
        tls:
          insecure: true
      
      # Lokiæ—¥å¿—
      loki:
        endpoint: http://loki:3100/loki/api/v1/push
      
      # è°ƒè¯•è¾“å‡º
      logging:
        loglevel: info
    
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, k8sattributes, batch]
          exporters: [jaeger, logging]
        
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, k8sattributes, batch]
          exporters: [prometheusremotewrite]
```

#### Collector Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: observability
spec:
  replicas: 2
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      serviceAccountName: otel-collector
      containers:
        - name: otel-collector
          image: otel/opentelemetry-collector-k8s:0.112.0
          args:
            - --config=/etc/otel/collector.yaml
          ports:
            - name: otlp-grpc
              containerPort: 4317
            - name: otlp-http
              containerPort: 4318
            - name: metrics
              containerPort: 8888
          resources:
            requests:
              cpu: 200m
              memory: 400Mi
            limits:
              cpu: 1000m
              memory: 2Gi
          volumeMounts:
            - name: config
              mountPath: /etc/otel
      volumes:
        - name: config
          configMap:
            name: otel-collector-config
---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: observability
spec:
  selector:
    app: otel-collector
  ports:
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
    - name: otlp-http
      port: 4318
      targetPort: 4318
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: observability
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-collector
rules:
  - apiGroups: [""]
    resources: ["pods", "nodes", "namespaces"]
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-collector
subjects:
  - kind: ServiceAccount
    name: otel-collector
    namespace: observability
roleRef:
  kind: ClusterRole
  name: otel-collector
  apiGroup: rbac.authorization.k8s.io
```

---

### 4.3 Prometheusç›‘æ§é›†æˆ

#### ServiceMonitorï¼ˆPrometheus Operatorï¼‰

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: myapp
  namespace: production
spec:
  selector:
    matchLabels:
      app: myapp
  endpoints:
    - port: metrics
      interval: 15s
      path: /metrics
      scrapeTimeout: 10s
```

#### è‡ªå®šä¹‰ä¸šåŠ¡æŒ‡æ ‡

```go
// pkg/metrics/metrics.go
package metrics

import (
 "time"

 "go.opentelemetry.io/otel"
 "go.opentelemetry.io/otel/attribute"
 "go.opentelemetry.io/otel/metric"
)

type Metrics struct {
 // è®¡æ•°å™¨
 HTTPRequests   metric.Int64Counter
 OrdersCreated  metric.Int64Counter
 PaymentsFailed metric.Int64Counter

 // ç›´æ–¹å›¾
 HTTPDuration    metric.Float64Histogram
 OrderAmount     metric.Float64Histogram
 DBQueryDuration metric.Float64Histogram

 // å¼‚æ­¥Gauge
 ActiveConnections metric.Int64ObservableGauge
 QueueDepth        metric.Int64ObservableGauge
}

func New() (*Metrics, error) {
 meter := otel.Meter("myapp")

 httpRequests, err := meter.Int64Counter(
  "http_requests_total",
  metric.WithDescription("Total HTTP requests"),
  metric.WithUnit("{request}"),
 )
 if err != nil {
  return nil, err
 }

 httpDuration, err := meter.Float64Histogram(
  "http_request_duration_seconds",
  metric.WithDescription("HTTP request latency"),
  metric.WithUnit("s"),
  metric.WithExplicitBucketBoundaries(0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5),
 )
 if err != nil {
  return nil, err
 }

 ordersCreated, err := meter.Int64Counter(
  "orders_created_total",
  metric.WithDescription("Total orders created"),
  metric.WithUnit("{order}"),
 )
 if err != nil {
  return nil, err
 }

 orderAmount, err := meter.Float64Histogram(
  "order_amount_dollars",
  metric.WithDescription("Order amount distribution"),
  metric.WithUnit("$"),
  metric.WithExplicitBucketBoundaries(10, 50, 100, 200, 500, 1000, 5000),
 )
 if err != nil {
  return nil, err
 }

 // å¼‚æ­¥Gaugeï¼ˆå®šæœŸå›è°ƒï¼‰
 activeConns, err := meter.Int64ObservableGauge(
  "active_connections",
  metric.WithDescription("Current active connections"),
  metric.WithUnit("{connection}"),
 )
 if err != nil {
  return nil, err
 }

 // æ³¨å†Œå›è°ƒ
 _, err = meter.RegisterCallback(func(ctx context.Context, o metric.Observer) error {
  // ä»è¿æ¥æ± è·å–å½“å‰è¿æ¥æ•°
  conns := getActiveConnectionsFromPool()
  o.ObserveInt64(activeConns, conns)
  return nil
 }, activeConns)
 if err != nil {
  return nil, err
 }

 return &Metrics{
  HTTPRequests:    httpRequests,
  OrdersCreated:   ordersCreated,
  HTTPDuration:    httpDuration,
  OrderAmount:     orderAmount,
  ActiveConnections: activeConns,
 }, nil
}

func (m *Metrics) RecordHTTPRequest(ctx context.Context, method, endpoint string, status int, duration time.Duration) {
 attrs := []attribute.KeyValue{
  attribute.String("method", method),
  attribute.String("endpoint", endpoint),
  attribute.Int("status", status),
 }

 m.HTTPRequests.Add(ctx, 1, metric.WithAttributes(attrs...))
 m.HTTPDuration.Record(ctx, duration.Seconds(), metric.WithAttributes(attrs...))
}

func getActiveConnectionsFromPool() int64 {
 // å®é™…å®ç°è·å–è¿æ¥æ± çŠ¶æ€
 return 42
}
```

#### Prometheuså‘Šè­¦è§„åˆ™

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: myapp-alerts
  namespace: production
spec:
  groups:
    - name: myapp
      interval: 30s
      rules:
        # é«˜é”™è¯¯ç‡å‘Šè­¦
        - alert: HighErrorRate
          expr: |
            sum(rate(http_requests_total{status=~"5.."}[5m])) /
            sum(rate(http_requests_total[5m])) > 0.05
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "High HTTP error rate ({{ $value | humanizePercentage }})"
            description: "Service has {{ $value | humanizePercentage }} error rate for 5 minutes"

        # é«˜å»¶è¿Ÿå‘Šè­¦
        - alert: HighLatency
          expr: |
            histogram_quantile(0.95,
              sum(rate(http_request_duration_seconds_bucket[5m])) by (le, endpoint)
            ) > 1
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "High P95 latency ({{ $value }}s)"
            description: "Endpoint {{ $labels.endpoint }} has P95 latency > 1s"

        # Podé‡å¯å‘Šè­¦
        - alert: PodRestarting
          expr: |
            rate(kube_pod_container_status_restarts_total{namespace="production",pod=~"myapp-.*"}[15m]) > 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.pod }} is restarting"
            description: "Pod has restarted {{ $value }} times in the last 15 minutes"

        # å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦
        - alert: HighMemoryUsage
          expr: |
            container_memory_working_set_bytes{namespace="production",pod=~"myapp-.*"} /
            container_spec_memory_limit_bytes{namespace="production",pod=~"myapp-.*"} > 0.9
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "High memory usage ({{ $value | humanizePercentage }})"
            description: "Pod {{ $labels.pod }} is using > 90% memory"
```

---

### 4.4 Grafanaå¯è§†åŒ–

#### Grafana Dashboard JSONï¼ˆç®€åŒ–ï¼‰

```json
{
  "dashboard": {
    "title": "Go Application Metrics",
    "panels": [
      {
        "title": "HTTP Request Rate",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total[5m])) by (endpoint, status)"
          }
        ],
        "type": "graph"
      },
      {
        "title": "P95 Latency",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, endpoint))"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Error Rate",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m]))"
          }
        ],
        "type": "stat",
        "fieldConfig": {
          "thresholds": [
            {"value": 0, "color": "green"},
            {"value": 0.01, "color": "yellow"},
            {"value": 0.05, "color": "red"}
          ]
        }
      }
    ]
  }
}
```

---

## ç¬¬5ç« : æ—¥å¿—èšåˆä¸åˆ†æ

### 5.1 ç»“æ„åŒ–æ—¥å¿—æœ€ä½³å®è·µ

#### Zap Loggeråˆå§‹åŒ–

```go
// pkg/logger/logger.go
package logger

import (
 "context"
 "os"

 "go.opentelemetry.io/otel/trace"
 "go.uber.org/zap"
 "go.uber.org/zap/zapcore"
)

var globalLogger *zap.Logger

func Init(env string) error {
 var config zap.Config

 if env == "production" {
  config = zap.NewProductionConfig()
  config.Encoding = "json"
 } else {
  config = zap.NewDevelopmentConfig()
  config.Encoding = "console"
 }

 // è‡ªå®šä¹‰æ—¶é—´æ ¼å¼
 config.EncoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder
 config.EncoderConfig.EncodeLevel = zapcore.CapitalLevelEncoder

 // è¾“å‡ºåˆ°stdoutï¼ˆå®¹å™¨ç¯å¢ƒï¼‰
 config.OutputPaths = []string{"stdout"}
 config.ErrorOutputPaths = []string{"stderr"}

 // åŠ¨æ€æ—¥å¿—ç­‰çº§
 config.Level = zap.NewAtomicLevelAt(getLogLevel(os.Getenv("LOG_LEVEL")))

 logger, err := config.Build(
  zap.AddCaller(),
  zap.AddStacktrace(zapcore.ErrorLevel),
 )
 if err != nil {
  return err
 }

 globalLogger = logger
 return nil
}

func getLogLevel(level string) zapcore.Level {
 switch level {
 case "debug":
  return zapcore.DebugLevel
 case "info":
  return zapcore.InfoLevel
 case "warn":
  return zapcore.WarnLevel
 case "error":
  return zapcore.ErrorLevel
 default:
  return zapcore.InfoLevel
 }
}

// ä»Contextæå–TraceID
func L(ctx context.Context) *zap.Logger {
 spanCtx := trace.SpanContextFromContext(ctx)
 if spanCtx.IsValid() {
  return globalLogger.With(
   zap.String("trace_id", spanCtx.TraceID().String()),
   zap.String("span_id", spanCtx.SpanID().String()),
  )
 }
 return globalLogger
}

func Sync() {
 _ = globalLogger.Sync()
}
```

**ä½¿ç”¨ç¤ºä¾‹**:

```go
func handleOrder(ctx context.Context, orderID string) {
 logger.L(ctx).Info("Processing order",
  zap.String("order_id", orderID),
  zap.String("user_id", getUserID(ctx)),
  zap.Duration("timeout", 30*time.Second),
 )

 // ä¸šåŠ¡é€»è¾‘...

 logger.L(ctx).Info("Order processed successfully",
  zap.String("order_id", orderID),
  zap.Float64("amount", 199.99),
 )
}
```

**æ—¥å¿—è¾“å‡º**:

```json
{
  "level": "INFO",
  "ts": "2025-10-17T10:30:45.123Z",
  "caller": "handlers/order.go:42",
  "msg": "Processing order",
  "trace_id": "a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6",
  "span_id": "a1b2c3d4e5f6g7h8",
  "order_id": "ORD-12345",
  "user_id": "USR-67890",
  "timeout": "30s"
}
```

---

### 5.2 Lokiæ—¥å¿—èšåˆ

#### Fluent Bit DaemonSetï¼ˆé‡‡é›†æ—¥å¿—ï¼‰

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit
  namespace: observability
spec:
  selector:
    matchLabels:
      app: fluent-bit
  template:
    metadata:
      labels:
        app: fluent-bit
    spec:
      serviceAccountName: fluent-bit
      containers:
        - name: fluent-bit
          image: fluent/fluent-bit:3.2.2
          volumeMounts:
            - name: varlog
              mountPath: /var/log
            - name: varlibdockercontainers
              mountPath: /var/lib/docker/containers
              readOnly: true
            - name: fluent-bit-config
              mountPath: /fluent-bit/etc/
      volumes:
        - name: varlog
          hostPath:
            path: /var/log
        - name: varlibdockercontainers
          hostPath:
            path: /var/lib/docker/containers
        - name: fluent-bit-config
          configMap:
            name: fluent-bit-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: observability
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         5
        Log_Level     info
        Daemon        off

    [INPUT]
        Name              tail
        Path              /var/log/containers/*_production_*.log
        Parser            docker
        Tag               kube.*
        Refresh_Interval  5
        Mem_Buf_Limit     50MB
        Skip_Long_Lines   On

    [FILTER]
        Name                kubernetes
        Match               kube.*
        Kube_URL            https://kubernetes.default.svc:443
        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
        Merge_Log           On
        Keep_Log            Off
        K8S-Logging.Parser  On
        K8S-Logging.Exclude On

    [OUTPUT]
        Name              loki
        Match             kube.*
        Host              loki.observability.svc
        Port              3100
        Labels            job=fluentbit
        Auto_Kubernetes_Labels on
```

#### Loki Deployment

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: loki
  namespace: observability
spec:
  serviceName: loki
  replicas: 1
  selector:
    matchLabels:
      app: loki
  template:
    metadata:
      labels:
        app: loki
    spec:
      containers:
        - name: loki
          image: grafana/loki:3.2.1
          ports:
            - name: http
              containerPort: 3100
          volumeMounts:
            - name: loki-data
              mountPath: /loki
            - name: loki-config
              mountPath: /etc/loki
          resources:
            requests:
              cpu: 200m
              memory: 512Mi
            limits:
              cpu: 1000m
              memory: 2Gi
  volumeClaimTemplates:
    - metadata:
        name: loki-data
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 50Gi
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-config
  namespace: observability
data:
  loki.yaml: |
    auth_enabled: false

    server:
      http_listen_port: 3100

    ingester:
      lifecycler:
        ring:
          kvstore:
            store: inmemory
          replication_factor: 1
      chunk_idle_period: 5m
      chunk_retain_period: 30s

    schema_config:
      configs:
        - from: 2024-01-01
          store: tsdb
          object_store: filesystem
          schema: v13
          index:
            prefix: index_
            period: 24h

    storage_config:
      tsdb_shipper:
        active_index_directory: /loki/index
        cache_location: /loki/cache
      filesystem:
        directory: /loki/chunks

    limits_config:
      retention_period: 744h  # 31å¤©
      max_query_length: 721h
```

---

## ç¬¬6ç« : å®‰å…¨åŠ å›º

### 6.1 RBACæƒé™æ§åˆ¶

#### ServiceAccountä¸Role

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: myapp
  namespace: production
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: myapp-role
  namespace: production
rules:
  # åªè¯»ConfigMap
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "list", "watch"]
  
  # åªè¯»Secret
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get"]
  
  # åˆ›å»ºEventï¼ˆç”¨äºæ—¥å¿—ï¼‰
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "patch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: myapp-rolebinding
  namespace: production
subjects:
  - kind: ServiceAccount
    name: myapp
roleRef:
  kind: Role
  name: myapp-role
  apiGroup: rbac.authorization.k8s.io
```

---

### 6.2 Pod Security Standards

#### Restricted PodSecurityPolicy

```yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
  readOnlyRootFilesystem: true
```

---

### 6.3 ç½‘ç»œç­–ç•¥

#### NetworkPolicy

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: myapp-netpol
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: myapp
  policyTypes:
    - Ingress
    - Egress
  ingress:
    # åªå…è®¸æ¥è‡ªIngress Controllerçš„æµé‡
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx
      ports:
        - protocol: TCP
          port: 8080
  egress:
    # å…è®¸è®¿é—®DNS
    - to:
        - namespaceSelector:
            matchLabels:
              name: kube-system
      ports:
        - protocol: UDP
          port: 53
    
    # å…è®¸è®¿é—®æ•°æ®åº“
    - to:
        - podSelector:
            matchLabels:
              app: postgres
      ports:
        - protocol: TCP
          port: 5432
    
    # å…è®¸è®¿é—®OTLP Collector
    - to:
        - namespaceSelector:
            matchLabels:
              name: observability
        - podSelector:
            matchLabels:
              app: otel-collector
      ports:
        - protocol: TCP
          port: 4317
```

---

## ç¬¬7ç« : ç”Ÿäº§ç¯å¢ƒæ¡ˆä¾‹

### 7.1 ç”µå•†è®¢å•æœåŠ¡å®Œæ•´éƒ¨ç½²

#### æ¶æ„å›¾

```text
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚  Ingress NGINX  â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚               â”‚               â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
              â”‚  Order    â”‚   â”‚ Payment â”‚    â”‚ Inventory â”‚
              â”‚  Service  â”‚   â”‚ Service â”‚    â”‚  Service  â”‚
              â”‚  (Go)     â”‚   â”‚  (Go)   â”‚    â”‚   (Go)    â”‚
              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                    â”‚              â”‚               â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚          â”‚          â”‚
                  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ PostgreSQLâ”‚ â”‚ Redis  â”‚ â”‚OTLP       â”‚
                  â”‚           â”‚ â”‚        â”‚ â”‚Collector  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                                         â”‚        â”‚        â”‚
                                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”€â”
                                    â”‚Jaeger  â”‚ â”‚Prometh-â”‚ â”‚Loki   â”‚
                                    â”‚        â”‚ â”‚eus     â”‚ â”‚       â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Order Serviceéƒ¨ç½²æ¸…å•

```yaml
# order-service-deployment.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ecommerce
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: order-service-config
  namespace: ecommerce
data:
  config.yaml: |
    server:
      port: 8080
      timeout: 30s
    database:
      host: postgres.ecommerce.svc
      port: 5432
      database: orders
      max_connections: 50
    redis:
      host: redis.ecommerce.svc
      port: 6379
      pool_size: 20
    otlp:
      endpoint: otel-collector.observability.svc:4317
    payment_service:
      url: http://payment-service.ecommerce.svc/api
    inventory_service:
      url: http://inventory-service.ecommerce.svc/api
---
apiVersion: v1
kind: Secret
metadata:
  name: order-service-secrets
  namespace: ecommerce
type: Opaque
stringData:
  db-password: "ProductionPassword123!"
  redis-password: "RedisPassword456!"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
  namespace: ecommerce
spec:
  replicas: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2
      maxUnavailable: 0
  selector:
    matchLabels:
      app: order-service
  template:
    metadata:
      labels:
        app: order-service
        version: v1.2.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
    spec:
      serviceAccountName: order-service
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - order-service
              topologyKey: kubernetes.io/hostname
      containers:
        - name: order-service
          image: harbor.example.com/ecommerce/order-service:1.2.0
          ports:
            - name: http
              containerPort: 8080
            - name: metrics
              containerPort: 9090
          env:
            - name: GO_ENV
              value: "production"
            - name: LOG_LEVEL
              value: "info"
            - name: GOMAXPROCS
              valueFrom:
                resourceFieldRef:
                  resource: limits.cpu
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: order-service-secrets
                  key: db-password
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: "otel-collector.observability.svc:4317"
            - name: OTEL_SERVICE_NAME
              value: "order-service"
          resources:
            requests:
              cpu: 1000m
              memory: 1Gi
            limits:
              cpu: 2000m
              memory: 2Gi
          livenessProbe:
            httpGet:
              path: /health/live
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /health/ready
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
          volumeMounts:
            - name: config
              mountPath: /etc/app
      volumes:
        - name: config
          configMap:
            name: order-service-config
---
apiVersion: v1
kind: Service
metadata:
  name: order-service
  namespace: ecommerce
spec:
  selector:
    app: order-service
  ports:
    - name: http
      port: 80
      targetPort: 8080
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: order-service-hpa
  namespace: ecommerce
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: order-service
  minReplicas: 5
  maxReplicas: 50
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Pods
      pods:
        metric:
          name: http_requests_per_second
        target:
          type: AverageValue
          averageValue: "2000"
```

---

### 7.2 Helm Chartæ‰“åŒ…

#### Chart.yaml

```yaml
apiVersion: v2
name: order-service
description: E-commerce Order Service Helm Chart
version: 1.2.0
appVersion: "1.2.0"
keywords:
  - ecommerce
  - order
  - go
  - otlp
maintainers:
  - name: Platform Team
    email: platform@example.com
```

#### values.yaml

```yaml
replicaCount: 5

image:
  repository: harbor.example.com/ecommerce/order-service
  tag: "1.2.0"
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 80
  targetPort: 8080

resources:
  requests:
    cpu: 1000m
    memory: 1Gi
  limits:
    cpu: 2000m
    memory: 2Gi

autoscaling:
  enabled: true
  minReplicas: 5
  maxReplicas: 50
  targetCPUUtilizationPercentage: 70

config:
  logLevel: info
  database:
    host: postgres.ecommerce.svc
    port: 5432
    maxConnections: 50
  otlp:
    endpoint: otel-collector.observability.svc:4317

secrets:
  dbPassword: ""  # é€šè¿‡CI/CDæ³¨å…¥

ingress:
  enabled: true
  className: nginx
  hosts:
    - host: api.example.com
      paths:
        - path: /api/orders
          pathType: Prefix
  tls:
    - secretName: api-tls
      hosts:
        - api.example.com
```

**éƒ¨ç½²å‘½ä»¤**:

```bash
# å®‰è£…
helm install order-service ./charts/order-service \
    --namespace ecommerce \
    --create-namespace \
    --set secrets.dbPassword="${DB_PASSWORD}"

# å‡çº§
helm upgrade order-service ./charts/order-service \
    --namespace ecommerce \
    --set image.tag=1.3.0

# å›æ»š
helm rollback order-service 1 --namespace ecommerce
```

---

## ç¬¬8ç« : CI/CDæµæ°´çº¿

### 8.1 GitLab CIé…ç½®

```yaml
# .gitlab-ci.yml
stages:
  - lint
  - test
  - build
  - deploy

variables:
  GO_VERSION: "1.25.1"
  DOCKER_DRIVER: overlay2

# ä»£ç è´¨é‡æ£€æŸ¥
lint:
  stage: lint
  image: golangci/golangci-lint:v1.62.2
  script:
    - golangci-lint run --timeout 5m
  only:
    - merge_requests
    - main

# å•å…ƒæµ‹è¯•
test:unit:
  stage: test
  image: golang:1.25.1
  services:
    - postgres:16-alpine
    - redis:7-alpine
  variables:
    POSTGRES_DB: testdb
    POSTGRES_USER: test
    POSTGRES_PASSWORD: test
  script:
    - go mod download
    - go test -v -race -coverprofile=coverage.out ./...
    - go tool cover -func=coverage.out
  coverage: '/total:\s+\(statements\)\s+(\d+\.\d+)%/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml

# æ„å»ºDockeré•œåƒ
build:
  stage: build
  image: docker:27-dind
  services:
    - docker:27-dind
  before_script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  script:
    - docker build --build-arg VERSION=$CI_COMMIT_TAG -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA .
    - docker tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA $CI_REGISTRY_IMAGE:latest
    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
    - docker push $CI_REGISTRY_IMAGE:latest
  only:
    - main
    - tags

# éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
deploy:production:
  stage: deploy
  image: alpine/k8s:1.31.3
  script:
    - kubectl config use-context production
    - kubectl set image deployment/order-service order-service=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHA -n ecommerce
    - kubectl rollout status deployment/order-service -n ecommerce --timeout=5m
  environment:
    name: production
    url: https://api.example.com
  when: manual
  only:
    - tags
```

---

## æ€»ç»“

æœ¬æŒ‡å—æ¶µç›–äº†Goåº”ç”¨åœ¨ç”Ÿäº§ç¯å¢ƒçš„**å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼š

âœ… **å®¹å™¨åŒ–**: Dockerå¤šé˜¶æ®µæ„å»ºã€é•œåƒä¼˜åŒ–ï¼ˆ10-20MBï¼‰ã€å®‰å…¨æ‰«æ  
âœ… **Kubernetesç¼–æ’**: Deploymentã€HPA/VPAã€å¥åº·æ£€æŸ¥ä¸‰é‡å¥  
âœ… **é…ç½®ç®¡ç†**: ConfigMapçƒ­åŠ è½½ã€Sealed Secretsã€Vaulté›†æˆ  
âœ… **å¯è§‚æµ‹æ€§**: OTLPå…¨é“¾è·¯ã€PrometheusæŒ‡æ ‡ã€Grafanaå¯è§†åŒ–  
âœ… **æ—¥å¿—èšåˆ**: ç»“æ„åŒ–æ—¥å¿—ã€Fluent Bitã€Loki  
âœ… **å®‰å…¨åŠ å›º**: RBACã€Pod Security Standardsã€NetworkPolicy  
âœ… **è‡ªåŠ¨æ‰©ç¼©å®¹**: HPAæŒ‡æ ‡é©±åŠ¨ã€VPAèµ„æºä¼˜åŒ–  
âœ… **CI/CD**: GitLabè‡ªåŠ¨åŒ–æµæ°´çº¿ã€Helmæ‰“åŒ…éƒ¨ç½²

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**:

1. æ ¹æ®æœ¬æŒ‡å—æ­å»ºKubernetesé›†ç¾¤
2. éƒ¨ç½²OTLP Collector + Jaeger + Prometheus + Grafanaå¯è§‚æµ‹æ€§æ ˆ
3. ä½¿ç”¨Helmæ‰“åŒ…ç°æœ‰Goåº”ç”¨
4. é…ç½®CI/CDæµæ°´çº¿å®ç°è‡ªåŠ¨åŒ–éƒ¨ç½²
5. ç”Ÿäº§ç¯å¢ƒç›‘æ§å‘Šè­¦è§„åˆ™è°ƒä¼˜

**å…³é”®æˆåŠŸå› ç´ **:

- âš¡ é›¶åœæœºæ»šåŠ¨æ›´æ–°ï¼ˆ`maxUnavailable: 0`ï¼‰
- ğŸ”’ é»˜è®¤å®‰å…¨åŠ å›ºï¼ˆérootç”¨æˆ·ã€åªè¯»æ–‡ä»¶ç³»ç»Ÿï¼‰
- ğŸ“Š å…¨é“¾è·¯å¯è§‚æµ‹æ€§ï¼ˆTraces + Metrics + Logsï¼‰
- ğŸš€ è‡ªåŠ¨æ‰©ç¼©å®¹åº”å¯¹æµé‡é«˜å³°
- ğŸ›¡ï¸ å¤šå±‚æ¬¡å®‰å…¨é˜²æŠ¤ï¼ˆNetworkPolicy + RBAC + PSPï¼‰

---

**æ–‡æ¡£ç»Ÿè®¡**:

- æ€»è¡Œæ•°: **2,145è¡Œ** âœ…
- ä»£ç ç¤ºä¾‹: **23ä¸ª**
- YAMLé…ç½®: **18ä¸ª**
- æ¶æ„å›¾: **2ä¸ª**
- æœ€ä½³å®è·µ: **50+æ¡**

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-10-17  
**ç»´æŠ¤è€…**: å¹³å°å·¥ç¨‹å›¢é˜Ÿ
