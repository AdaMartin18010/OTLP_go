# èšåˆå®Œæ•´æŒ‡å—

## ğŸ“‹ ç›®å½•

- [èšåˆå®Œæ•´æŒ‡å—](#èšåˆå®Œæ•´æŒ‡å—)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [æ¦‚è¿°](#æ¦‚è¿°)
    - [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
    - [èšåˆæµç¨‹](#èšåˆæµç¨‹)
  - [Sum (æ±‚å’Œèšåˆ)](#sum-æ±‚å’Œèšåˆ)
    - [å®šä¹‰](#å®šä¹‰)
    - [é€‚ç”¨åœºæ™¯](#é€‚ç”¨åœºæ™¯)
    - [Temporality æ”¯æŒ](#temporality-æ”¯æŒ)
    - [Go å®ç°](#go-å®ç°)
  - [LastValue (æœ€æ–°å€¼èšåˆ)](#lastvalue-æœ€æ–°å€¼èšåˆ)
    - [å®šä¹‰2](#å®šä¹‰2)
    - [é€‚ç”¨åœºæ™¯2](#é€‚ç”¨åœºæ™¯2)
    - [æ—¶é—´æˆ³å¤„ç†](#æ—¶é—´æˆ³å¤„ç†)
    - [Go å®ç°2](#go-å®ç°2)
  - [Histogram (ç›´æ–¹å›¾èšåˆ)](#histogram-ç›´æ–¹å›¾èšåˆ)
    - [å®šä¹‰3](#å®šä¹‰3)
    - [æ¡¶è¾¹ç•Œé…ç½®](#æ¡¶è¾¹ç•Œé…ç½®)
    - [ç»Ÿè®¡è®¡ç®—](#ç»Ÿè®¡è®¡ç®—)
    - [Go å®ç°3](#go-å®ç°3)
  - [ExponentialHistogram (æŒ‡æ•°ç›´æ–¹å›¾èšåˆ)](#exponentialhistogram-æŒ‡æ•°ç›´æ–¹å›¾èšåˆ)
    - [å®šä¹‰4](#å®šä¹‰4)
    - [Scale å‚æ•°](#scale-å‚æ•°)
    - [æ¡¶ç´¢å¼•è®¡ç®—](#æ¡¶ç´¢å¼•è®¡ç®—)
    - [Go å®ç°4](#go-å®ç°4)
  - [èšåˆé…ç½®](#èšåˆé…ç½®)
    - [View æœºåˆ¶](#view-æœºåˆ¶)
    - [èšåˆé€‰æ‹©å™¨](#èšåˆé€‰æ‹©å™¨)
    - [è‡ªå®šä¹‰èšåˆ](#è‡ªå®šä¹‰èšåˆ)
    - [Go å®ç°5](#go-å®ç°5)
  - [å¤šç»´èšåˆ](#å¤šç»´èšåˆ)
    - [æŒ‰å±æ€§èšåˆ](#æŒ‰å±æ€§èšåˆ)
    - [èšåˆæ ‘](#èšåˆæ ‘)
    - [åŠ¨æ€èšåˆ](#åŠ¨æ€èšåˆ)
    - [Go å®ç°6](#go-å®ç°6)
  - [é¢„èšåˆ](#é¢„èšåˆ)
    - [ä»€ä¹ˆæ˜¯é¢„èšåˆ](#ä»€ä¹ˆæ˜¯é¢„èšåˆ)
    - [é¢„èšåˆç­–ç•¥](#é¢„èšåˆç­–ç•¥)
    - [Go å®ç°7](#go-å®ç°7)
  - [èšåˆåˆå¹¶](#èšåˆåˆå¹¶)
    - [Sum åˆå¹¶](#sum-åˆå¹¶)
    - [Histogram åˆå¹¶](#histogram-åˆå¹¶)
    - [ExponentialHistogram åˆå¹¶](#exponentialhistogram-åˆå¹¶)
    - [Go å®ç°8](#go-å®ç°8)
  - [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
    - [1. å¹¶å‘èšåˆ](#1-å¹¶å‘èšåˆ)
    - [2. å¢é‡èšåˆ](#2-å¢é‡èšåˆ)
    - [3. å†…å­˜ä¼˜åŒ–](#3-å†…å­˜ä¼˜åŒ–)
  - [å®Œæ•´å®ç°](#å®Œæ•´å®ç°)
    - [èšåˆç®¡ç†ç³»ç»Ÿ](#èšåˆç®¡ç†ç³»ç»Ÿ)
  - [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
    - [1. é€‰æ‹©åˆé€‚çš„èšåˆç±»å‹](#1-é€‰æ‹©åˆé€‚çš„èšåˆç±»å‹)
    - [2. åˆç†é…ç½® Histogram æ¡¶](#2-åˆç†é…ç½®-histogram-æ¡¶)
    - [3. æ§åˆ¶èšåˆç²’åº¦](#3-æ§åˆ¶èšåˆç²’åº¦)
    - [4. é¢„èšåˆçƒ­æ•°æ®](#4-é¢„èšåˆçƒ­æ•°æ®)
  - [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
    - [Q1: Sum å’Œ LastValue å¦‚ä½•é€‰æ‹©ï¼Ÿ](#q1-sum-å’Œ-lastvalue-å¦‚ä½•é€‰æ‹©)
    - [Q2: Histogram å’Œ ExponentialHistogram å¦‚ä½•é€‰æ‹©ï¼Ÿ](#q2-histogram-å’Œ-exponentialhistogram-å¦‚ä½•é€‰æ‹©)
    - [Q3: å¦‚ä½•ä¼˜åŒ– Histogram æ€§èƒ½ï¼Ÿ](#q3-å¦‚ä½•ä¼˜åŒ–-histogram-æ€§èƒ½)
    - [Q4: èšåˆæ•°æ®å¦‚ä½•æŒä¹…åŒ–ï¼Ÿ](#q4-èšåˆæ•°æ®å¦‚ä½•æŒä¹…åŒ–)
    - [Q5: å¦‚ä½•å¤„ç†èšåˆæ•°æ®çš„è¿‡æœŸï¼Ÿ](#q5-å¦‚ä½•å¤„ç†èšåˆæ•°æ®çš„è¿‡æœŸ)
  - [å‚è€ƒèµ„æº](#å‚è€ƒèµ„æº)
    - [å®˜æ–¹æ–‡æ¡£](#å®˜æ–¹æ–‡æ¡£)
    - [Go å®ç°9](#go-å®ç°9)
    - [ç›¸å…³æ–‡æ¡£](#ç›¸å…³æ–‡æ¡£)

---

## æ¦‚è¿°

**èšåˆ (Aggregation)** æ˜¯å°†å¤šä¸ªè§‚æµ‹å€¼ç»„åˆæˆå•ä¸ªæ•°æ®ç‚¹çš„è¿‡ç¨‹ï¼Œæ˜¯ Metrics ç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½ã€‚

### æ ¸å¿ƒæ¦‚å¿µ

- âœ… **Sum**: ç´¯åŠ æ‰€æœ‰è§‚æµ‹å€¼
- âœ… **LastValue**: ä¿ç•™æœ€æ–°çš„è§‚æµ‹å€¼
- âœ… **Histogram**: ç»Ÿè®¡å€¼çš„åˆ†å¸ƒ
- âœ… **ExponentialHistogram**: è‡ªé€‚åº”çš„åˆ†å¸ƒç»Ÿè®¡

### èšåˆæµç¨‹

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è§‚æµ‹å€¼è®°å½•    â”‚
â”‚ counter.Add() â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ èšåˆå¤„ç†     â”‚
â”‚ Sum/Histogram â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç”Ÿæˆæ•°æ®ç‚¹   â”‚
â”‚ DataPoint     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å¯¼å‡º         â”‚
â”‚ Exporter     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Sum (æ±‚å’Œèšåˆ)

### å®šä¹‰

**Sum** èšåˆå°†æ‰€æœ‰è§‚æµ‹å€¼ç´¯åŠ ï¼Œç”¨äº Counter å’Œ UpDownCounterã€‚

### é€‚ç”¨åœºæ™¯

```text
âœ… Counter:
- HTTP è¯·æ±‚æ€»æ•°
- é”™è¯¯æ€»æ•°
- å¤„ç†çš„å­—èŠ‚æ•°

âœ… UpDownCounter:
- æ´»åŠ¨è¿æ¥æ•°
- é˜Ÿåˆ—é•¿åº¦
- ç¼“å­˜æ¡ç›®æ•°
```

### Temporality æ”¯æŒ

```text
Cumulative (ç´¯ç§¯):
T0: sum = 10  (ä»å¯åŠ¨ç´¯ç§¯)
T1: sum = 25  (ä»å¯åŠ¨ç´¯ç§¯)
T2: sum = 42  (ä»å¯åŠ¨ç´¯ç§¯)

Delta (å¢é‡):
T0â†’T1: sum = 10  (æ—¶é—´æ®µå¢é‡)
T1â†’T2: sum = 15  (æ—¶é—´æ®µå¢é‡)
T2â†’T3: sum = 17  (æ—¶é—´æ®µå¢é‡)
```

### Go å®ç°

```go
package aggregation

import (
    "sync/atomic"
    "time"
    
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/sdk/metric/metricdata"
)

// Sum èšåˆå™¨
type SumAggregation struct {
    value       int64
    startTime   time.Time
    endTime     time.Time
    temporality metricdata.Temporality
    mu          sync.RWMutex
}

func NewSumAggregation(temporality metricdata.Temporality) *SumAggregation {
    return &SumAggregation{
        value:       0,
        startTime:   time.Now(),
        temporality: temporality,
    }
}

// è®°å½•è§‚æµ‹å€¼
func (sa *SumAggregation) Record(value int64) {
    atomic.AddInt64(&sa.value, value)
    sa.mu.Lock()
    sa.endTime = time.Now()
    sa.mu.Unlock()
}

// å¯¼å‡ºæ•°æ®ç‚¹
func (sa *SumAggregation) Export() metricdata.DataPoint[int64] {
    sa.mu.RLock()
    value := atomic.LoadInt64(&sa.value)
    startTime := sa.startTime
    endTime := sa.endTime
    sa.mu.RUnlock()
    
    dp := metricdata.DataPoint[int64]{
        Attributes: attribute.EmptySet(),
        StartTime:  startTime,
        Time:       endTime,
        Value:      value,
    }
    
    // Delta æ¨¡å¼ï¼šå¯¼å‡ºåé‡ç½®
    if sa.temporality == metricdata.DeltaTemporality {
        sa.Reset()
    }
    
    return dp
}

// é‡ç½® (Delta æ¨¡å¼)
func (sa *SumAggregation) Reset() {
    sa.mu.Lock()
    defer sa.mu.Unlock()
    
    atomic.StoreInt64(&sa.value, 0)
    sa.startTime = sa.endTime
}

// Float64 ç‰ˆæœ¬
type Float64SumAggregation struct {
    value       uint64  // å­˜å‚¨ float64 çš„ä½æ¨¡å¼
    startTime   time.Time
    endTime     time.Time
    temporality metricdata.Temporality
    mu          sync.RWMutex
}

func NewFloat64SumAggregation(temporality metricdata.Temporality) *Float64SumAggregation {
    return &Float64SumAggregation{
        startTime:   time.Now(),
        temporality: temporality,
    }
}

func (fsa *Float64SumAggregation) Record(value float64) {
    import "math"
    
    for {
        old := atomic.LoadUint64(&fsa.value)
        oldFloat := math.Float64frombits(old)
        newFloat := oldFloat + value
        new := math.Float64bits(newFloat)
        
        if atomic.CompareAndSwapUint64(&fsa.value, old, new) {
            break
        }
    }
    
    fsa.mu.Lock()
    fsa.endTime = time.Now()
    fsa.mu.Unlock()
}

func (fsa *Float64SumAggregation) Export() metricdata.DataPoint[float64] {
    import "math"
    
    fsa.mu.RLock()
    valueBits := atomic.LoadUint64(&fsa.value)
    value := math.Float64frombits(valueBits)
    startTime := fsa.startTime
    endTime := fsa.endTime
    fsa.mu.RUnlock()
    
    dp := metricdata.DataPoint[float64]{
        Attributes: attribute.EmptySet(),
        StartTime:  startTime,
        Time:       endTime,
        Value:      value,
    }
    
    if fsa.temporality == metricdata.DeltaTemporality {
        fsa.Reset()
    }
    
    return dp
}

func (fsa *Float64SumAggregation) Reset() {
    fsa.mu.Lock()
    defer fsa.mu.Unlock()
    
    atomic.StoreUint64(&fsa.value, 0)
    fsa.startTime = fsa.endTime
}
```

---

## LastValue (æœ€æ–°å€¼èšåˆ)

### å®šä¹‰2

**LastValue** èšåˆä¿ç•™æœ€æ–°çš„è§‚æµ‹å€¼ï¼Œç”¨äº Gaugeã€‚

### é€‚ç”¨åœºæ™¯2

```text
âœ… é€‚ç”¨åœºæ™¯:
- å†…å­˜ä½¿ç”¨é‡
- CPU ä½¿ç”¨ç‡
- æ¸©åº¦
- å½“å‰é˜Ÿåˆ—é•¿åº¦
- Goroutine æ•°é‡
```

### æ—¶é—´æˆ³å¤„ç†

```text
é‡‡ç”¨æœ€æ–°å€¼çš„æ—¶é—´æˆ³:

T0: value = 100, timestamp = T0
T1: value = 150, timestamp = T1  â† ä¿ç•™
T2: value = 120, timestamp = T2  â† ä¿ç•™ (æœ€æ–°)

å¯¼å‡º: value = 120, timestamp = T2
```

### Go å®ç°2

```go
// LastValue èšåˆå™¨
type LastValueAggregation struct {
    value     float64
    timestamp time.Time
    mu        sync.RWMutex
}

func NewLastValueAggregation() *LastValueAggregation {
    return &LastValueAggregation{
        timestamp: time.Now(),
    }
}

// è®°å½•è§‚æµ‹å€¼
func (lva *LastValueAggregation) Record(value float64) {
    lva.RecordWithTimestamp(value, time.Now())
}

// è®°å½•è§‚æµ‹å€¼ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
func (lva *LastValueAggregation) RecordWithTimestamp(
    value float64,
    timestamp time.Time,
) {
    lva.mu.Lock()
    defer lva.mu.Unlock()
    
    // ä»…ä¿ç•™æ›´æ–°çš„å€¼
    if timestamp.After(lva.timestamp) {
        lva.value = value
        lva.timestamp = timestamp
    }
}

// å¯¼å‡ºæ•°æ®ç‚¹
func (lva *LastValueAggregation) Export() metricdata.DataPoint[float64] {
    lva.mu.RLock()
    defer lva.mu.RUnlock()
    
    return metricdata.DataPoint[float64]{
        Attributes: attribute.EmptySet(),
        Time:       lva.timestamp,
        Value:      lva.value,
    }
}

// å¼‚æ­¥ Gauge å®ç°
type AsynchronousGauge struct {
    callback func() float64
    lastValue *LastValueAggregation
}

func NewAsynchronousGauge(callback func() float64) *AsynchronousGauge {
    return &AsynchronousGauge{
        callback:  callback,
        lastValue: NewLastValueAggregation(),
    }
}

// é‡‡é›†
func (ag *AsynchronousGauge) Collect() {
    value := ag.callback()
    ag.lastValue.Record(value)
}

// å¯¼å‡º
func (ag *AsynchronousGauge) Export() metricdata.DataPoint[float64] {
    return ag.lastValue.Export()
}

// ç¤ºä¾‹ï¼šç›‘æ§å†…å­˜ä½¿ç”¨
func monitorMemoryUsage() *AsynchronousGauge {
    return NewAsynchronousGauge(func() float64 {
        var m runtime.MemStats
        runtime.ReadMemStats(&m)
        return float64(m.Alloc)
    })
}
```

---

## Histogram (ç›´æ–¹å›¾èšåˆ)

### å®šä¹‰3

**Histogram** èšåˆç»Ÿè®¡å€¼çš„åˆ†å¸ƒï¼Œå°†è§‚æµ‹å€¼åˆ†é…åˆ°é¢„å®šä¹‰çš„æ¡¶ä¸­ã€‚

### æ¡¶è¾¹ç•Œé…ç½®

```go
// å¸¸ç”¨æ¡¶è¾¹ç•Œé…ç½®
var (
    // HTTP å»¶è¿Ÿ (ç§’)
    HTTPLatencyBounds = []float64{
        0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10,
    }
    
    // å“åº”å¤§å° (å­—èŠ‚)
    ResponseSizeBounds = []float64{
        100, 1000, 10000, 100000, 1000000, 10000000,
    }
    
    // æ•°æ®åº“æŸ¥è¯¢å»¶è¿Ÿ (ç§’)
    DBQueryBounds = []float64{
        0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5,
    }
    
    // é»˜è®¤æ¡¶ (æŒ‡æ•°å¢é•¿)
    DefaultBounds = []float64{
        0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75,
        1, 2.5, 5, 7.5, 10,
    }
)

// ç”ŸæˆæŒ‡æ•°æ¡¶
func generateExponentialBounds(start, factor float64, count int) []float64 {
    bounds := make([]float64, count)
    current := start
    for i := 0; i < count; i++ {
        bounds[i] = current
        current *= factor
    }
    return bounds
}

// ç¤ºä¾‹ï¼šç”Ÿæˆ 10 ä¸ªæ¡¶ï¼Œä» 0.01 å¼€å§‹ï¼Œæ¯æ¬¡ç¿»å€
// bounds := generateExponentialBounds(0.01, 2.0, 10)
// [0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12]
```

### ç»Ÿè®¡è®¡ç®—

```text
ç»Ÿè®¡æŒ‡æ ‡:
- Count: è§‚æµ‹å€¼æ€»æ•°
- Sum: è§‚æµ‹å€¼æ€»å’Œ
- Min: æœ€å°å€¼
- Max: æœ€å¤§å€¼
- Bucket Counts: å„æ¡¶è®¡æ•°
- Percentiles: ç™¾åˆ†ä½æ•° (P50, P95, P99)

è®¡ç®—ç™¾åˆ†ä½æ•°:
P50 (ä¸­ä½æ•°): 50% çš„å€¼ â‰¤ P50
P95: 95% çš„å€¼ â‰¤ P95
P99: 99% çš„å€¼ â‰¤ P99
```

### Go å®ç°3

```go
// Histogram èšåˆå™¨
type HistogramAggregation struct {
    bounds       []float64
    counts       []uint64
    sum          float64
    count        uint64
    min          float64
    max          float64
    startTime    time.Time
    endTime      time.Time
    temporality  metricdata.Temporality
    mu           sync.RWMutex
}

func NewHistogramAggregation(
    bounds []float64,
    temporality metricdata.Temporality,
) *HistogramAggregation {
    return &HistogramAggregation{
        bounds:      bounds,
        counts:      make([]uint64, len(bounds)+1),
        min:         math.Inf(1),
        max:         math.Inf(-1),
        startTime:   time.Now(),
        temporality: temporality,
    }
}

// è®°å½•è§‚æµ‹å€¼
func (ha *HistogramAggregation) Record(value float64) {
    ha.mu.Lock()
    defer ha.mu.Unlock()
    
    ha.count++
    ha.sum += value
    ha.endTime = time.Now()
    
    if value < ha.min {
        ha.min = value
    }
    if value > ha.max {
        ha.max = value
    }
    
    // æ‰¾åˆ°å¯¹åº”çš„æ¡¶
    bucketIdx := ha.findBucket(value)
    ha.counts[bucketIdx]++
}

// æŸ¥æ‰¾æ¡¶ç´¢å¼•
func (ha *HistogramAggregation) findBucket(value float64) int {
    // äºŒåˆ†æŸ¥æ‰¾
    left, right := 0, len(ha.bounds)
    for left < right {
        mid := (left + right) / 2
        if value <= ha.bounds[mid] {
            right = mid
        } else {
            left = mid + 1
        }
    }
    return left
}

// å¯¼å‡ºæ•°æ®ç‚¹
func (ha *HistogramAggregation) Export() metricdata.HistogramDataPoint[float64] {
    ha.mu.RLock()
    defer ha.mu.RUnlock()
    
    // å¤åˆ¶æ•°æ®
    counts := make([]uint64, len(ha.counts))
    copy(counts, ha.counts)
    
    dp := metricdata.HistogramDataPoint[float64]{
        Attributes:     attribute.EmptySet(),
        StartTime:      ha.startTime,
        Time:           ha.endTime,
        Count:          ha.count,
        Sum:            ha.sum,
        Min:            metricdata.NewExtrema(ha.min),
        Max:            metricdata.NewExtrema(ha.max),
        BucketCounts:   counts,
        Bounds:         ha.bounds,
    }
    
    return dp
}

// é‡ç½® (Delta æ¨¡å¼)
func (ha *HistogramAggregation) Reset() {
    ha.mu.Lock()
    defer ha.mu.Unlock()
    
    ha.count = 0
    ha.sum = 0
    ha.min = math.Inf(1)
    ha.max = math.Inf(-1)
    ha.startTime = ha.endTime
    
    for i := range ha.counts {
        ha.counts[i] = 0
    }
}

// è®¡ç®—ç™¾åˆ†ä½æ•°
func (ha *HistogramAggregation) Percentile(p float64) float64 {
    ha.mu.RLock()
    defer ha.mu.RUnlock()
    
    if ha.count == 0 {
        return 0
    }
    
    targetCount := uint64(float64(ha.count) * p)
    var cumCount uint64
    
    for i, count := range ha.counts {
        cumCount += count
        if cumCount >= targetCount {
            if i == 0 {
                return ha.min
            }
            if i >= len(ha.bounds) {
                return ha.max
            }
            return ha.bounds[i]
        }
    }
    
    return ha.max
}

// ä½¿ç”¨ç¤ºä¾‹
func exampleHistogramUsage() {
    histogram := NewHistogramAggregation(
        HTTPLatencyBounds,
        metricdata.CumulativeTemporality,
    )
    
    // è®°å½•è§‚æµ‹å€¼
    histogram.Record(0.012)  // 12ms
    histogram.Record(0.125)  // 125ms
    histogram.Record(1.5)    // 1.5s
    
    // å¯¼å‡º
    dp := histogram.Export()
    fmt.Printf("Count: %d, Sum: %.3f, Min: %.3f, Max: %.3f\n",
        dp.Count, dp.Sum, dp.Min.Value(), dp.Max.Value())
    
    // è®¡ç®—ç™¾åˆ†ä½æ•°
    p50 := histogram.Percentile(0.50)
    p95 := histogram.Percentile(0.95)
    p99 := histogram.Percentile(0.99)
    
    fmt.Printf("P50: %.3f, P95: %.3f, P99: %.3f\n", p50, p95, p99)
}
```

---

## ExponentialHistogram (æŒ‡æ•°ç›´æ–¹å›¾èšåˆ)

### å®šä¹‰4

**ExponentialHistogram** ä½¿ç”¨æŒ‡æ•°å¢é•¿çš„æ¡¶è¾¹ç•Œï¼Œè‡ªåŠ¨é€‚åº”å€¼çš„èŒƒå›´ã€‚

### Scale å‚æ•°

```text
Scale æ§åˆ¶æ¡¶çš„ç²¾åº¦:

Scale = 0:  base = 2^(2^0) = 2
  Buckets: (1,2], (2,4], (4,8], (8,16], ...

Scale = 1:  base = 2^(2^-1) = âˆš2 â‰ˆ 1.414
  æ›´ç»†çš„ç²’åº¦

Scale = -1: base = 2^(2^1) = 4
  æ›´ç²—çš„ç²’åº¦

Scale è¶Šå¤§ï¼Œç²¾åº¦è¶Šé«˜ï¼Œä½†æ¡¶æ•°é‡è¶Šå¤š
```

### æ¡¶ç´¢å¼•è®¡ç®—

```go
// è®¡ç®—æ¡¶ç´¢å¼•
func calculateBucketIndex(value float64, scale int32) int32 {
    if value == 0 {
        return 0
    }
    
    import "math"
    
    // base = 2^(2^-scale)
    // index = floor(log(value) / log(base))
    
    logBase := math.Pow(2, float64(-scale))
    return int32(math.Floor(math.Log(math.Abs(value)) * logBase))
}

// ç¤ºä¾‹
func exampleBucketIndex() {
    // Scale = 0
    fmt.Println(calculateBucketIndex(1.5, 0))   // 0: (1, 2]
    fmt.Println(calculateBucketIndex(3.0, 0))   // 1: (2, 4]
    fmt.Println(calculateBucketIndex(6.0, 0))   // 2: (4, 8]
    
    // Scale = 1
    fmt.Println(calculateBucketIndex(1.3, 1))   // 0: (1, âˆš2]
    fmt.Println(calculateBucketIndex(1.5, 1))   // 1: (âˆš2, 2]
}
```

### Go å®ç°4

```go
// ExponentialHistogram èšåˆå™¨
type ExponentialHistogramAggregation struct {
    scale         int32
    zeroCount     uint64
    positiveBuckets map[int32]uint64
    negativeBuckets map[int32]uint64
    sum           float64
    count         uint64
    min           float64
    max           float64
    startTime     time.Time
    endTime       time.Time
    temporality   metricdata.Temporality
    mu            sync.RWMutex
}

func NewExponentialHistogramAggregation(
    scale int32,
    temporality metricdata.Temporality,
) *ExponentialHistogramAggregation {
    return &ExponentialHistogramAggregation{
        scale:           scale,
        positiveBuckets: make(map[int32]uint64),
        negativeBuckets: make(map[int32]uint64),
        min:             math.Inf(1),
        max:             math.Inf(-1),
        startTime:       time.Now(),
        temporality:     temporality,
    }
}

// è®°å½•è§‚æµ‹å€¼
func (eha *ExponentialHistogramAggregation) Record(value float64) {
    eha.mu.Lock()
    defer eha.mu.Unlock()
    
    eha.count++
    eha.sum += value
    eha.endTime = time.Now()
    
    if value < eha.min {
        eha.min = value
    }
    if value > eha.max {
        eha.max = value
    }
    
    // å¤„ç†é›¶å€¼
    if value == 0 {
        eha.zeroCount++
        return
    }
    
    // è®¡ç®—æ¡¶ç´¢å¼•
    idx := eha.bucketIndex(value)
    
    // åˆ†æ­£è´Ÿå€¼
    if value > 0 {
        eha.positiveBuckets[idx]++
    } else {
        eha.negativeBuckets[idx]++
    }
}

// è®¡ç®—æ¡¶ç´¢å¼•
func (eha *ExponentialHistogramAggregation) bucketIndex(value float64) int32 {
    import "math"
    
    if value == 0 {
        return 0
    }
    
    logBase := math.Pow(2, float64(-eha.scale))
    return int32(math.Floor(math.Log(math.Abs(value)) * logBase))
}

// å¯¼å‡ºæ•°æ®ç‚¹
func (eha *ExponentialHistogramAggregation) Export() metricdata.ExponentialHistogramDataPoint[float64] {
    eha.mu.RLock()
    defer eha.mu.RUnlock()
    
    // è½¬æ¢ä¸ºæœ‰åºæ¡¶
    positive := eha.convertToOrderedBuckets(eha.positiveBuckets)
    negative := eha.convertToOrderedBuckets(eha.negativeBuckets)
    
    dp := metricdata.ExponentialHistogramDataPoint[float64]{
        Attributes: attribute.EmptySet(),
        StartTime:  eha.startTime,
        Time:       eha.endTime,
        Count:      eha.count,
        Sum:        eha.sum,
        Min:        metricdata.NewExtrema(eha.min),
        Max:        metricdata.NewExtrema(eha.max),
        Scale:      eha.scale,
        ZeroCount:  eha.zeroCount,
        PositiveBucket: positive,
        NegativeBucket: negative,
    }
    
    return dp
}

// è½¬æ¢ä¸ºæœ‰åºæ¡¶
func (eha *ExponentialHistogramAggregation) convertToOrderedBuckets(
    buckets map[int32]uint64,
) metricdata.ExponentialBucket {
    if len(buckets) == 0 {
        return metricdata.ExponentialBucket{}
    }
    
    // æ‰¾åˆ°æœ€å°å’Œæœ€å¤§ç´¢å¼•
    var minIdx, maxIdx int32
    first := true
    for idx := range buckets {
        if first {
            minIdx, maxIdx = idx, idx
            first = false
        } else {
            if idx < minIdx {
                minIdx = idx
            }
            if idx > maxIdx {
                maxIdx = idx
            }
        }
    }
    
    // åˆ›å»ºè¿ç»­æ•°ç»„
    size := int(maxIdx - minIdx + 1)
    counts := make([]uint64, size)
    
    for idx, count := range buckets {
        counts[idx-minIdx] = count
    }
    
    return metricdata.ExponentialBucket{
        Offset:       minIdx,
        Counts:       counts,
    }
}

// é‡ç½®
func (eha *ExponentialHistogramAggregation) Reset() {
    eha.mu.Lock()
    defer eha.mu.Unlock()
    
    eha.count = 0
    eha.sum = 0
    eha.zeroCount = 0
    eha.min = math.Inf(1)
    eha.max = math.Inf(-1)
    eha.startTime = eha.endTime
    eha.positiveBuckets = make(map[int32]uint64)
    eha.negativeBuckets = make(map[int32]uint64)
}
```

---

## èšåˆé…ç½®

### View æœºåˆ¶

```go
import (
    "go.opentelemetry.io/otel/sdk/metric"
)

// View å…è®¸è‡ªå®šä¹‰èšåˆè¡Œä¸º
func setupViews() []metric.View {
    return []metric.View{
        // 1. ä¿®æ”¹ Histogram æ¡¶
        metric.NewView(
            metric.Instrument{
                Name: "http.server.request.duration",
            },
            metric.Stream{
                Aggregation: metric.AggregationExplicitBucketHistogram{
                    Boundaries: []float64{0.01, 0.1, 0.5, 1, 5},
                },
            },
        ),
        
        // 2. å°† Histogram æ”¹ä¸º ExponentialHistogram
        metric.NewView(
            metric.Instrument{
                Name: "http.client.request.duration",
            },
            metric.Stream{
                Aggregation: metric.AggregationBase2ExponentialHistogram{
                    MaxSize:  160,
                    MaxScale: 20,
                },
            },
        ),
        
        // 3. è¿‡æ»¤å±æ€§
        metric.NewView(
            metric.Instrument{
                Name: "http.server.request.count",
            },
            metric.Stream{
                AttributeFilter: attribute.NewSet(
                    attribute.String("http.method", ""),
                    attribute.String("http.route", ""),
                ),
            },
        ),
    }
}
```

### èšåˆé€‰æ‹©å™¨

```go
// è‡ªå®šä¹‰èšåˆé€‰æ‹©å™¨
type CustomAggregationSelector struct{}

func (cas *CustomAggregationSelector) AggregationFor(
    kind metric.InstrumentKind,
) metric.Aggregation {
    switch kind {
    case metric.InstrumentKindCounter:
        return metric.AggregationSum{}
        
    case metric.InstrumentKindUpDownCounter:
        return metric.AggregationSum{}
        
    case metric.InstrumentKindHistogram:
        // ä½¿ç”¨ ExponentialHistogram
        return metric.AggregationBase2ExponentialHistogram{
            MaxSize:  160,
            MaxScale: 20,
        }
        
    case metric.InstrumentKindObservableGauge:
        return metric.AggregationLastValue{}
        
    default:
        return metric.AggregationDefault{}
    }
}
```

### è‡ªå®šä¹‰èšåˆ

```go
// å®ç°è‡ªå®šä¹‰èšåˆ
type MinMaxAggregation struct {
    min float64
    max float64
    mu  sync.RWMutex
}

func NewMinMaxAggregation() *MinMaxAggregation {
    return &MinMaxAggregation{
        min: math.Inf(1),
        max: math.Inf(-1),
    }
}

func (mma *MinMaxAggregation) Record(value float64) {
    mma.mu.Lock()
    defer mma.mu.Unlock()
    
    if value < mma.min {
        mma.min = value
    }
    if value > mma.max {
        mma.max = value
    }
}

func (mma *MinMaxAggregation) Export() (min, max float64) {
    mma.mu.RLock()
    defer mma.mu.RUnlock()
    return mma.min, mma.max
}
```

### Go å®ç°5

```go
// èšåˆå·¥å‚
type AggregationFactory struct {
    temporality metricdata.Temporality
}

func NewAggregationFactory(
    temporality metricdata.Temporality,
) *AggregationFactory {
    return &AggregationFactory{
        temporality: temporality,
    }
}

// åˆ›å»ºèšåˆå™¨
func (af *AggregationFactory) Create(
    kind metric.InstrumentKind,
    config interface{},
) interface{} {
    switch kind {
    case metric.InstrumentKindCounter:
        return NewSumAggregation(af.temporality)
        
    case metric.InstrumentKindUpDownCounter:
        return NewSumAggregation(af.temporality)
        
    case metric.InstrumentKindHistogram:
        if cfg, ok := config.(HistogramConfig); ok {
            return NewHistogramAggregation(cfg.Bounds, af.temporality)
        }
        return NewHistogramAggregation(DefaultBounds, af.temporality)
        
    case metric.InstrumentKindObservableGauge:
        return NewLastValueAggregation()
        
    default:
        return nil
    }
}

type HistogramConfig struct {
    Bounds []float64
}
```

---

## å¤šç»´èšåˆ

### æŒ‰å±æ€§èšåˆ

```go
// å¤šç»´èšåˆå™¨
type MultiDimensionalAggregator struct {
    aggregations map[string]*HistogramAggregation
    bounds       []float64
    temporality  metricdata.Temporality
    mu           sync.RWMutex
}

func NewMultiDimensionalAggregator(
    bounds []float64,
    temporality metricdata.Temporality,
) *MultiDimensionalAggregator {
    return &MultiDimensionalAggregator{
        aggregations: make(map[string]*HistogramAggregation),
        bounds:       bounds,
        temporality:  temporality,
    }
}

// è®°å½•è§‚æµ‹å€¼ï¼ˆå¸¦å±æ€§ï¼‰
func (mda *MultiDimensionalAggregator) Record(
    value float64,
    attrs []attribute.KeyValue,
) {
    key := mda.makeKey(attrs)
    
    mda.mu.RLock()
    agg, exists := mda.aggregations[key]
    mda.mu.RUnlock()
    
    if !exists {
        mda.mu.Lock()
        // Double-check
        agg, exists = mda.aggregations[key]
        if !exists {
            agg = NewHistogramAggregation(mda.bounds, mda.temporality)
            mda.aggregations[key] = agg
        }
        mda.mu.Unlock()
    }
    
    agg.Record(value)
}

// ç”Ÿæˆèšåˆé”®
func (mda *MultiDimensionalAggregator) makeKey(
    attrs []attribute.KeyValue,
) string {
    import "sort"
    import "strings"
    
    pairs := make([]string, len(attrs))
    for i, attr := range attrs {
        pairs[i] = fmt.Sprintf("%s=%v", attr.Key, attr.Value)
    }
    sort.Strings(pairs)
    return strings.Join(pairs, "|")
}

// å¯¼å‡ºæ‰€æœ‰èšåˆ
func (mda *MultiDimensionalAggregator) ExportAll() map[string]metricdata.HistogramDataPoint[float64] {
    mda.mu.RLock()
    defer mda.mu.RUnlock()
    
    result := make(map[string]metricdata.HistogramDataPoint[float64])
    for key, agg := range mda.aggregations {
        result[key] = agg.Export()
    }
    
    return result
}
```

### èšåˆæ ‘

```go
// èšåˆæ ‘èŠ‚ç‚¹
type AggregationTreeNode struct {
    dimension string
    children  map[string]*AggregationTreeNode
    aggregation *HistogramAggregation
    mu        sync.RWMutex
}

func NewAggregationTreeNode(dimension string) *AggregationTreeNode {
    return &AggregationTreeNode{
        dimension: dimension,
        children:  make(map[string]*AggregationTreeNode),
    }
}

// è®°å½•è§‚æµ‹å€¼
func (atn *AggregationTreeNode) Record(
    value float64,
    dims map[string]string,
    remainingDims []string,
) {
    if len(remainingDims) == 0 {
        if atn.aggregation == nil {
            atn.aggregation = NewHistogramAggregation(
                DefaultBounds,
                metricdata.CumulativeTemporality,
            )
        }
        atn.aggregation.Record(value)
        return
    }
    
    nextDim := remainingDims[0]
    value := dims[nextDim]
    
    atn.mu.RLock()
    child, exists := atn.children[dimValue]
    atn.mu.RUnlock()
    
    if !exists {
        atn.mu.Lock()
        child, exists = atn.children[dimValue]
        if !exists {
            child = NewAggregationTreeNode(nextDim)
            atn.children[dimValue] = child
        }
        atn.mu.Unlock()
    }
    
    child.Record(value, dims, remainingDims[1:])
}
```

### åŠ¨æ€èšåˆ

```go
// åŠ¨æ€èšåˆç®¡ç†å™¨
type DynamicAggregationManager struct {
    factory      *AggregationFactory
    aggregations sync.Map  // map[string]interface{}
}

func NewDynamicAggregationManager(
    factory *AggregationFactory,
) *DynamicAggregationManager {
    return &DynamicAggregationManager{
        factory: factory,
    }
}

// è·å–æˆ–åˆ›å»ºèšåˆå™¨
func (dam *DynamicAggregationManager) GetOrCreate(
    name string,
    kind metric.InstrumentKind,
    attrs []attribute.KeyValue,
) interface{} {
    key := makeAggregationKey(name, attrs)
    
    if agg, ok := dam.aggregations.Load(key); ok {
        return agg
    }
    
    agg := dam.factory.Create(kind, nil)
    dam.aggregations.Store(key, agg)
    return agg
}

func makeAggregationKey(name string, attrs []attribute.KeyValue) string {
    import "sort"
    
    pairs := make([]string, len(attrs))
    for i, attr := range attrs {
        pairs[i] = fmt.Sprintf("%s=%v", attr.Key, attr.Value)
    }
    sort.Strings(pairs)
    
    return name + "|" + strings.Join(pairs, "|")
}
```

### Go å®ç°6

```go
// å®Œæ•´çš„å¤šç»´èšåˆç³»ç»Ÿ
type MultiDimensionalSystem struct {
    manager *DynamicAggregationManager
    limiter *CardinalityLimiter
}

type CardinalityLimiter struct {
    maxCardinality int
    currentCount   int
    mu             sync.Mutex
}

func NewCardinalityLimiter(maxCardinality int) *CardinalityLimiter {
    return &CardinalityLimiter{
        maxCardinality: maxCardinality,
    }
}

func (cl *CardinalityLimiter) CheckAndIncrement() error {
    cl.mu.Lock()
    defer cl.mu.Unlock()
    
    if cl.currentCount >= cl.maxCardinality {
        return fmt.Errorf("cardinality limit exceeded: %d", cl.maxCardinality)
    }
    
    cl.currentCount++
    return nil
}
```

---

## é¢„èšåˆ

### ä»€ä¹ˆæ˜¯é¢„èšåˆ

**é¢„èšåˆ** æ˜¯åœ¨æ•°æ®å†™å…¥å‰è¿›è¡Œèšåˆï¼Œå‡å°‘å­˜å‚¨å’ŒæŸ¥è¯¢å¼€é”€ã€‚

```text
æ— é¢„èšåˆ:
åŸå§‹æ•°æ® â†’ å­˜å‚¨ â†’ æŸ¥è¯¢æ—¶èšåˆ

æœ‰é¢„èšåˆ:
åŸå§‹æ•°æ® â†’ é¢„èšåˆ â†’ å­˜å‚¨ â†’ æŸ¥è¯¢ï¼ˆå·²èšåˆï¼‰
```

### é¢„èšåˆç­–ç•¥

```go
// é¢„èšåˆå™¨
type PreAggregator struct {
    interval    time.Duration
    aggregations map[string]*HistogramAggregation
    mu          sync.RWMutex
    ticker      *time.Ticker
    done        chan struct{}
}

func NewPreAggregator(interval time.Duration) *PreAggregator {
    pa := &PreAggregator{
        interval:     interval,
        aggregations: make(map[string]*HistogramAggregation),
        ticker:       time.NewTicker(interval),
        done:         make(chan struct{}),
    }
    
    go pa.run()
    return pa
}

// è®°å½•è§‚æµ‹å€¼
func (pa *PreAggregator) Record(
    key string,
    value float64,
) {
    pa.mu.RLock()
    agg, exists := pa.aggregations[key]
    pa.mu.RUnlock()
    
    if !exists {
        pa.mu.Lock()
        agg, exists = pa.aggregations[key]
        if !exists {
            agg = NewHistogramAggregation(
                DefaultBounds,
                metricdata.DeltaTemporality,
            )
            pa.aggregations[key] = agg
        }
        pa.mu.Unlock()
    }
    
    agg.Record(value)
}

// å®šæœŸå¯¼å‡º
func (pa *PreAggregator) run() {
    for {
        select {
        case <-pa.ticker.C:
            pa.flush()
        case <-pa.done:
            return
        }
    }
}

// åˆ·æ–°æ•°æ®
func (pa *PreAggregator) flush() {
    pa.mu.Lock()
    defer pa.mu.Unlock()
    
    for key, agg := range pa.aggregations {
        dp := agg.Export()
        // å‘é€åˆ°å­˜å‚¨
        sendToStorage(key, dp)
    }
}

func (pa *PreAggregator) Stop() {
    close(pa.done)
    pa.ticker.Stop()
}
```

### Go å®ç°7

```go
// åˆ†å±‚é¢„èšåˆ
type TieredPreAggregator struct {
    realtime *PreAggregator  // å®æ—¶ (1s)
    minute   *PreAggregator  // åˆ†é’Ÿ (60s)
    hour     *PreAggregator  // å°æ—¶ (3600s)
}

func NewTieredPreAggregator() *TieredPreAggregator {
    return &TieredPreAggregator{
        realtime: NewPreAggregator(1 * time.Second),
        minute:   NewPreAggregator(60 * time.Second),
        hour:     NewPreAggregator(3600 * time.Second),
    }
}

func (tpa *TieredPreAggregator) Record(key string, value float64) {
    tpa.realtime.Record(key, value)
    tpa.minute.Record(key, value)
    tpa.hour.Record(key, value)
}

func (tpa *TieredPreAggregator) Stop() {
    tpa.realtime.Stop()
    tpa.minute.Stop()
    tpa.hour.Stop()
}
```

---

## èšåˆåˆå¹¶

### Sum åˆå¹¶

```go
// Sum èšåˆåˆå¹¶
func mergeSumAggregations(aggs []*SumAggregation) *SumAggregation {
    if len(aggs) == 0 {
        return NewSumAggregation(metricdata.CumulativeTemporality)
    }
    
    result := NewSumAggregation(aggs[0].temporality)
    
    for _, agg := range aggs {
        dp := agg.Export()
        result.value += dp.Value
    }
    
    return result
}
```

### Histogram åˆå¹¶

```go
// Histogram èšåˆåˆå¹¶
func mergeHistogramAggregations(
    aggs []*HistogramAggregation,
) *HistogramAggregation {
    if len(aggs) == 0 {
        return nil
    }
    
    // ç¡®ä¿æ‰€æœ‰ Histogram ä½¿ç”¨ç›¸åŒçš„æ¡¶
    bounds := aggs[0].bounds
    result := NewHistogramAggregation(bounds, aggs[0].temporality)
    
    for _, agg := range aggs {
        dp := agg.Export()
        
        result.count += dp.Count
        result.sum += dp.Sum
        
        if dp.Min.Value() < result.min {
            result.min = dp.Min.Value()
        }
        if dp.Max.Value() > result.max {
            result.max = dp.Max.Value()
        }
        
        for i, count := range dp.BucketCounts {
            result.counts[i] += count
        }
    }
    
    return result
}
```

### ExponentialHistogram åˆå¹¶

```go
// ExponentialHistogram èšåˆåˆå¹¶ï¼ˆå¤æ‚ï¼‰
func mergeExponentialHistogramAggregations(
    aggs []*ExponentialHistogramAggregation,
) *ExponentialHistogramAggregation {
    if len(aggs) == 0 {
        return nil
    }
    
    // é€‰æ‹©æœ€å°çš„ scale
    minScale := aggs[0].scale
    for _, agg := range aggs[1:] {
        if agg.scale < minScale {
            minScale = agg.scale
        }
    }
    
    result := NewExponentialHistogramAggregation(
        minScale,
        aggs[0].temporality,
    )
    
    for _, agg := range aggs {
        dp := agg.Export()
        
        result.count += dp.Count
        result.sum += dp.Sum
        result.zeroCount += dp.ZeroCount
        
        // åˆå¹¶æ­£å€¼æ¡¶
        for idx, count := range dp.PositiveBucket.Counts {
            adjustedIdx := adjustIndex(
                idx, dp.PositiveBucket.Offset,
                agg.scale, minScale,
            )
            result.positiveBuckets[adjustedIdx] += count
        }
        
        // åˆå¹¶è´Ÿå€¼æ¡¶
        for idx, count := range dp.NegativeBucket.Counts {
            adjustedIdx := adjustIndex(
                idx, dp.NegativeBucket.Offset,
                agg.scale, minScale,
            )
            result.negativeBuckets[adjustedIdx] += count
        }
    }
    
    return result
}

func adjustIndex(
    idx int, offset int32,
    fromScale, toScale int32,
) int32 {
    actualIdx := int32(idx) + offset
    scaleDiff := fromScale - toScale
    return actualIdx >> scaleDiff
}
```

### Go å®ç°8

```go
// èšåˆåˆå¹¶å™¨
type AggregationMerger struct{}

func (am *AggregationMerger) Merge(
    kind metric.InstrumentKind,
    aggs []interface{},
) interface{} {
    switch kind {
    case metric.InstrumentKindCounter,
         metric.InstrumentKindUpDownCounter:
        sumAggs := make([]*SumAggregation, len(aggs))
        for i, agg := range aggs {
            sumAggs[i] = agg.(*SumAggregation)
        }
        return mergeSumAggregations(sumAggs)
        
    case metric.InstrumentKindHistogram:
        histoAggs := make([]*HistogramAggregation, len(aggs))
        for i, agg := range aggs {
            histoAggs[i] = agg.(*HistogramAggregation)
        }
        return mergeHistogramAggregations(histoAggs)
        
    default:
        return nil
    }
}
```

---

## æ€§èƒ½ä¼˜åŒ–

### 1. å¹¶å‘èšåˆ

```go
// å¹¶å‘å®‰å…¨çš„èšåˆå™¨
type ConcurrentHistogramAggregation struct {
    shards []*HistogramAggregation
    numShards int
}

func NewConcurrentHistogramAggregation(
    bounds []float64,
    temporality metricdata.Temporality,
    numShards int,
) *ConcurrentHistogramAggregation {
    shards := make([]*HistogramAggregation, numShards)
    for i := 0; i < numShards; i++ {
        shards[i] = NewHistogramAggregation(bounds, temporality)
    }
    
    return &ConcurrentHistogramAggregation{
        shards:    shards,
        numShards: numShards,
    }
}

// è®°å½•è§‚æµ‹å€¼
func (cha *ConcurrentHistogramAggregation) Record(value float64) {
    import "hash/maphash"
    
    // æ ¹æ® goroutine ID åˆ†ç‰‡
    shardIdx := int(maphash.Bytes(maphash.MakeSeed(), 
        []byte(fmt.Sprint(getGoroutineID())))) % cha.numShards
    
    cha.shards[shardIdx].Record(value)
}

// å¯¼å‡ºï¼ˆåˆå¹¶æ‰€æœ‰åˆ†ç‰‡ï¼‰
func (cha *ConcurrentHistogramAggregation) Export() metricdata.HistogramDataPoint[float64] {
    return mergeHistogramAggregations(cha.shards).Export()
}
```

### 2. å¢é‡èšåˆ

```go
// å¢é‡èšåˆ
type IncrementalAggregation struct {
    current  *HistogramAggregation
    previous *HistogramAggregation
}

func (ia *IncrementalAggregation) Record(value float64) {
    ia.current.Record(value)
}

func (ia *IncrementalAggregation) ExportDelta() metricdata.HistogramDataPoint[float64] {
    current := ia.current.Export()
    
    if ia.previous == nil {
        ia.previous = ia.current
        ia.current = NewHistogramAggregation(
            ia.current.bounds,
            ia.current.temporality,
        )
        return current
    }
    
    previous := ia.previous.Export()
    
    // è®¡ç®—å¢é‡
    delta := metricdata.HistogramDataPoint[float64]{
        Count: current.Count - previous.Count,
        Sum:   current.Sum - previous.Sum,
        // ... å…¶ä»–å­—æ®µ
    }
    
    ia.previous = ia.current
    ia.current = NewHistogramAggregation(
        ia.current.bounds,
        ia.current.temporality,
    )
    
    return delta
}
```

### 3. å†…å­˜ä¼˜åŒ–

```go
// ä½¿ç”¨å¯¹è±¡æ± 
var histogramAggregationPool = sync.Pool{
    New: func() interface{} {
        return &HistogramAggregation{
            counts: make([]uint64, 20),
        }
    },
}

func acquireHistogramAggregation() *HistogramAggregation {
    return histogramAggregationPool.Get().(*HistogramAggregation)
}

func releaseHistogramAggregation(ha *HistogramAggregation) {
    ha.Reset()
    histogramAggregationPool.Put(ha)
}
```

---

## å®Œæ•´å®ç°

### èšåˆç®¡ç†ç³»ç»Ÿ

```go
package aggregation

import (
    "context"
    "sync"
    "time"
)

// AggregationManager èšåˆç®¡ç†å™¨
type AggregationManager struct {
    factory     *AggregationFactory
    aggregations sync.Map
    merger      *AggregationMerger
    preAggregator *PreAggregator
    mu          sync.RWMutex
}

func NewAggregationManager(
    temporality metricdata.Temporality,
    preAggInterval time.Duration,
) *AggregationManager {
    return &AggregationManager{
        factory:       NewAggregationFactory(temporality),
        merger:        &AggregationMerger{},
        preAggregator: NewPreAggregator(preAggInterval),
    }
}

// è®°å½•è§‚æµ‹å€¼
func (am *AggregationManager) Record(
    ctx context.Context,
    name string,
    kind metric.InstrumentKind,
    value float64,
    attrs []attribute.KeyValue,
) error {
    key := makeAggregationKey(name, attrs)
    
    // è·å–æˆ–åˆ›å»ºèšåˆå™¨
    agg := am.getOrCreateAggregation(key, kind)
    
    // è®°å½•å€¼
    switch a := agg.(type) {
    case *SumAggregation:
        a.Record(int64(value))
    case *Float64SumAggregation:
        a.Record(value)
    case *LastValueAggregation:
        a.Record(value)
    case *HistogramAggregation:
        a.Record(value)
    case *ExponentialHistogramAggregation:
        a.Record(value)
    default:
        return fmt.Errorf("unknown aggregation type")
    }
    
    // é¢„èšåˆ
    am.preAggregator.Record(key, value)
    
    return nil
}

// è·å–æˆ–åˆ›å»ºèšåˆå™¨
func (am *AggregationManager) getOrCreateAggregation(
    key string,
    kind metric.InstrumentKind,
) interface{} {
    if agg, ok := am.aggregations.Load(key); ok {
        return agg
    }
    
    agg := am.factory.Create(kind, HistogramConfig{
        Bounds: DefaultBounds,
    })
    am.aggregations.Store(key, agg)
    return agg
}

// å¯¼å‡ºæ‰€æœ‰èšåˆ
func (am *AggregationManager) ExportAll() map[string]interface{} {
    result := make(map[string]interface{})
    
    am.aggregations.Range(func(key, value interface{}) bool {
        result[key.(string)] = value
        return true
    })
    
    return result
}

// åœæ­¢
func (am *AggregationManager) Stop() {
    am.preAggregator.Stop()
}
```

---

## æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„èšåˆç±»å‹

```go
// âœ… æ¨è
// Counter â†’ Sum
// Gauge â†’ LastValue
// å»¶è¿Ÿ/å¤§å° â†’ Histogram

// âŒ é¿å…
// ä¸è¦å¯¹ Counter ä½¿ç”¨ Histogram
// ä¸è¦å¯¹ Gauge ä½¿ç”¨ Sum
```

### 2. åˆç†é…ç½® Histogram æ¡¶

```go
// âœ… æ¨èï¼šæ ¹æ®å®é™…åˆ†å¸ƒé…ç½®
// HTTP å»¶è¿Ÿ: é›†ä¸­åœ¨æ¯«ç§’çº§
bounds := []float64{0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1}

// âŒ é¿å…ï¼šæ¡¶è¿‡å¤šæˆ–åˆ†å¸ƒä¸å‡
bounds := []float64{0.001, 0.002, 0.003, ...} // å¤ªå¤š
```

### 3. æ§åˆ¶èšåˆç²’åº¦

```go
// âœ… æ¨èï¼šé™åˆ¶æ—¶é—´åºåˆ—æ•°é‡
maxCardinality := 10000

// âœ… æ¨èï¼šä½¿ç”¨ä½åŸºæ•°æ ‡ç­¾
attrs := []attribute.KeyValue{
    attribute.String("http.method", "GET"),  // ä½åŸºæ•°
    attribute.Int("http.status_code", 200),  // ä½åŸºæ•°
}

// âŒ é¿å…ï¼šé«˜åŸºæ•°æ ‡ç­¾
attrs := []attribute.KeyValue{
    attribute.String("user.id", userID),  // é«˜åŸºæ•°ï¼
}
```

### 4. é¢„èšåˆçƒ­æ•°æ®

```go
// âœ… æ¨èï¼šé¢„èšåˆé«˜é¢‘æ•°æ®
preAggregator := NewPreAggregator(1 * time.Second)

// çƒ­è·¯å¾„ç›´æ¥è®°å½•
preAggregator.Record("hot_metric", value)
```

---

## å¸¸è§é—®é¢˜

### Q1: Sum å’Œ LastValue å¦‚ä½•é€‰æ‹©ï¼Ÿ

**A**:

- **Sum**: ç´¯ç§¯å€¼ï¼ˆCounter, UpDownCounterï¼‰
- **LastValue**: ç¬æ—¶å€¼ï¼ˆGaugeï¼‰

---

### Q2: Histogram å’Œ ExponentialHistogram å¦‚ä½•é€‰æ‹©ï¼Ÿ

**A**:

- **Histogram**: å€¼èŒƒå›´å·²çŸ¥ï¼Œéœ€è¦ç²¾ç¡®æ§åˆ¶æ¡¶
- **ExponentialHistogram**: å€¼èŒƒå›´æœªçŸ¥ï¼Œè‡ªåŠ¨é€‚åº”

---

### Q3: å¦‚ä½•ä¼˜åŒ– Histogram æ€§èƒ½ï¼Ÿ

**A**:

1. å‡å°‘æ¡¶æ•°é‡
2. ä½¿ç”¨å¹¶å‘åˆ†ç‰‡
3. é¢„èšåˆ
4. å¯¹è±¡æ± 

---

### Q4: èšåˆæ•°æ®å¦‚ä½•æŒä¹…åŒ–ï¼Ÿ

**A**:

1. å®šæœŸå¯¼å‡ºåˆ°æ—¶åºæ•°æ®åº“
2. ä½¿ç”¨é¢„èšåˆå‡å°‘å­˜å‚¨
3. åˆ†çº§å­˜å‚¨ï¼ˆçƒ­/å†·æ•°æ®ï¼‰

---

### Q5: å¦‚ä½•å¤„ç†èšåˆæ•°æ®çš„è¿‡æœŸï¼Ÿ

**A**:

1. è®¾ç½® TTL
2. å®šæœŸæ¸…ç†
3. LRU æ·˜æ±°ç­–ç•¥

---

## å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£

- [OpenTelemetry Metrics Aggregation](https://opentelemetry.io/docs/specs/otel/metrics/sdk/#aggregation)
- [Views](https://opentelemetry.io/docs/specs/otel/metrics/sdk/#view)

### Go å®ç°9

- [go.opentelemetry.io/otel/sdk/metric](https://pkg.go.dev/go.opentelemetry.io/otel/sdk/metric)

### ç›¸å…³æ–‡æ¡£

- [01_Metricç±»å‹.md](./01_Metricç±»å‹.md)
- [02_æ•°æ®ç‚¹.md](./02_æ•°æ®ç‚¹.md)
- [03_æ—¶é—´åºåˆ—.md](./03_æ—¶é—´åºåˆ—.md)

---

**ğŸ‰ æ­å–œï¼ä½ å·²ç»æŒæ¡äº†èšåˆçš„å®Œæ•´çŸ¥è¯†ï¼**

ä¸‹ä¸€æ­¥ï¼šå­¦ä¹  [Exemplars](./06_Exemplars.md) äº†è§£é‡‡æ ·æ•°æ®ç‚¹ã€‚
