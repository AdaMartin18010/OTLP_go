# 04 - Kubernetes èµ„æºå±æ€§ (k8s.*)

## ğŸ“‹ ç›®å½•

- [04 - Kubernetes èµ„æºå±æ€§ (k8s.\*)](#04---kubernetes-èµ„æºå±æ€§-k8s)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. Kubernetes èµ„æºæ¦‚è¿°](#1-kubernetes-èµ„æºæ¦‚è¿°)
    - [1.1 k8s.\* å±æ€§å‘½åç©ºé—´](#11-k8s-å±æ€§å‘½åç©ºé—´)
    - [1.2 èµ„æºå±‚çº§å…³ç³»](#12-èµ„æºå±‚çº§å…³ç³»)
    - [1.3 å±æ€§ä¼˜å…ˆçº§](#13-å±æ€§ä¼˜å…ˆçº§)
  - [2. é›†ç¾¤çº§åˆ«å±æ€§](#2-é›†ç¾¤çº§åˆ«å±æ€§)
    - [2.1 k8s.cluster.\* å±æ€§](#21-k8scluster-å±æ€§)
    - [2.2 é›†ç¾¤æ ‡è¯†å®ç°](#22-é›†ç¾¤æ ‡è¯†å®ç°)
    - [2.3 å¤šé›†ç¾¤ç®¡ç†](#23-å¤šé›†ç¾¤ç®¡ç†)
  - [3. èŠ‚ç‚¹çº§åˆ«å±æ€§](#3-èŠ‚ç‚¹çº§åˆ«å±æ€§)
    - [3.1 k8s.node.\* å±æ€§](#31-k8snode-å±æ€§)
    - [3.2 èŠ‚ç‚¹ä¿¡æ¯è·å–](#32-èŠ‚ç‚¹ä¿¡æ¯è·å–)
    - [3.3 èŠ‚ç‚¹æ ‡ç­¾å’Œæ³¨è§£](#33-èŠ‚ç‚¹æ ‡ç­¾å’Œæ³¨è§£)
  - [4. å‘½åç©ºé—´å±æ€§](#4-å‘½åç©ºé—´å±æ€§)
    - [4.1 k8s.namespace.\* å±æ€§](#41-k8snamespace-å±æ€§)
    - [4.2 å‘½åç©ºé—´éš”ç¦»](#42-å‘½åç©ºé—´éš”ç¦»)
    - [4.3 å¤šç§Ÿæˆ·ç¯å¢ƒ](#43-å¤šç§Ÿæˆ·ç¯å¢ƒ)
  - [5. Pod çº§åˆ«å±æ€§](#5-pod-çº§åˆ«å±æ€§)
    - [5.1 k8s.pod.\* å±æ€§](#51-k8spod-å±æ€§)
    - [5.2 Pod ç”Ÿå‘½å‘¨æœŸè¿½è¸ª](#52-pod-ç”Ÿå‘½å‘¨æœŸè¿½è¸ª)
    - [5.3 Pod é‡å¯å’Œæ•…éšœ](#53-pod-é‡å¯å’Œæ•…éšœ)
  - [6. å®¹å™¨çº§åˆ«å±æ€§](#6-å®¹å™¨çº§åˆ«å±æ€§)
    - [6.1 k8s.container.\* å±æ€§](#61-k8scontainer-å±æ€§)
    - [6.2 å®¹å™¨è¿è¡Œæ—¶é›†æˆ](#62-å®¹å™¨è¿è¡Œæ—¶é›†æˆ)
    - [6.3 Sidecar å®¹å™¨](#63-sidecar-å®¹å™¨)
  - [7. å·¥ä½œè´Ÿè½½å±æ€§](#7-å·¥ä½œè´Ÿè½½å±æ€§)
    - [7.1 k8s.deployment.\* å±æ€§](#71-k8sdeployment-å±æ€§)
    - [7.2 k8s.replicaset.\* å±æ€§](#72-k8sreplicaset-å±æ€§)
    - [7.3 k8s.statefulset.\* å±æ€§](#73-k8sstatefulset-å±æ€§)
    - [7.4 k8s.daemonset.\* å±æ€§](#74-k8sdaemonset-å±æ€§)
    - [7.5 k8s.job.*å’Œ k8s.cronjob.* å±æ€§](#75-k8sjobå’Œ-k8scronjob-å±æ€§)
  - [8. Go å®ç° - K8s èµ„æºæ£€æµ‹å™¨](#8-go-å®ç°---k8s-èµ„æºæ£€æµ‹å™¨)
    - [8.1 Downward API æ–¹å¼](#81-downward-api-æ–¹å¼)
    - [8.2 ç¯å¢ƒå˜é‡æ³¨å…¥](#82-ç¯å¢ƒå˜é‡æ³¨å…¥)
    - [8.3 K8s API æŸ¥è¯¢](#83-k8s-api-æŸ¥è¯¢)
  - [9. èµ„æºæ£€æµ‹æœ€ä½³å®è·µ](#9-èµ„æºæ£€æµ‹æœ€ä½³å®è·µ)
    - [9.1 æ£€æµ‹ä¼˜å…ˆçº§](#91-æ£€æµ‹ä¼˜å…ˆçº§)
    - [9.2 æ€§èƒ½ä¼˜åŒ–](#92-æ€§èƒ½ä¼˜åŒ–)
    - [9.3 ç¼“å­˜ç­–ç•¥](#93-ç¼“å­˜ç­–ç•¥)
  - [10. å®Œæ•´ç¤ºä¾‹](#10-å®Œæ•´ç¤ºä¾‹)
    - [10.1 å…¨åŠŸèƒ½èµ„æºæ£€æµ‹å™¨](#101-å…¨åŠŸèƒ½èµ„æºæ£€æµ‹å™¨)
    - [10.2 Kubernetes Manifest](#102-kubernetes-manifest)
    - [10.3 Helm Chart é›†æˆ](#103-helm-chart-é›†æˆ)
  - [11. å¸¸è§é—®é¢˜ (FAQ)](#11-å¸¸è§é—®é¢˜-faq)
    - [Q1: ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ Downward API vs K8s API?](#q1-ä»€ä¹ˆæ—¶å€™ä½¿ç”¨-downward-api-vs-k8s-api)
    - [Q2: å¦‚ä½•æœ€å°åŒ– RBAC æƒé™?](#q2-å¦‚ä½•æœ€å°åŒ–-rbac-æƒé™)
    - [Q3: é›†ç¾¤åç§°å¦‚ä½•æ ‡å‡†åŒ–?](#q3-é›†ç¾¤åç§°å¦‚ä½•æ ‡å‡†åŒ–)
    - [Q4: å¤šé›†ç¾¤ç¯å¢ƒå¦‚ä½•åŒºåˆ†?](#q4-å¤šé›†ç¾¤ç¯å¢ƒå¦‚ä½•åŒºåˆ†)
    - [Q5: å¦‚ä½•å¤„ç† Pod é‡å¯?](#q5-å¦‚ä½•å¤„ç†-pod-é‡å¯)
    - [Q6: StatefulSet å¦‚ä½•å¤„ç†?](#q6-statefulset-å¦‚ä½•å¤„ç†)
    - [Q7: CronJob å¦‚ä½•è¿½è¸ª?](#q7-cronjob-å¦‚ä½•è¿½è¸ª)
    - [Q8: å¦‚ä½•å¤„ç†æ—  K8s ç¯å¢ƒ?](#q8-å¦‚ä½•å¤„ç†æ— -k8s-ç¯å¢ƒ)
    - [Q9: èµ„æºæ£€æµ‹å¤±è´¥å¦‚ä½•å¤„ç†?](#q9-èµ„æºæ£€æµ‹å¤±è´¥å¦‚ä½•å¤„ç†)
    - [Q10: æ€§èƒ½å½±å“å¦‚ä½•?](#q10-æ€§èƒ½å½±å“å¦‚ä½•)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)

---

## 1. Kubernetes èµ„æºæ¦‚è¿°

### 1.1 k8s.* å±æ€§å‘½åç©ºé—´

**æ ¸å¿ƒæ¦‚å¿µ**:

- `k8s.*` å‘½åç©ºé—´ä¸“ç”¨äº Kubernetes ç‰¹å®šçš„èµ„æºå±æ€§
- éµå¾ª OpenTelemetry Semantic Conventions 1.28.0+ æ ‡å‡†
- ä¸ `container.*` å±æ€§äº’è¡¥,æä¾›å®Œæ•´çš„å®¹å™¨ç¼–æ’ä¿¡æ¯

**å±æ€§åˆ†ç±»**:

| ç±»åˆ« | å‰ç¼€ | ç”¨é€” | ç¤ºä¾‹ |
|------|------|------|------|
| é›†ç¾¤ | `k8s.cluster.*` | é›†ç¾¤æ ‡è¯† | `k8s.cluster.name` |
| èŠ‚ç‚¹ | `k8s.node.*` | èŠ‚ç‚¹ä¿¡æ¯ | `k8s.node.name` |
| å‘½åç©ºé—´ | `k8s.namespace.*` | å‘½åç©ºé—´ | `k8s.namespace.name` |
| Pod | `k8s.pod.*` | Pod ä¿¡æ¯ | `k8s.pod.name` |
| å®¹å™¨ | `k8s.container.*` | å®¹å™¨ä¿¡æ¯ | `k8s.container.name` |
| å·¥ä½œè´Ÿè½½ | `k8s.deployment.*` ç­‰ | å·¥ä½œè´Ÿè½½ | `k8s.deployment.name` |

### 1.2 èµ„æºå±‚çº§å…³ç³»

```mermaid
graph TD
    A[k8s.cluster.name] --> B[k8s.node.name]
    A --> C[k8s.namespace.name]
    C --> D[k8s.deployment.name]
    D --> E[k8s.replicaset.name]
    E --> F[k8s.pod.name]
    F --> G[k8s.container.name]
    B --> F
```

**å±‚çº§è¯´æ˜**:

1. **é›†ç¾¤å±‚**: æœ€é¡¶å±‚,æ ‡è¯†æ•´ä¸ª K8s é›†ç¾¤
2. **èŠ‚ç‚¹å±‚**: ç‰©ç†/è™šæ‹ŸæœºèŠ‚ç‚¹
3. **å‘½åç©ºé—´å±‚**: é€»è¾‘éš”ç¦»è¾¹ç•Œ
4. **å·¥ä½œè´Ÿè½½å±‚**: Deployment/StatefulSet/DaemonSet ç­‰
5. **ReplicaSet å±‚**: Pod å‰¯æœ¬æ§åˆ¶å™¨
6. **Pod å±‚**: æœ€å°è°ƒåº¦å•å…ƒ
7. **å®¹å™¨å±‚**: å®é™…è¿è¡Œçš„å®¹å™¨

### 1.3 å±æ€§ä¼˜å…ˆçº§

**æ£€æµ‹é¡ºåº** (ä»é«˜åˆ°ä½):

```go
type K8sAttributePriority int

const (
    // ç¯å¢ƒå˜é‡ä¼˜å…ˆ (Downward API æ³¨å…¥)
    PriorityEnvVar K8sAttributePriority = 1
    
    // æ–‡ä»¶ç³»ç»ŸæŒ‚è½½ (Downward API Volume)
    PriorityVolume K8sAttributePriority = 2
    
    // K8s API Server æŸ¥è¯¢
    PriorityAPIServer K8sAttributePriority = 3
    
    // å®¹å™¨è¿è¡Œæ—¶æ¨æ–­
    PriorityInferred K8sAttributePriority = 4
)
```

**æœ€ä½³å®è·µ**:

- **ä¼˜å…ˆä½¿ç”¨ Downward API**: æ€§èƒ½æœ€ä¼˜,æ— éœ€ API Server æƒé™
- **é¿å… API Server æŸ¥è¯¢**: å¢åŠ å»¶è¿Ÿå’Œé›†ç¾¤è´Ÿè½½
- **ç¼“å­˜å·²è·å–çš„å±æ€§**: é¿å…é‡å¤æŸ¥è¯¢

---

## 2. é›†ç¾¤çº§åˆ«å±æ€§

### 2.1 k8s.cluster.* å±æ€§

**æ ‡å‡†å±æ€§**:

| å±æ€§å | ç±»å‹ | å¿…éœ€ | æè¿° | ç¤ºä¾‹ |
|--------|------|------|------|------|
| `k8s.cluster.name` | string | âœ… | é›†ç¾¤åç§° | `prod-us-east-1` |
| `k8s.cluster.uid` | string | æ¨è | é›†ç¾¤å”¯ä¸€ ID | `550e8400-e29b-41d4-a716-446655440000` |

**å±æ€§æ¥æº**:

```yaml
# æ–¹å¼1: é›†ç¾¤çº§åˆ« ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-info
  namespace: kube-public
data:
  cluster.name: "prod-us-east-1"
  cluster.uid: "550e8400-e29b-41d4-a716-446655440000"

---
# æ–¹å¼2: é€šè¿‡ kube-system Namespace UID (æ¨è)
# æŸ¥è¯¢å‘½ä»¤: kubectl get namespace kube-system -o jsonpath='{.metadata.uid}'
```

### 2.2 é›†ç¾¤æ ‡è¯†å®ç°

**Go å®ç°**:

```go
package k8sresource

import (
    "context"
    "os"
    "sync"
    
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/sdk/resource"
    semconv "go.opentelemetry.io/otel/semconv/v1.28.0"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes"
    "k8s.io/client-go/rest"
)

// ClusterDetector æ£€æµ‹é›†ç¾¤ä¿¡æ¯
type ClusterDetector struct {
    clientset *kubernetes.Clientset
    cache     *clusterCache
    mu        sync.RWMutex
}

type clusterCache struct {
    name string
    uid  string
}

func NewClusterDetector() (*ClusterDetector, error) {
    // ä½¿ç”¨ In-Cluster Config
    config, err := rest.InClusterConfig()
    if err != nil {
        return nil, err
    }
    
    clientset, err := kubernetes.NewForConfig(config)
    if err != nil {
        return nil, err
    }
    
    return &ClusterDetector{
        clientset: clientset,
        cache:     &clusterCache{},
    }, nil
}

func (d *ClusterDetector) Detect(ctx context.Context) (*resource.Resource, error) {
    // ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–
    if name := os.Getenv("K8S_CLUSTER_NAME"); name != "" {
        attrs := []attribute.KeyValue{
            semconv.K8SClusterName(name),
        }
        
        if uid := os.Getenv("K8S_CLUSTER_UID"); uid != "" {
            attrs = append(attrs, semconv.K8SClusterUID(uid))
        }
        
        return resource.NewWithAttributes(
            semconv.SchemaURL,
            attrs...,
        ), nil
    }
    
    // æ£€æŸ¥ç¼“å­˜
    d.mu.RLock()
    if d.cache.name != "" {
        defer d.mu.RUnlock()
        return d.buildResource(d.cache.name, d.cache.uid), nil
    }
    d.mu.RUnlock()
    
    // ä» kube-system Namespace è·å–é›†ç¾¤ UID
    ns, err := d.clientset.CoreV1().Namespaces().Get(
        ctx, 
        "kube-system", 
        metav1.GetOptions{},
    )
    if err != nil {
        return nil, err
    }
    
    clusterUID := string(ns.UID)
    
    // å°è¯•ä» ConfigMap è·å–é›†ç¾¤åç§°
    clusterName, err := d.getClusterName(ctx)
    if err != nil {
        // å¦‚æœæ— æ³•è·å–åç§°,ä½¿ç”¨ UID çš„å‰ç¼€ä½œä¸ºåç§°
        clusterName = "cluster-" + clusterUID[:8]
    }
    
    // æ›´æ–°ç¼“å­˜
    d.mu.Lock()
    d.cache.name = clusterName
    d.cache.uid = clusterUID
    d.mu.Unlock()
    
    return d.buildResource(clusterName, clusterUID), nil
}

func (d *ClusterDetector) getClusterName(ctx context.Context) (string, error) {
    cm, err := d.clientset.CoreV1().ConfigMaps("kube-public").Get(
        ctx,
        "cluster-info",
        metav1.GetOptions{},
    )
    if err != nil {
        return "", err
    }
    
    if name, ok := cm.Data["cluster.name"]; ok {
        return name, nil
    }
    
    return "", nil
}

func (d *ClusterDetector) buildResource(name, uid string) *resource.Resource {
    attrs := []attribute.KeyValue{
        semconv.K8SClusterName(name),
    }
    
    if uid != "" {
        attrs = append(attrs, semconv.K8SClusterUID(uid))
    }
    
    return resource.NewWithAttributes(
        semconv.SchemaURL,
        attrs...,
    )
}
```

### 2.3 å¤šé›†ç¾¤ç®¡ç†

**åœºæ™¯**: è·¨å¤šä¸ª Kubernetes é›†ç¾¤çš„æœåŠ¡

**ç­–ç•¥**:

```go
// å¤šé›†ç¾¤èµ„æºèšåˆ
func DetectMultiCluster(ctx context.Context) (*resource.Resource, error) {
    // ä¸»é›†ç¾¤ä¿¡æ¯
    primary, err := NewClusterDetector()
    if err != nil {
        return nil, err
    }
    primaryRes, err := primary.Detect(ctx)
    if err != nil {
        return nil, err
    }
    
    // æ·»åŠ é›†ç¾¤è§’è‰²æ ‡è¯†
    attrs := []attribute.KeyValue{
        attribute.String("k8s.cluster.role", "primary"),
    }
    
    // å¦‚æœæ˜¯è”é‚¦é›†ç¾¤,æ·»åŠ è”é‚¦ä¿¡æ¯
    if federationID := os.Getenv("K8S_FEDERATION_ID"); federationID != "" {
        attrs = append(attrs, 
            attribute.String("k8s.federation.id", federationID),
        )
    }
    
    return resource.Merge(
        primaryRes,
        resource.NewWithAttributes(semconv.SchemaURL, attrs...),
    )
}
```

---

## 3. èŠ‚ç‚¹çº§åˆ«å±æ€§

### 3.1 k8s.node.* å±æ€§

**æ ‡å‡†å±æ€§**:

| å±æ€§å | ç±»å‹ | å¿…éœ€ | æè¿° | ç¤ºä¾‹ |
|--------|------|------|------|------|
| `k8s.node.name` | string | âœ… | èŠ‚ç‚¹åç§° | `node-1.us-east-1.compute.internal` |
| `k8s.node.uid` | string | æ¨è | èŠ‚ç‚¹å”¯ä¸€ ID | `ac3d7f9e-2b5c-4d8a-9f1e-3c4b5a6d7e8f` |

**è·å–æ–¹å¼**:

```yaml
# Downward API - æ³¨å…¥åˆ°ç¯å¢ƒå˜é‡
env:
- name: K8S_NODE_NAME
  valueFrom:
    fieldRef:
      fieldPath: spec.nodeName
      
# Downward API - æŒ‚è½½åˆ°æ–‡ä»¶
volumes:
- name: podinfo
  downwardAPI:
    items:
    - path: "nodename"
      fieldRef:
        fieldPath: spec.nodeName
```

### 3.2 èŠ‚ç‚¹ä¿¡æ¯è·å–

**Go å®ç°**:

```go
package k8sresource

import (
    "context"
    "os"
    
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/sdk/resource"
    semconv "go.opentelemetry.io/otel/semconv/v1.28.0"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes"
)

// NodeDetector æ£€æµ‹èŠ‚ç‚¹ä¿¡æ¯
type NodeDetector struct {
    clientset *kubernetes.Clientset
}

func NewNodeDetector(clientset *kubernetes.Clientset) *NodeDetector {
    return &NodeDetector{clientset: clientset}
}

func (d *NodeDetector) Detect(ctx context.Context) (*resource.Resource, error) {
    // ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å– (Downward API æ³¨å…¥)
    nodeName := os.Getenv("K8S_NODE_NAME")
    if nodeName == "" {
        nodeName = os.Getenv("NODE_NAME") // å¤‡é€‰
    }
    
    if nodeName == "" {
        return resource.Empty(), nil
    }
    
    attrs := []attribute.KeyValue{
        semconv.K8SNodeName(nodeName),
    }
    
    // å¦‚æœæœ‰ API æƒé™,è·å–èŠ‚ç‚¹ UID
    if d.clientset != nil {
        node, err := d.clientset.CoreV1().Nodes().Get(
            ctx,
            nodeName,
            metav1.GetOptions{},
        )
        if err == nil {
            attrs = append(attrs, semconv.K8SNodeUID(string(node.UID)))
        }
    }
    
    return resource.NewWithAttributes(
        semconv.SchemaURL,
        attrs...,
    ), nil
}
```

### 3.3 èŠ‚ç‚¹æ ‡ç­¾å’Œæ³¨è§£

**æ‰©å±•å±æ€§** (é€šè¿‡èŠ‚ç‚¹æ ‡ç­¾):

```go
// è·å–èŠ‚ç‚¹çš„è‡ªå®šä¹‰æ ‡ç­¾å’Œæ³¨è§£
func (d *NodeDetector) DetectWithLabels(ctx context.Context) (*resource.Resource, error) {
    nodeName := os.Getenv("K8S_NODE_NAME")
    if nodeName == "" || d.clientset == nil {
        return d.Detect(ctx)
    }
    
    node, err := d.clientset.CoreV1().Nodes().Get(
        ctx,
        nodeName,
        metav1.GetOptions{},
    )
    if err != nil {
        return d.Detect(ctx)
    }
    
    attrs := []attribute.KeyValue{
        semconv.K8SNodeName(nodeName),
        semconv.K8SNodeUID(string(node.UID)),
    }
    
    // æ·»åŠ é‡è¦çš„èŠ‚ç‚¹æ ‡ç­¾
    if zone, ok := node.Labels["topology.kubernetes.io/zone"]; ok {
        attrs = append(attrs, attribute.String("k8s.node.zone", zone))
    }
    
    if region, ok := node.Labels["topology.kubernetes.io/region"]; ok {
        attrs = append(attrs, attribute.String("k8s.node.region", region))
    }
    
    if instanceType, ok := node.Labels["node.kubernetes.io/instance-type"]; ok {
        attrs = append(attrs, attribute.String("k8s.node.instance_type", instanceType))
    }
    
    return resource.NewWithAttributes(
        semconv.SchemaURL,
        attrs...,
    ), nil
}
```

---

## 4. å‘½åç©ºé—´å±æ€§

### 4.1 k8s.namespace.* å±æ€§

**æ ‡å‡†å±æ€§**:

| å±æ€§å | ç±»å‹ | å¿…éœ€ | æè¿° | ç¤ºä¾‹ |
|--------|------|------|------|------|
| `k8s.namespace.name` | string | âœ… | å‘½åç©ºé—´åç§° | `production` |

**è·å–æ–¹å¼**:

```yaml
# Deployment manifest
apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      containers:
      - name: app
        env:
        - name: K8S_NAMESPACE_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
```

### 4.2 å‘½åç©ºé—´éš”ç¦»

**Go å®ç°**:

```go
package k8sresource

import (
    "context"
    "os"
    
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/sdk/resource"
    semconv "go.opentelemetry.io/otel/semconv/v1.28.0"
)

// NamespaceDetector æ£€æµ‹å‘½åç©ºé—´
type NamespaceDetector struct{}

func NewNamespaceDetector() *NamespaceDetector {
    return &NamespaceDetector{}
}

func (d *NamespaceDetector) Detect(ctx context.Context) (*resource.Resource, error) {
    // æ–¹å¼1: Downward API ç¯å¢ƒå˜é‡
    namespace := os.Getenv("K8S_NAMESPACE_NAME")
    if namespace == "" {
        namespace = os.Getenv("POD_NAMESPACE") // å¤‡é€‰
    }
    
    // æ–¹å¼2: Service Account æŒ‚è½½ (fallback)
    if namespace == "" {
        data, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/namespace")
        if err == nil {
            namespace = string(data)
        }
    }
    
    if namespace == "" {
        return resource.Empty(), nil
    }
    
    return resource.NewWithAttributes(
        semconv.SchemaURL,
        semconv.K8SNamespaceName(namespace),
    ), nil
}
```

### 4.3 å¤šç§Ÿæˆ·ç¯å¢ƒ

**åœºæ™¯**: éœ€è¦è¯†åˆ«ç§Ÿæˆ·ä¿¡æ¯

```go
// ç»“åˆå‘½åç©ºé—´å’Œè‡ªå®šä¹‰æ ‡ç­¾è¯†åˆ«ç§Ÿæˆ·
func DetectTenant(ctx context.Context, clientset *kubernetes.Clientset) (*resource.Resource, error) {
    namespace := os.Getenv("K8S_NAMESPACE_NAME")
    if namespace == "" {
        return resource.Empty(), nil
    }
    
    attrs := []attribute.KeyValue{
        semconv.K8SNamespaceName(namespace),
    }
    
    // ä» Namespace æ ‡ç­¾è·å–ç§Ÿæˆ·ä¿¡æ¯
    if clientset != nil {
        ns, err := clientset.CoreV1().Namespaces().Get(
            ctx,
            namespace,
            metav1.GetOptions{},
        )
        if err == nil {
            if tenantID, ok := ns.Labels["tenant.id"]; ok {
                attrs = append(attrs, attribute.String("tenant.id", tenantID))
            }
            
            if tenantName, ok := ns.Labels["tenant.name"]; ok {
                attrs = append(attrs, attribute.String("tenant.name", tenantName))
            }
        }
    }
    
    return resource.NewWithAttributes(
        semconv.SchemaURL,
        attrs...,
    ), nil
}
```

---

## 5. Pod çº§åˆ«å±æ€§

### 5.1 k8s.pod.* å±æ€§

**æ ‡å‡†å±æ€§**:

| å±æ€§å | ç±»å‹ | å¿…éœ€ | æè¿° | ç¤ºä¾‹ |
|--------|------|------|------|------|
| `k8s.pod.name` | string | âœ… | Pod åç§° | `web-deployment-7d4f5c6b8-xk9mz` |
| `k8s.pod.uid` | string | âœ… | Pod å”¯ä¸€ ID | `275ecb36-5aa8-4c2a-9c47-d8bb681b9aff` |

**è·å–æ–¹å¼**:

```yaml
# Downward API æ³¨å…¥
env:
- name: K8S_POD_NAME
  valueFrom:
    fieldRef:
      fieldPath: metadata.name
- name: K8S_POD_UID
  valueFrom:
    fieldRef:
      fieldPath: metadata.uid
```

### 5.2 Pod ç”Ÿå‘½å‘¨æœŸè¿½è¸ª

**Go å®ç°**:

```go
package k8sresource

import (
    "context"
    "os"
    
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/sdk/resource"
    semconv "go.opentelemetry.io/otel/semconv/v1.28.0"
)

// PodDetector æ£€æµ‹ Pod ä¿¡æ¯
type PodDetector struct{}

func NewPodDetector() *PodDetector {
    return &PodDetector{}
}

func (d *PodDetector) Detect(ctx context.Context) (*resource.Resource, error) {
    podName := os.Getenv("K8S_POD_NAME")
    if podName == "" {
        podName = os.Getenv("POD_NAME") // å¤‡é€‰
    }
    
    podUID := os.Getenv("K8S_POD_UID")
    if podUID == "" {
        podUID = os.Getenv("POD_UID") // å¤‡é€‰
    }
    
    if podName == "" || podUID == "" {
        return resource.Empty(), nil
    }
    
    attrs := []attribute.KeyValue{
        semconv.K8SPodName(podName),
        semconv.K8SPodUID(podUID),
    }
    
    // å¯é€‰: Pod IP
    if podIP := os.Getenv("K8S_POD_IP"); podIP != "" {
        attrs = append(attrs, attribute.String("k8s.pod.ip", podIP))
    }
    
    return resource.NewWithAttributes(
        semconv.SchemaURL,
        attrs...,
    ), nil
}
```

### 5.3 Pod é‡å¯å’Œæ•…éšœ

**æ‰©å±•è¿½è¸ª**:

```go
// æ·»åŠ  Pod é‡å¯è®¡æ•°å’ŒçŠ¶æ€
func (d *PodDetector) DetectWithStatus(ctx context.Context) (*resource.Resource, error) {
    baseRes, err := d.Detect(ctx)
    if err != nil {
        return nil, err
    }
    
    attrs := []attribute.KeyValue{}
    
    // ä»ç¯å¢ƒå˜é‡è·å– Pod å¯åŠ¨æ—¶é—´
    if startTime := os.Getenv("K8S_POD_START_TIME"); startTime != "" {
        attrs = append(attrs, attribute.String("k8s.pod.start_time", startTime))
    }
    
    // é‡å¯è®¡æ•° (éœ€è¦ä» Pod Status è·å–,éœ€è¦ API è®¿é—®)
    // è¿™é‡Œå±•ç¤ºå¦‚ä½•æ·»åŠ è‡ªå®šä¹‰å±æ€§
    
    if len(attrs) == 0 {
        return baseRes, nil
    }
    
    return resource.Merge(
        baseRes,
        resource.NewWithAttributes(semconv.SchemaURL, attrs...),
    )
}
```

---

## 6. å®¹å™¨çº§åˆ«å±æ€§

### 6.1 k8s.container.* å±æ€§

**æ ‡å‡†å±æ€§**:

| å±æ€§å | ç±»å‹ | å¿…éœ€ | æè¿° | ç¤ºä¾‹ |
|--------|------|------|------|------|
| `k8s.container.name` | string | æ¨è | å®¹å™¨åç§° | `web-server` |
| `k8s.container.restart_count` | int | å¯é€‰ | é‡å¯æ¬¡æ•° | `3` |

**ä¸ container.* å±æ€§çš„åŒºåˆ«**:

- `container.name`: å®¹å™¨è¿è¡Œæ—¶åç§° (e.g., Docker container name)
- `k8s.container.name`: Pod manifest ä¸­å®šä¹‰çš„å®¹å™¨åç§°

### 6.2 å®¹å™¨è¿è¡Œæ—¶é›†æˆ

**Go å®ç°**:

```go
package k8sresource

import (
    "context"
    "os"
    
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/sdk/resource"
    semconv "go.opentelemetry.io/otel/semconv/v1.28.0"
)

// ContainerDetector æ£€æµ‹å®¹å™¨ä¿¡æ¯
type ContainerDetector struct{}

func NewContainerDetector() *ContainerDetector {
    return &ContainerDetector{}
}

func (d *ContainerDetector) Detect(ctx context.Context) (*resource.Resource, error) {
    // ä»ç¯å¢ƒå˜é‡è·å–å®¹å™¨åç§°
    containerName := os.Getenv("K8S_CONTAINER_NAME")
    if containerName == "" {
        containerName = os.Getenv("CONTAINER_NAME") // å¤‡é€‰
    }
    
    if containerName == "" {
        return resource.Empty(), nil
    }
    
    attrs := []attribute.KeyValue{
        semconv.K8SContainerName(containerName),
    }
    
    // å¯é€‰: å®¹å™¨é•œåƒä¿¡æ¯
    if image := os.Getenv("K8S_CONTAINER_IMAGE"); image != "" {
        attrs = append(attrs, attribute.String("k8s.container.image", image))
    }
    
    return resource.NewWithAttributes(
        semconv.SchemaURL,
        attrs...,
    ), nil
}
```

**Downward API é…ç½®**:

```yaml
env:
- name: K8S_CONTAINER_NAME
  valueFrom:
    fieldRef:
      fieldPath: metadata.name  # æ³¨æ„: è¿™æ˜¯ Pod åç§°
      
# å®¹å™¨åç§°éœ€è¦åœ¨ Deployment ä¸­ç›´æ¥è®¾ç½®
- name: CONTAINER_NAME
  value: "web-server"  # ç¡¬ç¼–ç å®¹å™¨åç§°
```

### 6.3 Sidecar å®¹å™¨

**åœºæ™¯**: åŒºåˆ†ä¸»å®¹å™¨å’Œ Sidecar

```go
// å®¹å™¨è§’è‰²æ£€æµ‹
func DetectContainerRole() string {
    containerName := os.Getenv("CONTAINER_NAME")
    
    // æ ¹æ®å‘½åçº¦å®šè¯†åˆ«è§’è‰²
    switch {
    case containerName == "app" || containerName == "main":
        return "primary"
    case containerName == "envoy" || containerName == "istio-proxy":
        return "sidecar.proxy"
    case containerName == "log-collector":
        return "sidecar.logging"
    case containerName == "metrics-exporter":
        return "sidecar.metrics"
    default:
        return "unknown"
    }
}

// å¸¦è§’è‰²çš„èµ„æºæ£€æµ‹
func (d *ContainerDetector) DetectWithRole(ctx context.Context) (*resource.Resource, error) {
    baseRes, err := d.Detect(ctx)
    if err != nil {
        return nil, err
    }
    
    role := DetectContainerRole()
    
    return resource.Merge(
        baseRes,
        resource.NewWithAttributes(
            semconv.SchemaURL,
            attribute.String("k8s.container.role", role),
        ),
    )
}
```

---

## 7. å·¥ä½œè´Ÿè½½å±æ€§

### 7.1 k8s.deployment.* å±æ€§

**æ ‡å‡†å±æ€§**:

| å±æ€§å | ç±»å‹ | å¿…éœ€ | æè¿° | ç¤ºä¾‹ |
|--------|------|------|------|------|
| `k8s.deployment.name` | string | æ¨è | Deployment åç§° | `web-deployment` |
| `k8s.deployment.uid` | string | å¯é€‰ | Deployment UID | `8f3d2e1c-4b5a-6d7e-8f9a-0b1c2d3e4f5a` |

**è·å–æ–¹å¼**:

```yaml
# ä» Pod çš„ OwnerReferences æ¨æ–­
# éœ€è¦é€šè¿‡ API æŸ¥è¯¢æˆ– Downward API æ³¨å…¥ Pod æ ‡ç­¾

env:
- name: K8S_DEPLOYMENT_NAME
  valueFrom:
    fieldRef:
      fieldPath: metadata.labels['app.kubernetes.io/name']
```

### 7.2 k8s.replicaset.* å±æ€§

**æ ‡å‡†å±æ€§**:

| å±æ€§å | ç±»å‹ | æè¿° | ç¤ºä¾‹ |
|--------|------|------|------|
| `k8s.replicaset.name` | string | ReplicaSet åç§° | `web-deployment-7d4f5c6b8` |
| `k8s.replicaset.uid` | string | ReplicaSet UID | `ac3d7f9e-2b5c-4d8a-9f1e-3c4b5a6d7e8f` |

### 7.3 k8s.statefulset.* å±æ€§

**æ ‡å‡†å±æ€§**:

| å±æ€§å | ç±»å‹ | æè¿° | ç¤ºä¾‹ |
|--------|------|------|------|
| `k8s.statefulset.name` | string | StatefulSet åç§° | `database` |
| `k8s.statefulset.uid` | string | StatefulSet UID | `bd4e8f0g-3c6d-5e9b-0g2h-4d5e6f7g8h9i` |

### 7.4 k8s.daemonset.* å±æ€§

**æ ‡å‡†å±æ€§**:

| å±æ€§å | ç±»å‹ | æè¿° | ç¤ºä¾‹ |
|--------|------|------|------|
| `k8s.daemonset.name` | string | DaemonSet åç§° | `log-collector` |
| `k8s.daemonset.uid` | string | DaemonSet UID | `ce5f9g1h-4d7e-6f0a-1h3i-5e6f7g8h9i0j` |

### 7.5 k8s.job.*å’Œ k8s.cronjob.* å±æ€§

**Job å±æ€§**:

| å±æ€§å | ç±»å‹ | æè¿° | ç¤ºä¾‹ |
|--------|------|------|------|
| `k8s.job.name` | string | Job åç§° | `data-migration-20231009` |
| `k8s.job.uid` | string | Job UID | `df6g0h2i-5e8f-7g1b-2i4j-6f7g8h9i0j1k` |

**CronJob å±æ€§**:

| å±æ€§å | ç±»å‹ | æè¿° | ç¤ºä¾‹ |
|--------|------|------|------|
| `k8s.cronjob.name` | string | CronJob åç§° | `daily-backup` |
| `k8s.cronjob.uid` | string | CronJob UID | `eg7h1i3j-6f9g-8h2c-3j5k-7g8h9i0j1k2l` |

**å·¥ä½œè´Ÿè½½æ£€æµ‹å™¨**:

```go
package k8sresource

import (
    "context"
    "os"
    "strings"
    
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/sdk/resource"
    semconv "go.opentelemetry.io/otel/semconv/v1.28.0"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes"
)

// WorkloadDetector æ£€æµ‹å·¥ä½œè´Ÿè½½ä¿¡æ¯
type WorkloadDetector struct {
    clientset *kubernetes.Clientset
}

func NewWorkloadDetector(clientset *kubernetes.Clientset) *WorkloadDetector {
    return &WorkloadDetector{clientset: clientset}
}

func (d *WorkloadDetector) Detect(ctx context.Context) (*resource.Resource, error) {
    podName := os.Getenv("K8S_POD_NAME")
    namespace := os.Getenv("K8S_NAMESPACE_NAME")
    
    if podName == "" || namespace == "" || d.clientset == nil {
        return d.detectFromLabels()
    }
    
    // ä» Pod çš„ OwnerReferences æ¨æ–­å·¥ä½œè´Ÿè½½
    pod, err := d.clientset.CoreV1().Pods(namespace).Get(
        ctx,
        podName,
        metav1.GetOptions{},
    )
    if err != nil {
        return d.detectFromLabels()
    }
    
    for _, owner := range pod.OwnerReferences {
        switch owner.Kind {
        case "ReplicaSet":
            return d.detectDeployment(ctx, namespace, owner.Name)
        case "StatefulSet":
            return d.buildWorkloadResource("statefulset", owner.Name, string(owner.UID))
        case "DaemonSet":
            return d.buildWorkloadResource("daemonset", owner.Name, string(owner.UID))
        case "Job":
            return d.detectJob(ctx, namespace, owner.Name, string(owner.UID))
        }
    }
    
    return resource.Empty(), nil
}

func (d *WorkloadDetector) detectDeployment(ctx context.Context, namespace, rsName string) (*resource.Resource, error) {
    // ReplicaSet åç§°æ ¼å¼: <deployment-name>-<hash>
    // æŸ¥è¯¢ ReplicaSet è·å– Deployment ä¿¡æ¯
    rs, err := d.clientset.AppsV1().ReplicaSets(namespace).Get(
        ctx,
        rsName,
        metav1.GetOptions{},
    )
    if err != nil {
        return resource.Empty(), err
    }
    
    // æ£€æŸ¥ ReplicaSet çš„ OwnerReferences
    for _, owner := range rs.OwnerReferences {
        if owner.Kind == "Deployment" {
            attrs := []attribute.KeyValue{
                semconv.K8SDeploymentName(owner.Name),
                semconv.K8SDeploymentUID(string(owner.UID)),
                semconv.K8SReplicaSetName(rsName),
                semconv.K8SReplicaSetUID(string(rs.UID)),
            }
            
            return resource.NewWithAttributes(
                semconv.SchemaURL,
                attrs...,
            ), nil
        }
    }
    
    // åªæœ‰ ReplicaSet,æ²¡æœ‰ Deployment
    return d.buildWorkloadResource("replicaset", rsName, string(rs.UID))
}

func (d *WorkloadDetector) detectJob(ctx context.Context, namespace, jobName, jobUID string) (*resource.Resource, error) {
    attrs := []attribute.KeyValue{
        semconv.K8SJobName(jobName),
        semconv.K8SJobUID(jobUID),
    }
    
    // æ£€æŸ¥æ˜¯å¦æ˜¯ CronJob åˆ›å»ºçš„ Job
    job, err := d.clientset.BatchV1().Jobs(namespace).Get(
        ctx,
        jobName,
        metav1.GetOptions{},
    )
    if err == nil {
        for _, owner := range job.OwnerReferences {
            if owner.Kind == "CronJob" {
                attrs = append(attrs,
                    semconv.K8SCronJobName(owner.Name),
                    semconv.K8SCronJobUID(string(owner.UID)),
                )
                break
            }
        }
    }
    
    return resource.NewWithAttributes(
        semconv.SchemaURL,
        attrs...,
    ), nil
}

func (d *WorkloadDetector) detectFromLabels() (*resource.Resource, error) {
    // ä»æ ‡å‡†æ ‡ç­¾æ¨æ–­å·¥ä½œè´Ÿè½½
    // app.kubernetes.io/name, app.kubernetes.io/instance
    
    workloadName := os.Getenv("K8S_WORKLOAD_NAME")
    if workloadName == "" {
        return resource.Empty(), nil
    }
    
    workloadType := os.Getenv("K8S_WORKLOAD_TYPE") // deployment, statefulset, etc.
    if workloadType == "" {
        workloadType = "deployment" // é»˜è®¤
    }
    
    return d.buildWorkloadResource(workloadType, workloadName, "")
}

func (d *WorkloadDetector) buildWorkloadResource(kind, name, uid string) (*resource.Resource, error) {
    var attrs []attribute.KeyValue
    
    switch strings.ToLower(kind) {
    case "deployment":
        attrs = []attribute.KeyValue{semconv.K8SDeploymentName(name)}
        if uid != "" {
            attrs = append(attrs, semconv.K8SDeploymentUID(uid))
        }
    case "statefulset":
        attrs = []attribute.KeyValue{semconv.K8SStatefulSetName(name)}
        if uid != "" {
            attrs = append(attrs, semconv.K8SStatefulSetUID(uid))
        }
    case "daemonset":
        attrs = []attribute.KeyValue{semconv.K8SDaemonSetName(name)}
        if uid != "" {
            attrs = append(attrs, semconv.K8SDaemonSetUID(uid))
        }
    case "replicaset":
        attrs = []attribute.KeyValue{semconv.K8SReplicaSetName(name)}
        if uid != "" {
            attrs = append(attrs, semconv.K8SReplicaSetUID(uid))
        }
    default:
        return resource.Empty(), nil
    }
    
    return resource.NewWithAttributes(
        semconv.SchemaURL,
        attrs...,
    ), nil
}
```

---

## 8. Go å®ç° - K8s èµ„æºæ£€æµ‹å™¨

### 8.1 Downward API æ–¹å¼

**æ¨èæ–¹å¼** - æ— éœ€ K8s API æƒé™,æ€§èƒ½æœ€ä¼˜:

```yaml
# Deployment manifest
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: app
        image: my-app:1.0.0
        env:
        # Pod ä¿¡æ¯
        - name: K8S_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: K8S_POD_UID
          valueFrom:
            fieldRef:
              fieldPath: metadata.uid
        - name: K8S_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        
        # å‘½åç©ºé—´
        - name: K8S_NAMESPACE_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        
        # èŠ‚ç‚¹ä¿¡æ¯
        - name: K8S_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        
        # å®¹å™¨åç§° (ç¡¬ç¼–ç )
        - name: K8S_CONTAINER_NAME
          value: "app"
        
        # å·¥ä½œè´Ÿè½½ä¿¡æ¯ (é€šè¿‡æ ‡ç­¾)
        - name: K8S_DEPLOYMENT_NAME
          value: "my-app"
        
        # é›†ç¾¤ä¿¡æ¯ (éœ€è¦æ‰‹åŠ¨é…ç½®)
        - name: K8S_CLUSTER_NAME
          value: "prod-us-east-1"
```

### 8.2 ç¯å¢ƒå˜é‡æ³¨å…¥

**å®Œæ•´çš„èµ„æºæ£€æµ‹å™¨**:

```go
package k8sresource

import (
    "context"
    "errors"
    
    "go.opentelemetry.io/otel/sdk/resource"
    semconv "go.opentelemetry.io/otel/semconv/v1.28.0"
)

// Detector ç»„åˆæ‰€æœ‰ K8s èµ„æºæ£€æµ‹å™¨
type Detector struct {
    clusterDetector   *ClusterDetector
    nodeDetector      *NodeDetector
    namespaceDetector *NamespaceDetector
    podDetector       *PodDetector
    containerDetector *ContainerDetector
    workloadDetector  *WorkloadDetector
}

// NewDetector åˆ›å»ºå®Œæ•´çš„ K8s èµ„æºæ£€æµ‹å™¨
func NewDetector() (*Detector, error) {
    return &Detector{
        clusterDetector:   nil, // éœ€è¦ K8s API è®¿é—®
        nodeDetector:      nil, // éœ€è¦ K8s API è®¿é—®
        namespaceDetector: NewNamespaceDetector(),
        podDetector:       NewPodDetector(),
        containerDetector: NewContainerDetector(),
        workloadDetector:  nil, // éœ€è¦ K8s API è®¿é—®
    }, nil
}

// NewDetectorWithAPI åˆ›å»ºå¸¦ API è®¿é—®çš„æ£€æµ‹å™¨
func NewDetectorWithAPI() (*Detector, error) {
    // åˆå§‹åŒ– K8s clientset
    clusterDet, err := NewClusterDetector()
    if err != nil {
        return nil, err
    }
    
    clientset := clusterDet.clientset
    
    return &Detector{
        clusterDetector:   clusterDet,
        nodeDetector:      NewNodeDetector(clientset),
        namespaceDetector: NewNamespaceDetector(),
        podDetector:       NewPodDetector(),
        containerDetector: NewContainerDetector(),
        workloadDetector:  NewWorkloadDetector(clientset),
    }, nil
}

// Detect æ£€æµ‹æ‰€æœ‰ K8s èµ„æºå±æ€§
func (d *Detector) Detect(ctx context.Context) (*resource.Resource, error) {
    resources := []*resource.Resource{}
    
    // æ£€æµ‹é›†ç¾¤ä¿¡æ¯
    if d.clusterDetector != nil {
        if res, err := d.clusterDetector.Detect(ctx); err == nil {
            resources = append(resources, res)
        }
    }
    
    // æ£€æµ‹èŠ‚ç‚¹ä¿¡æ¯
    if d.nodeDetector != nil {
        if res, err := d.nodeDetector.Detect(ctx); err == nil {
            resources = append(resources, res)
        }
    }
    
    // æ£€æµ‹å‘½åç©ºé—´
    if d.namespaceDetector != nil {
        if res, err := d.namespaceDetector.Detect(ctx); err == nil {
            resources = append(resources, res)
        }
    }
    
    // æ£€æµ‹ Pod
    if d.podDetector != nil {
        if res, err := d.podDetector.Detect(ctx); err == nil {
            resources = append(resources, res)
        }
    }
    
    // æ£€æµ‹å®¹å™¨
    if d.containerDetector != nil {
        if res, err := d.containerDetector.Detect(ctx); err == nil {
            resources = append(resources, res)
        }
    }
    
    // æ£€æµ‹å·¥ä½œè´Ÿè½½
    if d.workloadDetector != nil {
        if res, err := d.workloadDetector.Detect(ctx); err == nil {
            resources = append(resources, res)
        }
    }
    
    // åˆå¹¶æ‰€æœ‰èµ„æº
    if len(resources) == 0 {
        return resource.Empty(), errors.New("no k8s resources detected")
    }
    
    merged := resources[0]
    for i := 1; i < len(resources); i++ {
        merged, _ = resource.Merge(merged, resources[i])
    }
    
    return merged, nil
}
```

### 8.3 K8s API æŸ¥è¯¢

**ä½¿ç”¨ client-go è¿›è¡Œ API æŸ¥è¯¢** (é€‚ç”¨äºéœ€è¦åŠ¨æ€ä¿¡æ¯çš„åœºæ™¯):

```go
package k8sresource

import (
    "context"
    "fmt"
    
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes"
    "k8s.io/client-go/rest"
)

// APIBasedDetector åŸºäº K8s API çš„æ£€æµ‹å™¨
type APIBasedDetector struct {
    clientset *kubernetes.Clientset
}

func NewAPIBasedDetector() (*APIBasedDetector, error) {
    // In-Cluster Config
    config, err := rest.InClusterConfig()
    if err != nil {
        return nil, fmt.Errorf("failed to get in-cluster config: %w", err)
    }
    
    clientset, err := kubernetes.NewForConfig(config)
    if err != nil {
        return nil, fmt.Errorf("failed to create clientset: %w", err)
    }
    
    return &APIBasedDetector{
        clientset: clientset,
    }, nil
}

// GetPodInfo æŸ¥è¯¢å®Œæ•´çš„ Pod ä¿¡æ¯
func (d *APIBasedDetector) GetPodInfo(ctx context.Context, namespace, podName string) (*PodInfo, error) {
    pod, err := d.clientset.CoreV1().Pods(namespace).Get(
        ctx,
        podName,
        metav1.GetOptions{},
    )
    if err != nil {
        return nil, err
    }
    
    info := &PodInfo{
        Name:      pod.Name,
        UID:       string(pod.UID),
        Namespace: pod.Namespace,
        NodeName:  pod.Spec.NodeName,
        IP:        pod.Status.PodIP,
        Labels:    pod.Labels,
        Annotations: pod.Annotations,
    }
    
    // æå–å·¥ä½œè´Ÿè½½ä¿¡æ¯
    for _, owner := range pod.OwnerReferences {
        info.OwnerKind = owner.Kind
        info.OwnerName = owner.Name
        info.OwnerUID = string(owner.UID)
        break
    }
    
    return info, nil
}

type PodInfo struct {
    Name        string
    UID         string
    Namespace   string
    NodeName    string
    IP          string
    Labels      map[string]string
    Annotations map[string]string
    OwnerKind   string
    OwnerName   string
    OwnerUID    string
}
```

**RBAC é…ç½®** (å¦‚æœä½¿ç”¨ API æŸ¥è¯¢):

```yaml
# ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-app
  namespace: production

---
# Role
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: otel-app-reader
  namespace: production
rules:
- apiGroups: [""]
  resources: ["pods", "namespaces"]
  verbs: ["get", "list"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets", "statefulsets", "daemonsets"]
  verbs: ["get", "list"]
- apiGroups: ["batch"]
  resources: ["jobs", "cronjobs"]
  verbs: ["get", "list"]

---
# RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: otel-app-reader-binding
  namespace: production
subjects:
- kind: ServiceAccount
  name: otel-app
  namespace: production
roleRef:
  kind: Role
  name: otel-app-reader
  apiGroup: rbac.authorization.k8s.io

---
# ClusterRole (for cluster-level resources)
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-app-cluster-reader
rules:
- apiGroups: [""]
  resources: ["nodes", "namespaces"]
  verbs: ["get", "list"]

---
# ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-app-cluster-reader-binding
subjects:
- kind: ServiceAccount
  name: otel-app
  namespace: production
roleRef:
  kind: ClusterRole
  name: otel-app-cluster-reader
  apiGroup: rbac.authorization.k8s.io
```

---

## 9. èµ„æºæ£€æµ‹æœ€ä½³å®è·µ

### 9.1 æ£€æµ‹ä¼˜å…ˆçº§

**ä¼˜å…ˆçº§é¡ºåº**:

```go
// èµ„æºæ£€æµ‹ç­–ç•¥
type DetectionStrategy string

const (
    // 1. ä¼˜å…ˆ: Downward API ç¯å¢ƒå˜é‡ (æœ€å¿«,æ— éœ€æƒé™)
    StrategyDownwardAPIEnv DetectionStrategy = "downward_api_env"
    
    // 2. æ¬¡ä¼˜: Downward API Volume (éœ€è¦æŒ‚è½½)
    StrategyDownwardAPIVolume DetectionStrategy = "downward_api_volume"
    
    // 3. å¤‡é€‰: Service Account Token (å‘½åç©ºé—´)
    StrategyServiceAccount DetectionStrategy = "service_account"
    
    // 4. æœ€å: K8s API æŸ¥è¯¢ (éœ€è¦ RBAC æƒé™,æœ‰å»¶è¿Ÿ)
    StrategyAPIQuery DetectionStrategy = "api_query"
)

// StrategySelector é€‰æ‹©æœ€ä¼˜æ£€æµ‹ç­–ç•¥
type StrategySelector struct {
    strategies []DetectionStrategy
}

func NewStrategySelector() *StrategySelector {
    return &StrategySelector{
        strategies: []DetectionStrategy{
            StrategyDownwardAPIEnv,
            StrategyDownwardAPIVolume,
            StrategyServiceAccount,
            StrategyAPIQuery,
        },
    }
}

func (s *StrategySelector) SelectBest(ctx context.Context) DetectionStrategy {
    for _, strategy := range s.strategies {
        if s.isAvailable(strategy) {
            return strategy
        }
    }
    return StrategyDownwardAPIEnv // é»˜è®¤
}

func (s *StrategySelector) isAvailable(strategy DetectionStrategy) bool {
    switch strategy {
    case StrategyDownwardAPIEnv:
        // æ£€æŸ¥å…³é”®ç¯å¢ƒå˜é‡æ˜¯å¦å­˜åœ¨
        return os.Getenv("K8S_POD_NAME") != ""
        
    case StrategyDownwardAPIVolume:
        // æ£€æŸ¥æŒ‚è½½ç‚¹æ˜¯å¦å­˜åœ¨
        _, err := os.Stat("/etc/podinfo")
        return err == nil
        
    case StrategyServiceAccount:
        // æ£€æŸ¥ Service Account æŒ‚è½½
        _, err := os.Stat("/var/run/secrets/kubernetes.io/serviceaccount/namespace")
        return err == nil
        
    case StrategyAPIQuery:
        // æ£€æŸ¥æ˜¯å¦èƒ½åˆ›å»º K8s client
        _, err := rest.InClusterConfig()
        return err == nil
        
    default:
        return false
    }
}
```

### 9.2 æ€§èƒ½ä¼˜åŒ–

**æœ€ä½³å®è·µ**:

```go
// èµ„æºæ£€æµ‹å™¨é…ç½®
type DetectorConfig struct {
    // å¯ç”¨ç¼“å­˜
    EnableCache bool
    
    // ç¼“å­˜ TTL
    CacheTTL time.Duration
    
    // æ˜¯å¦ä½¿ç”¨ API æŸ¥è¯¢
    UseAPIQuery bool
    
    // API æŸ¥è¯¢è¶…æ—¶
    APITimeout time.Duration
    
    // å¹¶è¡Œæ£€æµ‹
    ParallelDetection bool
}

// OptimizedDetector ä¼˜åŒ–çš„èµ„æºæ£€æµ‹å™¨
type OptimizedDetector struct {
    config DetectorConfig
    cache  *resourceCache
    mu     sync.RWMutex
}

type resourceCache struct {
    resource  *resource.Resource
    timestamp time.Time
}

func NewOptimizedDetector(config DetectorConfig) *OptimizedDetector {
    return &OptimizedDetector{
        config: config,
        cache:  &resourceCache{},
    }
}

func (d *OptimizedDetector) Detect(ctx context.Context) (*resource.Resource, error) {
    // æ£€æŸ¥ç¼“å­˜
    if d.config.EnableCache {
        d.mu.RLock()
        if d.cache.resource != nil && time.Since(d.cache.timestamp) < d.config.CacheTTL {
            cached := d.cache.resource
            d.mu.RUnlock()
            return cached, nil
        }
        d.mu.RUnlock()
    }
    
    // æ‰§è¡Œæ£€æµ‹
    var res *resource.Resource
    var err error
    
    if d.config.ParallelDetection {
        res, err = d.detectParallel(ctx)
    } else {
        res, err = d.detectSequential(ctx)
    }
    
    if err != nil {
        return nil, err
    }
    
    // æ›´æ–°ç¼“å­˜
    if d.config.EnableCache {
        d.mu.Lock()
        d.cache.resource = res
        d.cache.timestamp = time.Now()
        d.mu.Unlock()
    }
    
    return res, nil
}

func (d *OptimizedDetector) detectParallel(ctx context.Context) (*resource.Resource, error) {
    type result struct {
        res *resource.Resource
        err error
    }
    
    results := make(chan result, 6)
    
    detectors := []func(context.Context) (*resource.Resource, error){
        NewNamespaceDetector().Detect,
        NewPodDetector().Detect,
        NewContainerDetector().Detect,
    }
    
    for _, detect := range detectors {
        go func(fn func(context.Context) (*resource.Resource, error)) {
            res, err := fn(ctx)
            results <- result{res, err}
        }(detect)
    }
    
    resources := []*resource.Resource{}
    for i := 0; i < len(detectors); i++ {
        r := <-results
        if r.err == nil && r.res != nil {
            resources = append(resources, r.res)
        }
    }
    
    if len(resources) == 0 {
        return resource.Empty(), errors.New("no resources detected")
    }
    
    merged := resources[0]
    for i := 1; i < len(resources); i++ {
        merged, _ = resource.Merge(merged, resources[i])
    }
    
    return merged, nil
}

func (d *OptimizedDetector) detectSequential(ctx context.Context) (*resource.Resource, error) {
    // åŸºç¡€å®ç°,æŒ‰é¡ºåºæ£€æµ‹
    detector, _ := NewDetector()
    return detector.Detect(ctx)
}
```

### 9.3 ç¼“å­˜ç­–ç•¥

**æ¨èç¼“å­˜é…ç½®**:

```go
// ä¸åŒç±»å‹èµ„æºçš„ç¼“å­˜ç­–ç•¥
var DefaultCacheTTL = map[string]time.Duration{
    "cluster":    24 * time.Hour,      // é›†ç¾¤ä¿¡æ¯å¾ˆå°‘å˜åŒ–
    "node":       1 * time.Hour,       // èŠ‚ç‚¹ä¿¡æ¯ä¸­ç­‰å˜åŒ–
    "namespace":  24 * time.Hour,      // å‘½åç©ºé—´å¾ˆå°‘å˜åŒ–
    "pod":        5 * time.Minute,     // Pod å¯èƒ½é‡å¯
    "container":  5 * time.Minute,     // å®¹å™¨å¯èƒ½é‡å¯
    "workload":   1 * time.Hour,       // å·¥ä½œè´Ÿè½½ä¸­ç­‰å˜åŒ–
}

// æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨
type CacheManager struct {
    caches map[string]*timedCache
    mu     sync.RWMutex
}

type timedCache struct {
    data      interface{}
    timestamp time.Time
    ttl       time.Duration
}

func NewCacheManager() *CacheManager {
    return &CacheManager{
        caches: make(map[string]*timedCache),
    }
}

func (cm *CacheManager) Get(key string) (interface{}, bool) {
    cm.mu.RLock()
    defer cm.mu.RUnlock()
    
    cache, ok := cm.caches[key]
    if !ok {
        return nil, false
    }
    
    if time.Since(cache.timestamp) > cache.ttl {
        return nil, false // å·²è¿‡æœŸ
    }
    
    return cache.data, true
}

func (cm *CacheManager) Set(key string, data interface{}, ttl time.Duration) {
    cm.mu.Lock()
    defer cm.mu.Unlock()
    
    cm.caches[key] = &timedCache{
        data:      data,
        timestamp: time.Now(),
        ttl:       ttl,
    }
}

func (cm *CacheManager) Invalidate(key string) {
    cm.mu.Lock()
    defer cm.mu.Unlock()
    
    delete(cm.caches, key)
}
```

---

## 10. å®Œæ•´ç¤ºä¾‹

### 10.1 å…¨åŠŸèƒ½èµ„æºæ£€æµ‹å™¨

**main.go**:

```go
package main

import (
    "context"
    "log"
    "time"
    
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
    "go.opentelemetry.io/otel/sdk/resource"
    sdktrace "go.opentelemetry.io/otel/sdk/trace"
    semconv "go.opentelemetry.io/otel/semconv/v1.28.0"
    
    "myapp/k8sresource"
)

func main() {
    ctx := context.Background()
    
    // 1. æ£€æµ‹ K8s èµ„æº
    k8sRes, err := detectK8sResources(ctx)
    if err != nil {
        log.Printf("Warning: failed to detect k8s resources: %v", err)
        k8sRes = resource.Empty()
    }
    
    // 2. åˆå¹¶å…¶ä»–èµ„æº
    serviceRes := resource.NewWithAttributes(
        semconv.SchemaURL,
        semconv.ServiceName("my-app"),
        semconv.ServiceVersion("1.0.0"),
    )
    
    res, err := resource.Merge(serviceRes, k8sRes)
    if err != nil {
        log.Fatalf("Failed to merge resources: %v", err)
    }
    
    // 3. åˆå§‹åŒ– TracerProvider
    tp, err := initTracerProvider(ctx, res)
    if err != nil {
        log.Fatalf("Failed to initialize tracer provider: %v", err)
    }
    defer func() {
        if err := tp.Shutdown(ctx); err != nil {
            log.Printf("Error shutting down tracer provider: %v", err)
        }
    }()
    
    otel.SetTracerProvider(tp)
    
    // 4. åº”ç”¨é€»è¾‘
    log.Println("Application started with K8s resource detection")
    runApp(ctx)
}

func detectK8sResources(ctx context.Context) (*resource.Resource, error) {
    // å°è¯•ä½¿ç”¨ API è®¿é—®
    detector, err := k8sresource.NewDetectorWithAPI()
    if err != nil {
        // é™çº§åˆ°ä»…ç¯å¢ƒå˜é‡
        log.Printf("API access not available, using env vars only: %v", err)
        detector, _ = k8sresource.NewDetector()
    }
    
    // é…ç½®æ£€æµ‹å™¨
    optimized := k8sresource.NewOptimizedDetector(k8sresource.DetectorConfig{
        EnableCache:       true,
        CacheTTL:          5 * time.Minute,
        UseAPIQuery:       false, // ä»…ä½¿ç”¨ç¯å¢ƒå˜é‡
        ParallelDetection: true,
    })
    
    return optimized.Detect(ctx)
}

func initTracerProvider(ctx context.Context, res *resource.Resource) (*sdktrace.TracerProvider, error) {
    // OTLP gRPC Exporter
    exporter, err := otlptracegrpc.New(
        ctx,
        otlptracegrpc.WithEndpoint("otel-collector:4317"),
        otlptracegrpc.WithInsecure(),
    )
    if err != nil {
        return nil, err
    }
    
    tp := sdktrace.NewTracerProvider(
        sdktrace.WithResource(res),
        sdktrace.WithBatcher(exporter),
        sdktrace.WithSampler(sdktrace.AlwaysSample()),
    )
    
    return tp, nil
}

func runApp(ctx context.Context) {
    tracer := otel.Tracer("my-app")
    
    // åˆ›å»º Span
    ctx, span := tracer.Start(ctx, "app.startup")
    defer span.End()
    
    // æ¨¡æ‹Ÿä¸šåŠ¡é€»è¾‘
    time.Sleep(100 * time.Millisecond)
    span.AddEvent("app started successfully")
}
```

### 10.2 Kubernetes Manifest

**deployment.yaml**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  namespace: production
  labels:
    app: my-app
    version: v1.0.0
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
        version: v1.0.0
    spec:
      serviceAccountName: otel-app
      
      containers:
      - name: app
        image: my-app:1.0.0
        ports:
        - containerPort: 8080
          name: http
        
        env:
        # ===== K8s èµ„æºä¿¡æ¯ (Downward API) =====
        
        # Pod ä¿¡æ¯
        - name: K8S_POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        
        - name: K8S_POD_UID
          valueFrom:
            fieldRef:
              fieldPath: metadata.uid
        
        - name: K8S_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        
        # å‘½åç©ºé—´
        - name: K8S_NAMESPACE_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        
        # èŠ‚ç‚¹ä¿¡æ¯
        - name: K8S_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        
        # å®¹å™¨åç§° (ç¡¬ç¼–ç )
        - name: K8S_CONTAINER_NAME
          value: "app"
        
        # å·¥ä½œè´Ÿè½½ä¿¡æ¯ (ç¡¬ç¼–ç æˆ–é€šè¿‡æ ‡ç­¾)
        - name: K8S_DEPLOYMENT_NAME
          value: "my-app"
        
        # é›†ç¾¤ä¿¡æ¯ (ConfigMap å¼•ç”¨)
        - name: K8S_CLUSTER_NAME
          valueFrom:
            configMapKeyRef:
              name: cluster-info
              key: cluster.name
        
        - name: K8S_CLUSTER_UID
          valueFrom:
            configMapKeyRef:
              name: cluster-info
              key: cluster.uid
        
        # OTLP Exporter é…ç½®
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "http://otel-collector:4317"
        
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5

---
# ConfigMap for cluster info
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-info
  namespace: production
data:
  cluster.name: "prod-us-east-1"
  cluster.uid: "550e8400-e29b-41d4-a716-446655440000"

---
# Service
apiVersion: v1
kind: Service
metadata:
  name: my-app
  namespace: production
spec:
  selector:
    app: my-app
  ports:
  - port: 80
    targetPort: 8080
    name: http
  type: ClusterIP
```

### 10.3 Helm Chart é›†æˆ

**values.yaml**:

```yaml
# Helm Chart values
app:
  name: my-app
  version: 1.0.0
  replicas: 3

k8s:
  cluster:
    name: "prod-us-east-1"
    uid: "550e8400-e29b-41d4-a716-446655440000"
  
  # å¯ç”¨ Downward API
  downwardAPI:
    enabled: true
    
  # å¯ç”¨ API æŸ¥è¯¢ (éœ€è¦ RBAC)
  apiAccess:
    enabled: false

otel:
  collector:
    endpoint: "http://otel-collector:4317"
  
  # èµ„æºæ£€æµ‹é…ç½®
  resourceDetection:
    cache:
      enabled: true
      ttl: "5m"
    
    parallel: true
    
    sources:
      - downward_api
      - service_account
      # - api_query  # éœ€è¦ RBAC
```

**templates/deployment.yaml**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Values.app.name }}
  namespace: {{ .Release.Namespace }}
spec:
  replicas: {{ .Values.app.replicas }}
  template:
    spec:
      containers:
      - name: {{ .Values.app.name }}
        env:
        {{- if .Values.k8s.downwardAPI.enabled }}
        # Downward API æ³¨å…¥
        - name: K8S_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: K8S_POD_UID
          valueFrom:
            fieldRef:
              fieldPath: metadata.uid
        - name: K8S_NAMESPACE_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: K8S_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        {{- end }}
        
        # é›†ç¾¤ä¿¡æ¯
        - name: K8S_CLUSTER_NAME
          value: {{ .Values.k8s.cluster.name | quote }}
        - name: K8S_CLUSTER_UID
          value: {{ .Values.k8s.cluster.uid | quote }}
        
        # OTLP é…ç½®
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: {{ .Values.otel.collector.endpoint | quote }}
```

---

## 11. å¸¸è§é—®é¢˜ (FAQ)

### Q1: ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ Downward API vs K8s API?

**A**: ä¼˜å…ˆçº§:

1. **Downward API ç¯å¢ƒå˜é‡**: é€‚ç”¨äºé™æ€ä¿¡æ¯ (Pod name, UID, namespace),æ€§èƒ½æœ€ä¼˜
2. **Downward API Volume**: é€‚ç”¨äºéœ€è¦æ–‡ä»¶æŒ‚è½½çš„åœºæ™¯
3. **K8s API æŸ¥è¯¢**: ä»…åœ¨éœ€è¦åŠ¨æ€ä¿¡æ¯æˆ–æ— æ³•é€šè¿‡ Downward API è·å–æ—¶ä½¿ç”¨

**ç¤ºä¾‹**:

```go
// ä¼˜å…ˆä½¿ç”¨ Downward API
podName := os.Getenv("K8S_POD_NAME")

// ä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨ API
if needDetailedInfo {
    pod, _ := clientset.CoreV1().Pods(namespace).Get(ctx, podName, metav1.GetOptions{})
}
```

### Q2: å¦‚ä½•æœ€å°åŒ– RBAC æƒé™?

**A**: åªæˆäºˆå¿…è¦çš„æƒé™:

```yaml
# æœ€å°æƒé™ç¤ºä¾‹
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get"]
  resourceNames: ["<specific-pod-name>"]  # é™åˆ¶åˆ°ç‰¹å®šèµ„æº
```

### Q3: é›†ç¾¤åç§°å¦‚ä½•æ ‡å‡†åŒ–?

**A**: æ¨èå‘½åçº¦å®š:

```text
<env>-<region>-<number>

ç¤ºä¾‹:
- prod-us-east-1
- staging-eu-west-2
- dev-local
```

### Q4: å¤šé›†ç¾¤ç¯å¢ƒå¦‚ä½•åŒºåˆ†?

**A**: ä½¿ç”¨é›†ç¾¤ UID + åç§°:

```go
attrs := []attribute.KeyValue{
    semconv.K8SClusterName("prod-us-east-1"),
    semconv.K8SClusterUID("550e8400-e29b-41d4-a716-446655440000"),
}
```

### Q5: å¦‚ä½•å¤„ç† Pod é‡å¯?

**A**: Pod UID ä¼šå˜åŒ–,éœ€è¦åŠ¨æ€æ£€æµ‹:

```go
// æ¯æ¬¡å¯åŠ¨æ—¶é‡æ–°æ£€æµ‹
func init() {
    podUID = os.Getenv("K8S_POD_UID")
}
```

### Q6: StatefulSet å¦‚ä½•å¤„ç†?

**A**: StatefulSet Pod åç§°ç¨³å®š:

```go
// StatefulSet Pod: my-statefulset-0, my-statefulset-1, ...
podName := os.Getenv("K8S_POD_NAME")

// æå–åºå·
if strings.Contains(podName, "-") {
    parts := strings.Split(podName, "-")
    ordinal := parts[len(parts)-1]
    // ä½¿ç”¨ ordinal è¿›è¡Œåˆ†ç‰‡ç­‰æ“ä½œ
}
```

### Q7: CronJob å¦‚ä½•è¿½è¸ª?

**A**: CronJob -> Job -> Pod:

```go
// æ·»åŠ  Job å’Œ CronJob å±æ€§
attrs := []attribute.KeyValue{
    semconv.K8SJobName("data-migration-20231009"),
    semconv.K8SCronJobName("daily-backup"),
}
```

### Q8: å¦‚ä½•å¤„ç†æ—  K8s ç¯å¢ƒ?

**A**: æ£€æµ‹å¹¶é™çº§:

```go
func isRunningInK8s() bool {
    return os.Getenv("K8S_POD_NAME") != "" ||
           os.Getenv("KUBERNETES_SERVICE_HOST") != ""
}

func detectResources(ctx context.Context) (*resource.Resource, error) {
    if isRunningInK8s() {
        return detectK8sResources(ctx)
    }
    return detectLocalResources(ctx)
}
```

### Q9: èµ„æºæ£€æµ‹å¤±è´¥å¦‚ä½•å¤„ç†?

**A**: ä¼˜é›…é™çº§:

```go
k8sRes, err := detector.Detect(ctx)
if err != nil {
    log.Printf("Warning: k8s resource detection failed: %v", err)
    k8sRes = resource.Empty() // ä½¿ç”¨ç©ºèµ„æº
}

// ç»§ç»­ä½¿ç”¨åŸºæœ¬çš„æœåŠ¡èµ„æº
serviceRes := resource.NewWithAttributes(
    semconv.SchemaURL,
    semconv.ServiceName("my-app"),
)

finalRes, _ := resource.Merge(serviceRes, k8sRes)
```

### Q10: æ€§èƒ½å½±å“å¦‚ä½•?

**A**: æ€§èƒ½å¯¹æ¯”:

| æ–¹æ³• | å»¶è¿Ÿ | CPU | å†…å­˜ |
|------|------|-----|------|
| Downward API (ç¯å¢ƒå˜é‡) | ~0ms | å¯å¿½ç•¥ | å¯å¿½ç•¥ |
| Downward API (Volume) | ~1ms | ä½ | ä½ |
| K8s API æŸ¥è¯¢ | ~10-50ms | ä¸­ | ä¸­ |

**ä¼˜åŒ–å»ºè®®**:

- å¯ç”¨ç¼“å­˜ (TTL 5-60åˆ†é’Ÿ)
- ä½¿ç”¨ Downward API
- é¿å…é¢‘ç¹ API æŸ¥è¯¢

---

## ğŸ“š å‚è€ƒèµ„æº

1. **OpenTelemetry Semantic Conventions**:
   - K8s Resource Attributes: <https://opentelemetry.io/docs/specs/semconv/resource/k8s/>

2. **Kubernetes Downward API**:
   - Official Docs: <https://kubernetes.io/docs/concepts/workloads/pods/downward-api/>

3. **client-go**:
   - GitHub: <https://github.com/kubernetes/client-go>

4. **ç›¸å…³æ–‡æ¡£**:
   - [01_æœåŠ¡èµ„æº.md](./01_æœåŠ¡èµ„æº.md) - Service å±æ€§
   - [02_éƒ¨ç½²ç¯å¢ƒ.md](./02_éƒ¨ç½²ç¯å¢ƒ.md) - Deployment ç¯å¢ƒ
   - [03_äº‘å¹³å°èµ„æº.md](./03_äº‘å¹³å°èµ„æº.md) - äº‘å¹³å°é›†æˆ
   - [05_ä¸»æœºä¸è¿›ç¨‹.md](./05_ä¸»æœºä¸è¿›ç¨‹.md) - Host å’Œ Process å±æ€§

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0.0  
**æœ€åæ›´æ–°**: 2025-10-09  
**OpenTelemetry ç‰ˆæœ¬**: 1.32.0+  
**Semantic Conventions ç‰ˆæœ¬**: 1.28.0+
